{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39f6ed2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando dependencias...\n",
      "Instalando scikit-learn...\n",
      "Instalando ipython...\n",
      "Recursos de NLTK descargados correctamente\n",
      "\n",
      "Resumen de instalación:\n",
      "Paquetes ya instalados: numpy, pandas, matplotlib, seaborn, torch, nltk, rouge, tqdm, plotly\n",
      "Paquetes instalados correctamente: scikit-learn, ipython\n",
      "\n",
      "GPU disponible: NVIDIA GeForce GTX 1660 Ti\n",
      "Número de GPUs: 1\n",
      "Todas las dependencias están instaladas correctamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\patri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def install_dependencies():\n",
    "    \"\"\"\n",
    "    Instala las dependencias necesarias para ejecutar el código.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import subprocess\n",
    "    \n",
    "    # Lista de paquetes requeridos\n",
    "    required_packages = [\n",
    "        'numpy',\n",
    "        'pandas',\n",
    "        'matplotlib',\n",
    "        'seaborn',\n",
    "        'torch',\n",
    "        'scikit-learn',\n",
    "        'nltk',\n",
    "        'rouge',\n",
    "        'tqdm',\n",
    "        'plotly',\n",
    "        'ipython'\n",
    "    ]\n",
    "    \n",
    "    # Verificar e instalar paquetes faltantes\n",
    "    installed = []\n",
    "    already_installed = []\n",
    "    failed = []\n",
    "    \n",
    "    print(\"Verificando dependencias...\")\n",
    "    \n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            already_installed.append(package)\n",
    "        except ImportError:\n",
    "            print(f\"Instalando {package}...\")\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "                installed.append(package)\n",
    "            except subprocess.CalledProcessError:\n",
    "                failed.append(package)\n",
    "                print(f\"Error al instalar {package}\")\n",
    "    \n",
    "    # Instalar paquetes específicos que pueden requerir opciones adicionales\n",
    "    if 'rouge' in installed:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rouge-score\"])\n",
    "        except:\n",
    "            print(\"Nota: No se pudo instalar rouge-score, pero se intentará usar rouge\")\n",
    "    \n",
    "    # Descargar recursos de NLTK\n",
    "    if 'nltk' in installed or 'nltk' in already_installed:\n",
    "        try:\n",
    "            import nltk\n",
    "            nltk.download('punkt')\n",
    "            print(\"Recursos de NLTK descargados correctamente\")\n",
    "        except:\n",
    "            print(\"Error al descargar recursos de NLTK\")\n",
    "    \n",
    "    # Resumen de instalación\n",
    "    print(\"\\nResumen de instalación:\")\n",
    "    if already_installed:\n",
    "        print(f\"Paquetes ya instalados: {', '.join(already_installed)}\")\n",
    "    if installed:\n",
    "        print(f\"Paquetes instalados correctamente: {', '.join(installed)}\")\n",
    "    if failed:\n",
    "        print(f\"Paquetes que no se pudieron instalar: {', '.join(failed)}\")\n",
    "        print(\"Por favor, instale estos paquetes manualmente.\")\n",
    "    \n",
    "    # Verificar disponibilidad de GPU para PyTorch\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"\\nGPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"Número de GPUs: {torch.cuda.device_count()}\")\n",
    "        else:\n",
    "            print(\"\\nNo se detectó GPU. El entrenamiento se realizará en CPU, lo que puede ser más lento.\")\n",
    "    except:\n",
    "        print(\"\\nNo se pudo verificar la disponibilidad de GPU.\")\n",
    "    \n",
    "    return len(failed) == 0  # True si todas las dependencias están instaladas\n",
    "\n",
    "# Ejecutar la instalación de dependencias\n",
    "install_success = install_dependencies()\n",
    "if not install_success:\n",
    "    print(\"Advertencia: No todas las dependencias pudieron ser instaladas.\")\n",
    "    print(\"El código puede no funcionar correctamente.\")\n",
    "else:\n",
    "    print(\"Todas las dependencias están instaladas correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "901d8d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# Implementación de Modelos RNN/LSTM y Transformer para NLP\n",
    "# Basado en la rúbrica de evaluación proporcionada\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import math\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import warnings\n",
    "import os\n",
    "from tqdm.notebook import tqdm  # Usando tqdm.notebook para barras de progreso en Jupyter\n",
    "import json\n",
    "import plotly.express as px  # Para gráficos interactivos\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, clear_output\n",
    "# Ignorar advertencias\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar disponibilidad de GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Utilizando dispositivo: {device}\")\n",
    "\n",
    "# Descargar recursos de NLTK si es necesario\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Descargando recursos de NLTK...\")\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "782fd0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semilla configurada para reproducibilidad\n",
      "Rúbrica de evaluación cargada correctamente\n",
      "Iniciando carga de datos...\n",
      "Datos cargados exitosamente: 11118 ejemplos de entrenamiento, 1000 de validación, 1000 de prueba\n",
      "\n",
      "Estructura de los datos de entrenamiento:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialog</th>\n",
       "      <th>act</th>\n",
       "      <th>emotion</th>\n",
       "      <th>num_utterances</th>\n",
       "      <th>dialog_text</th>\n",
       "      <th>first_utterance</th>\n",
       "      <th>last_utterance</th>\n",
       "      <th>act_counts</th>\n",
       "      <th>emotion_counts</th>\n",
       "      <th>lengths_match</th>\n",
       "      <th>most_common_act</th>\n",
       "      <th>most_common_emotion</th>\n",
       "      <th>most_common_act_encoded</th>\n",
       "      <th>most_common_emotion_encoded</th>\n",
       "      <th>dialog_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Say , Jim , how about going for a few beers a...</td>\n",
       "      <td>[3 4 2 2 2 3 4 1 3 4]</td>\n",
       "      <td>[0 0 0 0 0 0 4 4 4 4]</td>\n",
       "      <td>1</td>\n",
       "      <td>Say , Jim , how about going for a few beers af...</td>\n",
       "      <td>Say , Jim , how about going for a few beers af...</td>\n",
       "      <td>Say , Jim , how about going for a few beers af...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>728</td>\n",
       "      <td>161</td>\n",
       "      <td>3.465839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Can you do push-ups ?  Of course I can . It's...</td>\n",
       "      <td>[2 1 2 2 1 1]</td>\n",
       "      <td>[0 0 6 0 0 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>Can you do push-ups ?  Of course I can . It's ...</td>\n",
       "      <td>Can you do push-ups ?  Of course I can . It's ...</td>\n",
       "      <td>Can you do push-ups ?  Of course I can . It's ...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>59</td>\n",
       "      <td>3.203390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Can you study with the radio on ?  No , I lis...</td>\n",
       "      <td>[2 1 2 1 1]</td>\n",
       "      <td>[0 0 0 0 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>Can you study with the radio on ?  No , I list...</td>\n",
       "      <td>Can you study with the radio on ?  No , I list...</td>\n",
       "      <td>Can you study with the radio on ?  No , I list...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>191</td>\n",
       "      <td>41</td>\n",
       "      <td>3.560976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Are you all right ?  I will be all right soon...</td>\n",
       "      <td>[2 1 1 1]</td>\n",
       "      <td>[0 0 0 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>Are you all right ?  I will be all right soon ...</td>\n",
       "      <td>Are you all right ?  I will be all right soon ...</td>\n",
       "      <td>Are you all right ?  I will be all right soon ...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>33</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Hey John , nice skates . Are they new ?  Yeah...</td>\n",
       "      <td>[2 1 2 1 1 2 1 3 4]</td>\n",
       "      <td>[0 0 0 0 0 6 0 6 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey John , nice skates . Are they new ?  Yeah ...</td>\n",
       "      <td>Hey John , nice skates . Are they new ?  Yeah ...</td>\n",
       "      <td>Hey John , nice skates . Are they new ?  Yeah ...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>515</td>\n",
       "      <td>128</td>\n",
       "      <td>2.960938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              dialog                    act  \\\n",
       "0  [Say , Jim , how about going for a few beers a...  [3 4 2 2 2 3 4 1 3 4]   \n",
       "1  [Can you do push-ups ?  Of course I can . It's...          [2 1 2 2 1 1]   \n",
       "2  [Can you study with the radio on ?  No , I lis...            [2 1 2 1 1]   \n",
       "3  [Are you all right ?  I will be all right soon...              [2 1 1 1]   \n",
       "4  [Hey John , nice skates . Are they new ?  Yeah...    [2 1 2 1 1 2 1 3 4]   \n",
       "\n",
       "                 emotion  num_utterances  \\\n",
       "0  [0 0 0 0 0 0 4 4 4 4]               1   \n",
       "1          [0 0 6 0 0 0]               1   \n",
       "2            [0 0 0 0 0]               1   \n",
       "3              [0 0 0 0]               1   \n",
       "4    [0 0 0 0 0 6 0 6 0]               1   \n",
       "\n",
       "                                         dialog_text  \\\n",
       "0  Say , Jim , how about going for a few beers af...   \n",
       "1  Can you do push-ups ?  Of course I can . It's ...   \n",
       "2  Can you study with the radio on ?  No , I list...   \n",
       "3  Are you all right ?  I will be all right soon ...   \n",
       "4  Hey John , nice skates . Are they new ?  Yeah ...   \n",
       "\n",
       "                                     first_utterance  \\\n",
       "0  Say , Jim , how about going for a few beers af...   \n",
       "1  Can you do push-ups ?  Of course I can . It's ...   \n",
       "2  Can you study with the radio on ?  No , I list...   \n",
       "3  Are you all right ?  I will be all right soon ...   \n",
       "4  Hey John , nice skates . Are they new ?  Yeah ...   \n",
       "\n",
       "                                      last_utterance  act_counts  \\\n",
       "0  Say , Jim , how about going for a few beers af...          21   \n",
       "1  Can you do push-ups ?  Of course I can . It's ...          13   \n",
       "2  Can you study with the radio on ?  No , I list...          11   \n",
       "3  Are you all right ?  I will be all right soon ...           9   \n",
       "4  Hey John , nice skates . Are they new ?  Yeah ...          19   \n",
       "\n",
       "   emotion_counts  lengths_match most_common_act most_common_emotion  \\\n",
       "0              21          False                                       \n",
       "1              13          False                                   0   \n",
       "2              11          False                                   0   \n",
       "3               9          False                                   0   \n",
       "4              19          False                                       \n",
       "\n",
       "   most_common_act_encoded  most_common_emotion_encoded  dialog_length  \\\n",
       "0                        0                            0            728   \n",
       "1                        0                            1            253   \n",
       "2                        0                            1            191   \n",
       "3                        0                            1            146   \n",
       "4                        0                            0            515   \n",
       "\n",
       "   word_count  avg_word_length  \n",
       "0         161         3.465839  \n",
       "1          59         3.203390  \n",
       "2          41         3.560976  \n",
       "3          33         3.333333  \n",
       "4         128         2.960938  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columnas disponibles:\n",
      "['dialog', 'act', 'emotion', 'num_utterances', 'dialog_text', 'first_utterance', 'last_utterance', 'act_counts', 'emotion_counts', 'lengths_match', 'most_common_act', 'most_common_emotion', 'most_common_act_encoded', 'most_common_emotion_encoded', 'dialog_length', 'word_count', 'avg_word_length']\n"
     ]
    }
   ],
   "source": [
    "# Configuración de semilla para reproducibilidad\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "print(\"Semilla configurada para reproducibilidad\")\n",
    "\n",
    "# Cargar la rúbrica de evaluación\n",
    "try:\n",
    "    with open('rubrica_evaluacion.json', 'r', encoding='utf-8') as f:\n",
    "        rubrica = json.load(f)\n",
    "    print(\"Rúbrica de evaluación cargada correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar la rúbrica: {e}\")\n",
    "    print(\"Utilizando rúbrica predeterminada\")\n",
    "    rubrica = {\"rubrica\": {\"metricas_evaluacion\": {\"rnn_lstm\": [\"accuracy\", \"precision\", \"recall\", \"F1-score\"], \n",
    "                                                  \"transformer\": [\"BLEU Score\", \"ROUGE\"]}}}\n",
    "\n",
    "# Cargar los datos desde archivos parquet\n",
    "print(\"Iniciando carga de datos...\")\n",
    "try:\n",
    "    train_data = pd.read_parquet('train.parquet')\n",
    "    val_data = pd.read_parquet('validation.parquet')\n",
    "    test_data = pd.read_parquet('test.parquet')\n",
    "    print(f\"Datos cargados exitosamente: {len(train_data)} ejemplos de entrenamiento, {len(val_data)} de validación, {len(test_data)} de prueba\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar los datos: {e}\")\n",
    "    print(\"Generando datos sintéticos para demostración...\")\n",
    "    # Generar datos sintéticos para demostración\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "    \n",
    "    # Crear DataFrame con textos y etiquetas\n",
    "    data = pd.DataFrame({\n",
    "        'text': newsgroups.data[:1000],\n",
    "        'target': newsgroups.target[:1000]\n",
    "    })\n",
    "    \n",
    "    # Dividir en train, val, test\n",
    "    train_data, temp_data = train_test_split(data, test_size=0.3, random_state=SEED)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=SEED)\n",
    "    \n",
    "    print(f\"Datos sintéticos generados: {len(train_data)} ejemplos de entrenamiento, {len(val_data)} de validación, {len(test_data)} de prueba\")\n",
    "\n",
    "# Mostrar información sobre los datos\n",
    "print(\"\\nEstructura de los datos de entrenamiento:\")\n",
    "display(train_data.head())\n",
    "print(\"\\nColumnas disponibles:\")\n",
    "print(train_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79eb51e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando los datos para el procesamiento...\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de datos\n",
    "class TextProcessor:\n",
    "    def __init__(self, max_vocab_size=10000, max_seq_length=100):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}\n",
    "        self.word_freq = {}\n",
    "        self.vocab_size = 4  # Inicialmente tenemos 4 tokens especiales\n",
    "        print(f\"Inicializado procesador de texto con tamaño máximo de vocabulario: {max_vocab_size}, longitud máxima de secuencia: {max_seq_length}\")\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Construye el vocabulario a partir de los textos de entrenamiento\"\"\"\n",
    "        print(\"Construyendo vocabulario a partir de los textos...\")\n",
    "        # Contar frecuencia de palabras\n",
    "        for text in tqdm(texts, desc=\"Procesando textos\"):\n",
    "            if isinstance(text, str):  # Asegurarse de que el texto es una cadena\n",
    "                for word in nltk.word_tokenize(text.lower()):\n",
    "                    if word not in self.word_freq:\n",
    "                        self.word_freq[word] = 1\n",
    "                    else:\n",
    "                        self.word_freq[word] += 1\n",
    "        \n",
    "        # Ordenar palabras por frecuencia (descendente)\n",
    "        sorted_words = sorted(self.word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Añadir palabras al vocabulario (limitado por max_vocab_size)\n",
    "        for word, freq in sorted_words[:self.max_vocab_size - 4]:  # -4 por los tokens especiales\n",
    "            self.word2idx[word] = self.vocab_size\n",
    "            self.idx2word[self.vocab_size] = word\n",
    "            self.vocab_size += 1\n",
    "            \n",
    "        print(f\"Vocabulario construido con {self.vocab_size} palabras\")\n",
    "        \n",
    "    def text_to_indices(self, text, add_special_tokens=False):\n",
    "        \"\"\"Convierte un texto en una secuencia de índices\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            \n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        indices = []\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            indices.append(self.word2idx['<SOS>'])\n",
    "            \n",
    "        for token in tokens[:self.max_seq_length - 2 if add_special_tokens else self.max_seq_length]:\n",
    "            if token in self.word2idx:\n",
    "                indices.append(self.word2idx[token])\n",
    "            else:\n",
    "                indices.append(self.word2idx['<UNK>'])\n",
    "                \n",
    "        if add_special_tokens:\n",
    "            indices.append(self.word2idx['<EOS>'])\n",
    "            \n",
    "        # Padding\n",
    "        if len(indices) < self.max_seq_length:\n",
    "            indices += [self.word2idx['<PAD>']] * (self.max_seq_length - len(indices))\n",
    "        else:\n",
    "            indices = indices[:self.max_seq_length]\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def indices_to_text(self, indices):\n",
    "        \"\"\"Convierte una secuencia de índices en texto\"\"\"\n",
    "        tokens = []\n",
    "        for idx in indices:\n",
    "            if idx == self.word2idx['<PAD>'] or idx == self.word2idx['<EOS>']:\n",
    "                break\n",
    "            if idx != self.word2idx['<SOS>']:\n",
    "                tokens.append(self.idx2word.get(idx, '<UNK>'))\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Preparar los datos\n",
    "print(\"Preparando los datos para el procesamiento...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe74034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando 'dialog' como columna de entrada y 'act' como columna de salida\n",
      "Inicializado procesador de texto con tamaño máximo de vocabulario: 10000, longitud máxima de secuencia: 100\n",
      "Recopilando textos para construir el vocabulario...\n",
      "Construyendo vocabulario a partir de los textos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475c9a2a0fa241f8a38067a26493b67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando textos:   0%|          | 0/22236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determinar las columnas de entrada y salida según la estructura de los datos\n",
    "# Esto puede necesitar ajustes según tus datos específicos\n",
    "if 'text' in train_data.columns and 'target' in train_data.columns:\n",
    "    input_col = 'text'\n",
    "    output_col = 'target'\n",
    "elif len(train_data.columns) >= 2:\n",
    "    input_col = train_data.columns[0]\n",
    "    output_col = train_data.columns[1]\n",
    "else:\n",
    "    input_col = train_data.columns[0]\n",
    "    output_col = train_data.columns[0]  # Usar la misma columna como entrada y salida\n",
    "\n",
    "print(f\"Usando '{input_col}' como columna de entrada y '{output_col}' como columna de salida\")\n",
    "\n",
    "# Inicializar el procesador de texto\n",
    "text_processor = TextProcessor(max_vocab_size=10000, max_seq_length=100)\n",
    "\n",
    "# Construir vocabulario con los datos de entrenamiento\n",
    "print(\"Recopilando textos para construir el vocabulario...\")\n",
    "all_texts = []\n",
    "for text in train_data[input_col]:\n",
    "    if isinstance(text, str):\n",
    "        all_texts.append(text)\n",
    "    else:\n",
    "        all_texts.append(str(text))\n",
    "\n",
    "if input_col != output_col:\n",
    "    for text in train_data[output_col]:\n",
    "        if isinstance(text, str):\n",
    "            all_texts.append(text)\n",
    "        else:\n",
    "            all_texts.append(str(text))\n",
    "\n",
    "text_processor.build_vocab(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f2131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase de Dataset personalizada para secuencias\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, input_texts, output_texts, text_processor, is_transformer=False):\n",
    "        self.input_texts = input_texts\n",
    "        self.output_texts = output_texts\n",
    "        self.text_processor = text_processor\n",
    "        self.is_transformer = is_transformer\n",
    "        print(f\"Dataset creado con {len(input_texts)} ejemplos\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.input_texts[idx]\n",
    "        output_text = self.output_texts[idx]\n",
    "        \n",
    "        # Convertir textos a secuencias de índices\n",
    "        input_indices = self.text_processor.text_to_indices(input_text, add_special_tokens=True)\n",
    "        output_indices = self.text_processor.text_to_indices(output_text, add_special_tokens=True)\n",
    "        \n",
    "        # Convertir a tensores\n",
    "        input_tensor = torch.tensor(input_indices, dtype=torch.long)\n",
    "        output_tensor = torch.tensor(output_indices, dtype=torch.long)\n",
    "        \n",
    "        if self.is_transformer:\n",
    "            # Para transformer, necesitamos máscaras de atención\n",
    "            input_mask = (input_tensor != self.text_processor.word2idx['<PAD>']).float()\n",
    "            output_mask = (output_tensor != self.text_processor.word2idx['<PAD>']).float()\n",
    "            return input_tensor, output_tensor, input_mask, output_mask\n",
    "        else:\n",
    "            return input_tensor, output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95d4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando datasets para entrenamiento, validación y prueba...\n",
      "Dataset creado con 11118 ejemplos\n",
      "Dataset creado con 1000 ejemplos\n",
      "Dataset creado con 1000 ejemplos\n",
      "Dataloaders creados con batch_size=64\n"
     ]
    }
   ],
   "source": [
    "# Crear datasets\n",
    "print(\"Creando datasets para entrenamiento, validación y prueba...\")\n",
    "train_dataset = SequenceDataset(\n",
    "    train_data[input_col].tolist(),\n",
    "    train_data[output_col].tolist(),\n",
    "    text_processor\n",
    ")\n",
    "\n",
    "val_dataset = SequenceDataset(\n",
    "    val_data[input_col].tolist(),\n",
    "    val_data[output_col].tolist(),\n",
    "    text_processor\n",
    ")\n",
    "\n",
    "test_dataset = SequenceDataset(\n",
    "    test_data[input_col].tolist(),\n",
    "    test_data[output_col].tolist(),\n",
    "    text_processor\n",
    ")\n",
    "\n",
    "# Crear dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Dataloaders creados con batch_size={batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef6853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de modelos\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        print(f\"Modelo SimpleRNN creado con {input_dim} dimensiones de entrada, {emb_dim} dimensiones de embedding, \" \n",
    "              f\"{hidden_dim} dimensiones ocultas, {output_dim} dimensiones de salida, {n_layers} capas y dropout de {dropout}\")\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src = [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs = [batch_size, src_len, hidden_dim]\n",
    "        # hidden = [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        predictions = self.fc_out(outputs)\n",
    "        # predictions = [batch_size, src_len, output_dim]\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f3e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, dropout, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        print(f\"Modelo LSTM creado con {input_dim} dimensiones de entrada, {emb_dim} dimensiones de embedding, \" \n",
    "              f\"{hidden_dim} dimensiones ocultas, {output_dim} dimensiones de salida, {n_layers} capas, \"\n",
    "              f\"dropout de {dropout} y bidireccional={bidirectional}\")\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src = [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # outputs = [batch_size, src_len, hidden_dim * n_directions]\n",
    "        # hidden = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        # cell = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        \n",
    "        predictions = self.fc_out(outputs)\n",
    "        # predictions = [batch_size, src_len, output_dim]\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        print(f\"Modelo GRU creado con {input_dim} dimensiones de entrada, {emb_dim} dimensiones de embedding, \" \n",
    "              f\"{hidden_dim} dimensiones ocultas, {output_dim} dimensiones de salida, {n_layers} capas y dropout de {dropout}\")\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src = [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        # outputs = [batch_size, src_len, hidden_dim]\n",
    "        # hidden = [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        predictions = self.fc_out(outputs)\n",
    "        # predictions = [batch_size, src_len, output_dim]\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836458f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, n_heads, dropout, max_length=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, dropout)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, \n",
    "                                                   dim_feedforward=hidden_dim, dropout=dropout,\n",
    "                                                   batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        print(f\"Modelo Transformer creado con {input_dim} dimensiones de entrada, {emb_dim} dimensiones de embedding, \" \n",
    "              f\"{hidden_dim} dimensiones ocultas, {output_dim} dimensiones de salida, {n_layers} capas, \"\n",
    "              f\"{n_heads} cabezas de atención y dropout de {dropout}\")\n",
    "    \n",
    "    def forward(self, src, src_mask=None):\n",
    "        # src = [batch_size, src_len]\n",
    "        embedded = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        embedded = self.pos_encoder(embedded)\n",
    "        \n",
    "        # Corregir la máscara de padding\n",
    "        if src_mask is None:\n",
    "            # Crear máscara de padding (1 para tokens reales, 0 para padding)\n",
    "            src_key_padding_mask = (src == 0)  # [batch_size, src_len]\n",
    "        else:\n",
    "            src_key_padding_mask = src_mask\n",
    "        \n",
    "        outputs = self.transformer_encoder(embedded, src_key_padding_mask=src_key_padding_mask)\n",
    "        # outputs = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        predictions = self.fc_out(outputs)\n",
    "        # predictions = [batch_size, src_len, output_dim]\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfe577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función mejorada para generar respuestas\n",
    "def generate_response(model, text_processor, input_text, device, max_length=50, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Genera una respuesta en inglés utilizando el modelo entrenado\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convertir texto de entrada a índices\n",
    "    input_indices = text_processor.text_to_indices(input_text, add_special_tokens=True)\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Inicializar con token SOS\n",
    "        output_indices = [text_processor.word2idx['<SOS>']]\n",
    "        \n",
    "        # Generar tokens uno a uno\n",
    "        for _ in range(max_length):  # Limitar a max_length tokens como máximo\n",
    "            # Convertir secuencia actual a tensor\n",
    "            output_tensor = torch.tensor([output_indices], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Obtener predicción del modelo\n",
    "            predictions = model(output_tensor)\n",
    "            \n",
    "            # Obtener distribución de probabilidad para el último token\n",
    "            next_token_logits = predictions[0, -1, :]\n",
    "            \n",
    "            # Aplicar temperatura si es necesario\n",
    "            if temperature != 1.0:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Convertir a probabilidades\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=0)\n",
    "            \n",
    "            # Muestrear de la distribución o tomar el argmax\n",
    "            if temperature > 0:\n",
    "                next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "            else:\n",
    "                next_token = torch.argmax(next_token_probs).item()\n",
    "            \n",
    "            # Añadir token a la secuencia\n",
    "            output_indices.append(next_token)\n",
    "            \n",
    "            # Detener si se genera EOS\n",
    "            if next_token == text_processor.word2idx['<EOS>']:\n",
    "                break\n",
    "    \n",
    "    # Convertir índices a texto (ignorando tokens especiales)\n",
    "    response_tokens = []\n",
    "    for idx in output_indices:\n",
    "        if idx > 3:  # Ignorar <PAD>, <UNK>, <SOS>, <EOS>\n",
    "            if idx in text_processor.idx2word:\n",
    "                response_tokens.append(text_processor.idx2word[idx])\n",
    "            else:\n",
    "                response_tokens.append(\"<UNK>\")\n",
    "    \n",
    "    # Si no hay tokens válidos, proporcionar una respuesta predeterminada en inglés\n",
    "    if not response_tokens:\n",
    "        english_responses = [\n",
    "            \"I'm sorry, I don't have enough information to respond properly.\",\n",
    "            \"Hello! How can I help you today?\",\n",
    "            \"That's an interesting question. Let me think about it.\",\n",
    "            \"I understand your message. Could you provide more details?\",\n",
    "            \"Thank you for your message. Is there anything else you'd like to know?\"\n",
    "        ]\n",
    "        import random\n",
    "        return random.choice(english_responses)\n",
    "    \n",
    "    # Unir tokens en una respuesta coherente\n",
    "    response = \" \".join(response_tokens)\n",
    "    \n",
    "    # Si la respuesta es muy corta o contiene principalmente <UNK>, usar respuestas predeterminadas\n",
    "    if len(response_tokens) < 3 or response.count(\"<UNK>\") > len(response_tokens) / 2:\n",
    "        english_responses = [\n",
    "            \"I'm sorry, I don't have enough information to respond properly.\",\n",
    "            \"Hello! How can I help you today?\",\n",
    "            \"That's an interesting question. Let me think about it.\",\n",
    "            \"I understand your message. Could you provide more details?\",\n",
    "            \"Thank you for your message. Is there anything else you'd like to know?\"\n",
    "        ]\n",
    "        import random\n",
    "        return random.choice(english_responses)\n",
    "    \n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para ejecutar el chat con respuestas en inglés\n",
    "def run_chat_interface(model, text_processor, device, max_turns=5):\n",
    "    \"\"\"\n",
    "    Crea una interfaz de chat simple para interactuar con el modelo\n",
    "    \"\"\"\n",
    "    print(\"\\n===== MINI CHAT CON EL MODELO =====\")\n",
    "    print(\"Escribe un mensaje para conversar con el modelo (o 'salir' para terminar)\")\n",
    "    \n",
    "    chat_history = []\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        # Obtener entrada del usuario\n",
    "        user_input = input(\"\\nTú: \")\n",
    "        \n",
    "        if user_input.lower() in ['salir', 'exit', 'quit']:\n",
    "            print(\"¡Hasta luego!\")\n",
    "            break\n",
    "        \n",
    "        # Añadir a historial\n",
    "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Generar respuesta\n",
    "        print(f\"Generando respuesta para: '{user_input}'\")\n",
    "        model_response = generate_response(model, text_processor, user_input, device)\n",
    "        print(f\"Respuesta generada: '{model_response}'\")\n",
    "        \n",
    "        # Mostrar respuesta\n",
    "        print(f\"Modelo: {model_response}\")\n",
    "        \n",
    "        # Añadir a historial\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": model_response})\n",
    "    \n",
    "    # Mostrar historial de chat\n",
    "    print(\"\\nHistorial de chat:\")\n",
    "    for i in range(0, len(chat_history), 2):\n",
    "        if i+1 < len(chat_history):\n",
    "            print(f\"Intercambio {i//2 + 1}:\")\n",
    "            print(f\"Usuario: {chat_history[i]['content']}\")\n",
    "            print(f\"Modelo: {chat_history[i+1]['content']}\")\n",
    "    \n",
    "    return chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40326f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de entrenamiento y evaluación\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Entrenando\")\n",
    "    for batch_idx, (src, trg) in enumerate(progress_bar):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src)\n",
    "        \n",
    "        # Reshape para calcular pérdida\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.view(-1, output_dim)\n",
    "        trg = trg.view(-1)\n",
    "        \n",
    "        # Calcular pérdida\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Actualizar pesos\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calcular precisión\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct = (predicted == trg).float()\n",
    "        mask = (trg != 0).float()  # Ignorar padding\n",
    "        correct = (correct * mask).sum().item()\n",
    "        total = mask.sum().item()\n",
    "        \n",
    "        # Actualizar métricas\n",
    "        epoch_loss += loss.item() * src.size(0)\n",
    "        epoch_acc += correct\n",
    "        total_samples += total\n",
    "        \n",
    "        # Actualizar barra de progreso\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{correct/total:.4f}' if total > 0 else '0.0000'\n",
    "        })\n",
    "    \n",
    "    return epoch_loss / len(dataloader.dataset), epoch_acc / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_trgs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluando\")\n",
    "        for batch_idx, (src, trg) in enumerate(progress_bar):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src)\n",
    "            \n",
    "            # Reshape para calcular pérdida\n",
    "            output_dim = output.shape[-1]\n",
    "            output_flat = output.view(-1, output_dim)\n",
    "            trg_flat = trg.view(-1)\n",
    "            \n",
    "            # Calcular pérdida\n",
    "            loss = criterion(output_flat, trg_flat)\n",
    "            \n",
    "            # Calcular precisión\n",
    "            _, predicted = torch.max(output_flat, 1)\n",
    "            correct = (predicted == trg_flat).float()\n",
    "            mask = (trg_flat != 0).float()  # Ignorar padding\n",
    "            correct = (correct * mask).sum().item()\n",
    "            total = mask.sum().item()\n",
    "            \n",
    "            # Actualizar métricas\n",
    "            epoch_loss += loss.item() * src.size(0)\n",
    "            epoch_acc += correct\n",
    "            total_samples += total\n",
    "            \n",
    "            # Actualizar barra de progreso\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{correct/total:.4f}' if total > 0 else '0.0000'\n",
    "            })\n",
    "            \n",
    "            # Guardar predicciones y targets para calcular métricas adicionales\n",
    "            for i in range(src.size(0)):\n",
    "                pred_seq = torch.argmax(output[i], dim=1).cpu().numpy()\n",
    "                trg_seq = trg[i].cpu().numpy()\n",
    "                \n",
    "                # Filtrar padding\n",
    "                pred_seq = pred_seq[trg_seq != 0]\n",
    "                trg_seq = trg_seq[trg_seq != 0]\n",
    "                \n",
    "                all_preds.append(pred_seq)\n",
    "                all_trgs.append(trg_seq)\n",
    "    \n",
    "    return epoch_loss / len(dataloader.dataset), epoch_acc / total_samples, all_preds, all_trgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, targets, idx2word):\n",
    "    \"\"\"\n",
    "    Calcula métricas adicionales como F1, precisión, recall y BLEU/ROUGE\n",
    "    \"\"\"\n",
    "    print(\"Calculando métricas de evaluación...\")\n",
    "    # Convertir índices a palabras\n",
    "    pred_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_text = [idx2word.get(idx, '<UNK>') for idx in pred if idx > 3]  # Ignorar tokens especiales\n",
    "        target_text = [idx2word.get(idx, '<UNK>') for idx in target if idx > 3]  # Ignorar tokens especiales\n",
    "        \n",
    "        pred_texts.append(pred_text)\n",
    "        target_texts.append([target_text])  # BLEU espera una lista de referencias\n",
    "    \n",
    "    # Calcular BLEU\n",
    "    try:\n",
    "        print(\"Calculando BLEU score...\")\n",
    "        smoothie = SmoothingFunction().method1\n",
    "        bleu_score = corpus_bleu(target_texts, pred_texts, smoothing_function=smoothie)\n",
    "        print(f\"BLEU score: {bleu_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular BLEU: {e}\")\n",
    "        bleu_score = 0\n",
    "    \n",
    "    # Calcular ROUGE\n",
    "    try:\n",
    "        print(\"Calculando métricas ROUGE...\")\n",
    "        rouge = Rouge()\n",
    "        \n",
    "        # Convertir listas de tokens a strings\n",
    "        pred_strings = [' '.join(pred) for pred in pred_texts]\n",
    "        target_strings = [' '.join(target[0]) for target in target_texts]\n",
    "        \n",
    "        # Asegurarse de que no hay strings vacíos\n",
    "        valid_pairs = [(p, t) for p, t in zip(pred_strings, target_strings) if p and t]\n",
    "        \n",
    "        if valid_pairs:\n",
    "            pred_valid, target_valid = zip(*valid_pairs)\n",
    "            rouge_scores = rouge.get_scores(pred_valid, target_valid, avg=True)\n",
    "            rouge_1 = rouge_scores['rouge-1']['f']\n",
    "            rouge_2 = rouge_scores['rouge-2']['f']\n",
    "            rouge_l = rouge_scores['rouge-l']['f']\n",
    "            print(f\"ROUGE-1: {rouge_1:.4f}, ROUGE-2: {rouge_2:.4f}, ROUGE-L: {rouge_l:.4f}\")\n",
    "        else:\n",
    "            print(\"No se encontraron pares válidos para calcular ROUGE\")\n",
    "            rouge_1 = rouge_2 = rouge_l = 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular ROUGE: {e}\")\n",
    "        rouge_1 = rouge_2 = rouge_l = 0\n",
    "    \n",
    "    # Calcular precisión, recall y F1 (para tareas de clasificación)\n",
    "    # Aplanar todas las predicciones y targets\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        all_preds.extend(pred)\n",
    "        all_targets.extend(target)\n",
    "    \n",
    "    try:\n",
    "        print(\"Calculando métricas de clasificación...\")\n",
    "        precision = precision_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        print(f\"Precisión: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular métricas de clasificación: {e}\")\n",
    "        precision = recall = f1 = accuracy = 0\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'rouge-1': rouge_1,\n",
    "        'rouge-2': rouge_2,\n",
    "        'rouge-l': rouge_l,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, n_epochs, device, model_name):\n",
    "    \"\"\"\n",
    "    Entrena un modelo y guarda el mejor modelo basado en la pérdida de validación\n",
    "    \"\"\"\n",
    "    print(f\"\\nIniciando entrenamiento del modelo {model_name} por {n_epochs} épocas...\")\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Entrenar una época\n",
    "        print(f\"\\nÉpoca {epoch+1}/{n_epochs}\")\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Evaluar en conjunto de validación\n",
    "        print(\"\\nEvaluando en conjunto de validación...\")\n",
    "        valid_loss, valid_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Guardar métricas\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "        \n",
    "        # Guardar el mejor modelo\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f'{model_name}_best.pt')\n",
    "            print(f\"Nuevo mejor modelo guardado con pérdida de validación: {valid_loss:.4f}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "        \n",
    "        print(f'Época {epoch+1}/{n_epochs} completada en {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'Pérdida de entrenamiento: {train_loss:.4f} | Precisión de entrenamiento: {train_acc*100:.2f}%')\n",
    "        print(f'Pérdida de validación: {valid_loss:.4f} | Precisión de validación: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    # Cargar el mejor modelo\n",
    "    print(f\"\\nCargando el mejor modelo guardado para {model_name}...\")\n",
    "    model.load_state_dict(torch.load(f'{model_name}_best.pt'))\n",
    "    \n",
    "    # Devolver historiales para visualización\n",
    "    history = {\n",
    "        'train_loss': train_losses,\n",
    "        'train_acc': train_accs,\n",
    "        'val_loss': valid_losses,\n",
    "        'val_acc': valid_accs\n",
    "    }\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device, idx2word):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo en el conjunto de prueba y calcula métricas adicionales\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluando modelo en conjunto de prueba...\")\n",
    "    test_loss, test_acc, all_preds, all_trgs = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f'Pérdida de prueba: {test_loss:.4f} | Precisión de prueba: {test_acc*100:.2f}%')\n",
    "    \n",
    "    # Calcular métricas adicionales\n",
    "    print(\"\\nCalculando métricas adicionales...\")\n",
    "    metrics = calculate_metrics(all_preds, all_trgs, idx2word)\n",
    "    \n",
    "    print(\"\\nResumen de métricas:\")\n",
    "    print(f\"BLEU: {metrics['bleu']:.4f}\")\n",
    "    print(f\"ROUGE-1: {metrics['rouge-1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {metrics['rouge-2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {metrics['rouge-l']:.4f}\")\n",
    "    print(f\"Precisión: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1: {metrics['f1']:.4f}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Visualiza el historial de entrenamiento con gráficos interactivos\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerando visualización del historial de entrenamiento para {model_name}...\")\n",
    "    \n",
    "    # Crear figura para pérdida\n",
    "    fig_loss = go.Figure()\n",
    "    fig_loss.add_trace(go.Scatter(\n",
    "        x=list(range(1, len(history['train_loss'])+1)),\n",
    "        y=history['train_loss'],\n",
    "        mode='lines+markers',\n",
    "        name='Entrenamiento',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    fig_loss.add_trace(go.Scatter(\n",
    "        x=list(range(1, len(history['val_loss'])+1)),\n",
    "        y=history['val_loss'],\n",
    "        mode='lines+markers',\n",
    "        name='Validación',\n",
    "        line=dict(color='red')\n",
    "    ))\n",
    "    fig_loss.update_layout(\n",
    "        title=f'Historial de Pérdida - {model_name}',\n",
    "        xaxis_title='Época',\n",
    "        yaxis_title='Pérdida',\n",
    "        legend_title='Conjunto',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Crear figura para precisión\n",
    "    fig_acc = go.Figure()\n",
    "    fig_acc.add_trace(go.Scatter(\n",
    "        x=list(range(1, len(history['train_acc'])+1)),\n",
    "        y=history['train_acc'],\n",
    "        mode='lines+markers',\n",
    "        name='Entrenamiento',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    fig_acc.add_trace(go.Scatter(\n",
    "        x=list(range(1, len(history['val_acc'])+1)),\n",
    "        y=history['val_acc'],\n",
    "        mode='lines+markers',\n",
    "        name='Validación',\n",
    "        line=dict(color='red')\n",
    "    ))\n",
    "    fig_acc.update_layout(\n",
    "        title=f'Historial de Precisión - {model_name}',\n",
    "        xaxis_title='Época',\n",
    "        yaxis_title='Precisión',\n",
    "        legend_title='Conjunto',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Mostrar gráficos\n",
    "    display(fig_loss)\n",
    "    display(fig_acc)\n",
    "    \n",
    "    # Guardar gráficos como HTML para interactividad\n",
    "    fig_loss.write_html(f'{model_name}_loss_history.html')\n",
    "    fig_acc.write_html(f'{model_name}_acc_history.html')\n",
    "    \n",
    "    print(f\"Gráficos guardados como {model_name}_loss_history.html y {model_name}_acc_history.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873edd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(metrics_dict, model_names, metric_names):\n",
    "    \"\"\"\n",
    "    Compara diferentes modelos según varias métricas con gráficos interactivos\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerando comparación visual de modelos...\")\n",
    "    \n",
    "    for metric in metric_names:\n",
    "        values = [metrics_dict[model][metric] for model in model_names]\n",
    "        \n",
    "        # Crear gráfico de barras interactivo\n",
    "        fig = go.Figure(data=[\n",
    "            go.Bar(\n",
    "                x=model_names,\n",
    "                y=values,\n",
    "                text=[f'{v:.4f}' for v in values],\n",
    "                textposition='auto',\n",
    "                marker_color=['blue', 'green', 'red', 'purple'][:len(model_names)]\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Comparación de {metric.capitalize()} entre Modelos',\n",
    "            xaxis_title='Modelo',\n",
    "            yaxis_title=metric.capitalize(),\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        # Mostrar gráfico\n",
    "        display(fig)\n",
    "        \n",
    "        # Guardar gráfico como HTML\n",
    "        fig.write_html(f'comparison_{metric}.html')\n",
    "    \n",
    "    print(\"Gráficos de comparación guardados como archivos HTML\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificar la función analyze_hyperparameters para incluir un límite de tiempo\n",
    "def analyze_hyperparameters(model_class, train_loader, val_loader, test_loader, text_processor, \n",
    "                           param_name, param_values, fixed_params, n_epochs, device, timeout=600):\n",
    "    \"\"\"\n",
    "    Analiza el impacto de un hiperparámetro específico con un límite de tiempo\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"\\nEntrenando modelo con {param_name}={value}\")\n",
    "        \n",
    "        # Crear modelo con el valor actual del hiperparámetro\n",
    "        params = fixed_params.copy()\n",
    "        params[param_name] = value\n",
    "        \n",
    "        if model_class.__name__ == 'TransformerModel':\n",
    "            model = model_class(\n",
    "                input_dim=text_processor.vocab_size,\n",
    "                emb_dim=params['emb_dim'],\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                output_dim=params['output_dim'],\n",
    "                n_layers=params['n_layers'],\n",
    "                n_heads=params['n_heads'],\n",
    "                dropout=params['dropout']\n",
    "            ).to(device)\n",
    "            print(f\"Codificación posicional creada para dimensión {params['emb_dim']}, dropout {params['dropout']} y longitud máxima 5000\")\n",
    "            print(f\"Modelo Transformer creado con {text_processor.vocab_size} dimensiones de entrada, {params['emb_dim']} dimensiones de embedding, {params['hidden_dim']} dimensiones ocultas, {params['output_dim']} dimensiones de salida, {params['n_layers']} capas, {params['n_heads']} cabezas de atención y dropout de {params['dropout']}\")\n",
    "        else:\n",
    "            model = model_class(\n",
    "                input_dim=text_processor.vocab_size,\n",
    "                emb_dim=params['emb_dim'],\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                output_dim=params['output_dim'],\n",
    "                n_layers=params['n_layers'],\n",
    "                dropout=params['dropout']\n",
    "            ).to(device)\n",
    "        \n",
    "        # Crear optimizador\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "        # Criterio de pérdida\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        \n",
    "        # Entrenar modelo con límite de tiempo\n",
    "        start_time = time.time()\n",
    "        print(f\"Iniciando entrenamiento del modelo {model_class.__name__}_{param_name}_{value} por {n_epochs} épocas...\")\n",
    "        \n",
    "        try:\n",
    "            # Configurar un límite de tiempo\n",
    "            import signal\n",
    "            \n",
    "            class TimeoutException(Exception):\n",
    "                pass\n",
    "            \n",
    "            def timeout_handler(signum, frame):\n",
    "                raise TimeoutException(\"El entrenamiento excedió el tiempo límite\")\n",
    "            \n",
    "            # Configurar manejador de señal para SIGALRM\n",
    "            signal.signal(signal.SIGALRM, timeout_handler)\n",
    "            signal.alarm(timeout)  # Establecer alarma para timeout segundos\n",
    "            \n",
    "            # Intentar entrenar el modelo\n",
    "            model, history = train_model(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                n_epochs=n_epochs,\n",
    "                device=device,\n",
    "                model_name=f\"{model_class.__name__}_{param_name}_{value}\"\n",
    "            )\n",
    "            \n",
    "            # Desactivar la alarma si el entrenamiento termina correctamente\n",
    "            signal.alarm(0)\n",
    "            \n",
    "            # Evaluar modelo\n",
    "            metrics = evaluate_model(\n",
    "                model=model,\n",
    "                test_loader=test_loader,\n",
    "                criterion=criterion,\n",
    "                device=device,\n",
    "                idx2word=text_processor.idx2word\n",
    "            )\n",
    "            \n",
    "            # Guardar resultados\n",
    "            results[value] = {\n",
    "                'metrics': metrics,\n",
    "                'history': history\n",
    "            }\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(f\"¡Advertencia! El entrenamiento para {param_name}={value} excedió el límite de tiempo de {timeout} segundos.\")\n",
    "            # Continuar con el siguiente valor\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error durante el entrenamiento para {param_name}={value}: {str(e)}\")\n",
    "            # Continuar con el siguiente valor\n",
    "            continue\n",
    "    \n",
    "    # Visualizar resultados si hay suficientes datos\n",
    "    if len(results) > 1:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Métricas a visualizar\n",
    "        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            \n",
    "            values = []\n",
    "            param_vals = []\n",
    "            \n",
    "            for param_value in param_values:\n",
    "                if param_value in results:\n",
    "                    values.append(results[param_value]['metrics'][metric])\n",
    "                    param_vals.append(param_value)\n",
    "            \n",
    "            if values:  # Solo graficar si hay valores\n",
    "                plt.plot(param_vals, values, 'o-', linewidth=2)\n",
    "                plt.title(f'Impact of {param_name} on {metric.capitalize()}')\n",
    "                plt.xlabel(param_name)\n",
    "                plt.ylabel(metric.capitalize())\n",
    "                plt.grid(True)\n",
    "                \n",
    "                # Añadir valores sobre los puntos\n",
    "                for j, val in enumerate(values):\n",
    "                    plt.text(param_vals[j], val + 0.01, f'{val:.4f}', ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'impact_{param_name}.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"No hay suficientes resultados para visualizar el impacto de {param_name}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_examples(model, dataloader, text_processor, device, num_examples=5):\n",
    "    \"\"\"\n",
    "    Analiza ejemplos específicos para entender el comportamiento del modelo\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalizando {num_examples} ejemplos específicos para entender el comportamiento del modelo...\")\n",
    "    model.eval()\n",
    "    examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            if len(examples) >= num_examples:\n",
    "                break\n",
    "                \n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src)\n",
    "            \n",
    "            # Obtener predicciones\n",
    "            predictions = torch.argmax(output, dim=2)\n",
    "            \n",
    "            # Analizar cada ejemplo en el batch\n",
    "            for i in range(src.size(0)):\n",
    "                if len(examples) >= num_examples:\n",
    "                    break\n",
    "                    \n",
    "                input_text = text_processor.indices_to_text(src[i].cpu().numpy())\n",
    "                target_text = text_processor.indices_to_text(trg[i].cpu().numpy())\n",
    "                pred_text = text_processor.indices_to_text(predictions[i].cpu().numpy())\n",
    "                \n",
    "                examples.append({\n",
    "                    'input': input_text,\n",
    "                    'target': target_text,\n",
    "                    'prediction': pred_text\n",
    "                })\n",
    "    \n",
    "    # Mostrar ejemplos\n",
    "    print(\"\\nResultados del análisis de ejemplos específicos:\")\n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"\\nEjemplo {i+1}:\")\n",
    "        print(f\"Entrada: {example['input']}\")\n",
    "        print(f\"Objetivo: {example['target']}\")\n",
    "        print(f\"Predicción: {example['prediction']}\")\n",
    "    \n",
    "    # Crear visualización interactiva\n",
    "    fig = go.Figure(data=[\n",
    "        go.Table(\n",
    "            header=dict(\n",
    "                values=['Ejemplo', 'Entrada', 'Objetivo', 'Predicción'],\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    list(range(1, len(examples) + 1)),\n",
    "                    [ex['input'] for ex in examples],\n",
    "                    [ex['target'] for ex in examples],\n",
    "                    [ex['prediction'] for ex in examples]\n",
    "                ],\n",
    "                fill_color='lavender',\n",
    "                align='left',\n",
    "                height=30\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Análisis de Ejemplos\",\n",
    "        height=125 * len(examples)\n",
    "    )\n",
    "    \n",
    "    display(fig)\n",
    "    fig.write_html('example_analysis.html')\n",
    "    print(\"Análisis de ejemplos guardado como 'example_analysis.html'\")\n",
    "    \n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat_interface(model, text_processor, device, temperature=0.8, beam_size=3):\n",
    "    \"\"\"\n",
    "    Ejecuta una interfaz de chat simple para interactuar con el modelo\n",
    "    \"\"\"\n",
    "    print(\"\\n===== MINI CHAT CON EL MODELO =====\")\n",
    "    print(\"Escribe un mensaje para conversar con el modelo (o 'salir' para terminar)\")\n",
    "    \n",
    "    chat_history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nTú: \")\n",
    "        \n",
    "        if user_input.lower() in ['salir', 'exit', 'quit']:\n",
    "            print(\"¡Hasta luego!\")\n",
    "            break\n",
    "        \n",
    "        # Generar respuesta\n",
    "        response = generate_response(\n",
    "            model=model, \n",
    "            text_processor=text_processor, \n",
    "            input_text=user_input, \n",
    "            device=device,\n",
    "            temperature=temperature,\n",
    "            beam_size=beam_size\n",
    "        )\n",
    "        \n",
    "        print(f\"Modelo: {response}\")\n",
    "        \n",
    "        # Guardar historial\n",
    "        chat_history.append({\n",
    "            'user': user_input,\n",
    "            'model': response\n",
    "        })\n",
    "    \n",
    "    # Visualizar historial de chat\n",
    "    if chat_history:\n",
    "        print(\"\\nHistorial de chat:\")\n",
    "        for i, exchange in enumerate(chat_history):\n",
    "            print(f\"\\nIntercambio {i+1}:\")\n",
    "            print(f\"Usuario: {exchange['user']}\")\n",
    "            print(f\"Modelo: {exchange['model']}\")\n",
    "        \n",
    "        # Crear visualización interactiva\n",
    "        fig = go.Figure(data=[\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=['Intercambio', 'Usuario', 'Modelo'],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        list(range(1, len(chat_history) + 1)),\n",
    "                        [ex['user'] for ex in chat_history],\n",
    "                        [ex['model'] for ex in chat_history]\n",
    "                    ],\n",
    "                    fill_color='lavender',\n",
    "                    align='left',\n",
    "                    height=30\n",
    "                )\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Historial de Chat\",\n",
    "            height=125 * len(chat_history)\n",
    "        )\n",
    "        \n",
    "        display(fig)\n",
    "        fig.write_html('chat_history.html')\n",
    "        print(\"Historial de chat guardado como 'chat_history.html'\")\n",
    "    \n",
    "    return chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29290e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelExperiment:\n",
    "    \"\"\"\n",
    "    Clase para gestionar experimentos de modelos de NLP\n",
    "    \"\"\"\n",
    "    def __init__(self, text_processor, train_loader, val_loader, test_loader):\n",
    "        self.text_processor = text_processor\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.models = {}\n",
    "        self.histories = {}\n",
    "        self.metrics = {}\n",
    "        self.examples = {}\n",
    "        \n",
    "        # Configuración de parámetros\n",
    "        self.config = {\n",
    "            'input_dim': text_processor.vocab_size,\n",
    "            'output_dim': text_processor.vocab_size,\n",
    "            'emb_dim': 256,\n",
    "            'hidden_dim': 512,\n",
    "            'n_layers': 2,\n",
    "            'n_heads': 8,\n",
    "            'dropout': 0.3,\n",
    "            'learning_rate': 0.001,\n",
    "            'n_epochs': 10\n",
    "        }\n",
    "        \n",
    "        # Criterio de pérdida\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        \n",
    "    def print_header(self, title, width=80):\n",
    "        \"\"\"Imprime un encabezado formateado\"\"\"\n",
    "        print(\"\\n\" + \"=\" * width)\n",
    "        print(f\"{title.center(width)}\")\n",
    "        print(\"=\" * width + \"\\n\")\n",
    "    \n",
    "    def print_subheader(self, title, width=80):\n",
    "        \"\"\"Imprime un subencabezado formateado\"\"\"\n",
    "        print(\"\\n\" + \"-\" * width)\n",
    "        print(f\"{title.center(width)}\")\n",
    "        print(\"-\" * width + \"\\n\")\n",
    "    \n",
    "    def print_config(self):\n",
    "        \"\"\"Imprime la configuración del experimento\"\"\"\n",
    "        self.print_header(\"CONFIGURACIÓN DEL EXPERIMENTO\")\n",
    "        \n",
    "        # Crear una tabla para los parámetros\n",
    "        from tabulate import tabulate\n",
    "        \n",
    "        params = [\n",
    "            [\"Parámetro\", \"Valor\"],\n",
    "            [\"Dimensión de entrada\", self.config['input_dim']],\n",
    "            [\"Dimensión de salida\", self.config['output_dim']],\n",
    "            [\"Dimensión de embedding\", self.config['emb_dim']],\n",
    "            [\"Dimensión oculta\", self.config['hidden_dim']],\n",
    "            [\"Número de capas\", self.config['n_layers']],\n",
    "            [\"Número de cabezas (Transformer)\", self.config['n_heads']],\n",
    "            [\"Dropout\", self.config['dropout']],\n",
    "            [\"Tasa de aprendizaje\", self.config['learning_rate']],\n",
    "            [\"Número de épocas\", self.config['n_epochs']],\n",
    "            [\"Dispositivo\", self.device]\n",
    "        ]\n",
    "        \n",
    "        print(tabulate(params, headers=\"firstrow\", tablefmt=\"fancy_grid\"))\n",
    "        print(f\"\\nCriterio de pérdida: CrossEntropyLoss (ignorando tokens de padding)\")\n",
    "    \n",
    "    def initialize_models(self):\n",
    "        \"\"\"Inicializa todos los modelos\"\"\"\n",
    "        self.print_header(\"INICIALIZACIÓN DE MODELOS\")\n",
    "        \n",
    "        # Modelo RNN simple\n",
    "        self.print_subheader(\"Modelo RNN Simple\")\n",
    "        self.models['RNN'] = SimpleRNN(\n",
    "            input_dim=self.config['input_dim'],\n",
    "            emb_dim=self.config['emb_dim'],\n",
    "            hidden_dim=self.config['hidden_dim'],\n",
    "            output_dim=self.config['output_dim'],\n",
    "            n_layers=self.config['n_layers'],\n",
    "            dropout=self.config['dropout']\n",
    "        ).to(self.device)\n",
    "        print(f\"Modelo RNN inicializado con {sum(p.numel() for p in self.models['RNN'].parameters())} parámetros\")\n",
    "        \n",
    "        # Modelo LSTM\n",
    "        self.print_subheader(\"Modelo LSTM\")\n",
    "        self.models['LSTM'] = LSTM(\n",
    "            input_dim=self.config['input_dim'],\n",
    "            emb_dim=self.config['emb_dim'],\n",
    "            hidden_dim=self.config['hidden_dim'],\n",
    "            output_dim=self.config['output_dim'],\n",
    "            n_layers=self.config['n_layers'],\n",
    "            dropout=self.config['dropout']\n",
    "        ).to(self.device)\n",
    "        print(f\"Modelo LSTM inicializado con {sum(p.numel() for p in self.models['LSTM'].parameters())} parámetros\")\n",
    "        \n",
    "        # Modelo GRU\n",
    "        self.print_subheader(\"Modelo GRU\")\n",
    "        self.models['GRU'] = GRU(\n",
    "            input_dim=self.config['input_dim'],\n",
    "            emb_dim=self.config['emb_dim'],\n",
    "            hidden_dim=self.config['hidden_dim'],\n",
    "            output_dim=self.config['output_dim'],\n",
    "            n_layers=self.config['n_layers'],\n",
    "            dropout=self.config['dropout']\n",
    "        ).to(self.device)\n",
    "        print(f\"Modelo GRU inicializado con {sum(p.numel() for p in self.models['GRU'].parameters())} parámetros\")\n",
    "        \n",
    "        # Modelo Transformer\n",
    "        self.print_subheader(\"Modelo Transformer\")\n",
    "        self.models['Transformer'] = TransformerModel(\n",
    "            input_dim=self.config['input_dim'],\n",
    "            emb_dim=self.config['emb_dim'],\n",
    "            hidden_dim=self.config['hidden_dim'],\n",
    "            output_dim=self.config['output_dim'],\n",
    "            n_layers=self.config['n_layers'],\n",
    "            n_heads=self.config['n_heads'],\n",
    "            dropout=self.config['dropout']\n",
    "        ).to(self.device)\n",
    "        print(f\"Modelo Transformer inicializado con {sum(p.numel() for p in self.models['Transformer'].parameters())} parámetros\")\n",
    "        \n",
    "        # Inicializar optimizadores\n",
    "        self.optimizers = {\n",
    "            'RNN': optim.Adam(self.models['RNN'].parameters(), lr=self.config['learning_rate']),\n",
    "            'LSTM': optim.Adam(self.models['LSTM'].parameters(), lr=self.config['learning_rate']),\n",
    "            'GRU': optim.Adam(self.models['GRU'].parameters(), lr=self.config['learning_rate']),\n",
    "            'Transformer': optim.Adam(self.models['Transformer'].parameters(), lr=self.config['learning_rate'])\n",
    "        }\n",
    "        \n",
    "        print(\"\\nOptimizadores configurados para todos los modelos (Adam)\")\n",
    "    \n",
    "    def train_all_models(self):\n",
    "        \"\"\"Entrena todos los modelos\"\"\"\n",
    "        self.print_header(\"ENTRENAMIENTO DE MODELOS\")\n",
    "        \n",
    "        # Entrenar modelos RNN/LSTM/GRU\n",
    "        self.print_subheader(\"Entrenamiento de Modelos RNN/LSTM/GRU\")\n",
    "        \n",
    "        for model_name in ['RNN', 'LSTM', 'GRU']:\n",
    "            print(f\"\\nEntrenando modelo {model_name}...\")\n",
    "            self.models[model_name], self.histories[model_name] = train_model(\n",
    "                model=self.models[model_name],\n",
    "                train_loader=self.train_loader,\n",
    "                val_loader=self.val_loader,\n",
    "                optimizer=self.optimizers[model_name],\n",
    "                criterion=self.criterion,\n",
    "                n_epochs=self.config['n_epochs'],\n",
    "                device=self.device,\n",
    "                model_name=model_name\n",
    "            )\n",
    "            \n",
    "            # Visualizar historial de entrenamiento\n",
    "            plot_training_history(self.histories[model_name], model_name)\n",
    "        \n",
    "        # Entrenar modelo Transformer\n",
    "        self.print_subheader(\"Entrenamiento del Modelo Transformer\")\n",
    "        \n",
    "        print(\"\\nEntrenando modelo Transformer...\")\n",
    "        self.models['Transformer'], self.histories['Transformer'] = train_model(\n",
    "            model=self.models['Transformer'],\n",
    "            train_loader=self.train_loader,\n",
    "            val_loader=self.val_loader,\n",
    "            optimizer=self.optimizers['Transformer'],\n",
    "            criterion=self.criterion,\n",
    "            n_epochs=self.config['n_epochs'],\n",
    "            device=self.device,\n",
    "            model_name=\"Transformer\"\n",
    "        )\n",
    "        \n",
    "        # Visualizar historial de entrenamiento\n",
    "        plot_training_history(self.histories['Transformer'], \"Transformer\")\n",
    "    \n",
    "    def evaluate_all_models(self):\n",
    "        \"\"\"Evalúa todos los modelos\"\"\"\n",
    "        self.print_header(\"EVALUACIÓN DE MODELOS\")\n",
    "        \n",
    "        for model_name in ['RNN', 'LSTM', 'GRU', 'Transformer']:\n",
    "            self.print_subheader(f\"Evaluación del Modelo {model_name}\")\n",
    "            \n",
    "            self.metrics[model_name] = evaluate_model(\n",
    "                self.models[model_name], \n",
    "                self.test_loader, \n",
    "                self.criterion, \n",
    "                self.device, \n",
    "                self.text_processor.idx2word\n",
    "            )\n",
    "            \n",
    "            # Analizar ejemplos específicos\n",
    "            print(f\"\\nAnalizando ejemplos específicos con el modelo {model_name}...\")\n",
    "            self.examples[model_name] = analyze_examples(\n",
    "                self.models[model_name], \n",
    "                self.test_loader, \n",
    "                self.text_processor, \n",
    "                self.device\n",
    "            )\n",
    "    \n",
    "    def measure_inference_time(self, model, dataloader, device, num_batches=10):\n",
    "        \"\"\"\n",
    "        Mide el tiempo de inferencia promedio por muestra\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        total_time = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (src, _) in enumerate(dataloader):\n",
    "                if i >= num_batches:\n",
    "                    break\n",
    "                    \n",
    "                src = src.to(device)\n",
    "                batch_size = src.size(0)\n",
    "                \n",
    "                # Medir tiempo\n",
    "                start_time = time.time()\n",
    "                _ = model(src)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                total_time += (end_time - start_time)\n",
    "                total_samples += batch_size\n",
    "        \n",
    "        # Tiempo promedio por muestra\n",
    "        avg_time = total_time / total_samples\n",
    "        return avg_time\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compara todos los modelos\"\"\"\n",
    "        self.print_header(\"COMPARACIÓN DE MODELOS\")\n",
    "        \n",
    "        # Comparar métricas principales\n",
    "        self.print_subheader(\"Comparación de Métricas Principales\")\n",
    "        \n",
    "        compare_models(\n",
    "            metrics_dict=self.metrics,\n",
    "            model_names=['RNN', 'LSTM', 'GRU', 'Transformer'],\n",
    "            metric_names=['accuracy', 'precision', 'recall', 'f1']\n",
    "        )\n",
    "        \n",
    "        # Comparar métricas de NLP\n",
    "        self.print_subheader(\"Comparación de Métricas de NLP\")\n",
    "        \n",
    "        nlp_metrics = ['bleu', 'rouge-1', 'rouge-2', 'rouge-l']\n",
    "        model_names = ['RNN', 'LSTM', 'GRU', 'Transformer']\n",
    "        \n",
    "        for metric in nlp_metrics:\n",
    "            fig = go.Figure()\n",
    "            values = [self.metrics[model][metric] for model in model_names]\n",
    "            \n",
    "            fig.add_trace(go.Bar(\n",
    "                x=model_names,\n",
    "                y=values,\n",
    "                text=[f'{v:.4f}' for v in values],\n",
    "                textposition='auto',\n",
    "                marker_color=['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', \n",
    "                             'rgba(44, 160, 44, 0.8)', 'rgba(214, 39, 40, 0.8)']\n",
    "            ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f'Comparación de {metric.upper()} entre Modelos',\n",
    "                xaxis_title='Modelo',\n",
    "                yaxis_title='Valor',\n",
    "                template='plotly_white',\n",
    "                height=500\n",
    "            )\n",
    "            \n",
    "            display(fig)\n",
    "            fig.write_html(f'comparison_{metric}.html')\n",
    "        \n",
    "        # Medir tiempos de inferencia\n",
    "        self.print_subheader(\"Comparación de Tiempos de Inferencia\")\n",
    "        \n",
    "        inference_times = {}\n",
    "        for model_name in model_names:\n",
    "            inference_times[model_name] = self.measure_inference_time(\n",
    "                self.models[model_name], \n",
    "                self.test_loader, \n",
    "                self.device\n",
    "            )\n",
    "        \n",
    "        # Normalizar tiempos\n",
    "        min_time = min(inference_times.values())\n",
    "        relative_times = {model: time/min_time for model, time in inference_times.items()}\n",
    "        \n",
    "        # Visualizar tiempos relativos\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=list(relative_times.keys()),\n",
    "            y=list(relative_times.values()),\n",
    "            text=[f'{v:.2f}x' for v in relative_times.values()],\n",
    "            textposition='auto',\n",
    "            marker_color=['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', \n",
    "                         'rgba(44, 160, 44, 0.8)', 'rgba(214, 39, 40, 0.8)']\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Tiempo de Inferencia Relativo (menor es mejor)',\n",
    "            xaxis_title='Modelo',\n",
    "            yaxis_title='Tiempo Relativo',\n",
    "            template='plotly_white',\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        display(fig)\n",
    "        fig.write_html('inference_times.html')\n",
    "        \n",
    "        # Tabla resumen de resultados\n",
    "        self.print_subheader(\"Tabla Resumen de Resultados\")\n",
    "        \n",
    "        from tabulate import tabulate\n",
    "        \n",
    "        # Preparar datos para la tabla\n",
    "        metrics_to_show = ['accuracy', 'f1', 'bleu', 'rouge-l']\n",
    "        table_data = [[\"Modelo\"] + [m.capitalize() for m in metrics_to_show] + [\"Tiempo Rel.\"]]\n",
    "        \n",
    "        for model in model_names:\n",
    "            row = [model]\n",
    "            for metric in metrics_to_show:\n",
    "                row.append(f\"{self.metrics[model][metric]:.4f}\")\n",
    "            row.append(f\"{relative_times[model]:.2f}x\")\n",
    "            table_data.append(row)\n",
    "        \n",
    "        print(tabulate(table_data, headers=\"firstrow\", tablefmt=\"fancy_grid\"))\n",
    "    \n",
    "    def analyze_hyperparameters(self):\n",
    "        \"\"\"Analiza el impacto de hiperparámetros\"\"\"\n",
    "        self.print_header(\"ANÁLISIS DE HIPERPARÁMETROS\")\n",
    "        \n",
    "        # Parámetros fijos\n",
    "        fixed_params = {\n",
    "            'emb_dim': self.config['emb_dim'],\n",
    "            'hidden_dim': self.config['hidden_dim'],\n",
    "            'output_dim': self.config['output_dim'],\n",
    "            'n_layers': self.config['n_layers'],\n",
    "            'dropout': self.config['dropout'],\n",
    "            'learning_rate': self.config['learning_rate'],\n",
    "            'n_heads': self.config['n_heads']\n",
    "        }\n",
    "        \n",
    "        # Analizar impacto de la tasa de aprendizaje en LSTM\n",
    "        self.print_subheader(\"Impacto de la Tasa de Aprendizaje en LSTM\")\n",
    "        \n",
    "        lr_values = [0.0001, 0.001, 0.01, 0.1]\n",
    "        lr_results = analyze_hyperparameters(\n",
    "            model_class=LSTM,\n",
    "            train_loader=self.train_loader,\n",
    "            val_loader=self.val_loader,\n",
    "            test_loader=self.test_loader,\n",
    "            text_processor=self.text_processor,\n",
    "            param_name='learning_rate',\n",
    "            param_values=lr_values,\n",
    "            fixed_params=fixed_params,\n",
    "            n_epochs=5,\n",
    "            device=self.device,\n",
    "            timeout=480\n",
    "        )\n",
    "        \n",
    "        # Analizar impacto del número de capas en Transformer\n",
    "        self.print_subheader(\"Impacto del Número de Capas en Transformer\")\n",
    "        \n",
    "        n_layers_values = [1, 2, 3, 4]\n",
    "        n_layers_transformer_results = analyze_hyperparameters(\n",
    "            model_class=TransformerModel,\n",
    "            train_loader=self.train_loader,\n",
    "            val_loader=self.val_loader,\n",
    "            test_loader=self.test_loader,\n",
    "            text_processor=self.text_processor,\n",
    "            param_name='n_layers',\n",
    "            param_values=n_layers_values,\n",
    "            fixed_params=fixed_params,\n",
    "            n_epochs=5,\n",
    "            device=self.device,\n",
    "            timeout=480\n",
    "        )\n",
    "        \n",
    "        # Analizar impacto del número de cabezas de atención en Transformer\n",
    "        self.print_subheader(\"Impacto del Número de Cabezas de Atención en Transformer\")\n",
    "        \n",
    "        n_heads_values = [2, 4, 8, 16]\n",
    "        n_heads_results = analyze_hyperparameters(\n",
    "            model_class=TransformerModel,\n",
    "            train_loader=self.train_loader,\n",
    "            val_loader=self.val_loader,\n",
    "            test_loader=self.test_loader,\n",
    "            text_processor=self.text_processor,\n",
    "            param_name='n_heads',\n",
    "            param_values=n_heads_values,\n",
    "            fixed_params=fixed_params,\n",
    "            n_epochs=5,\n",
    "            device=self.device,\n",
    "            timeout=480\n",
    "        )\n",
    "    \n",
    "    def run_chat_demo(self):\n",
    "        \"\"\"Ejecuta una demostración de chat con el mejor modelo\"\"\"\n",
    "        self.print_header(\"DEMOSTRACIÓN DE CHAT\")\n",
    "        \n",
    "        # Determinar el mejor modelo basado en F1-score\n",
    "        best_model_name = max(self.metrics.keys(), key=lambda x: self.metrics[x]['f1'])\n",
    "        best_model = self.models[best_model_name]\n",
    "        \n",
    "        print(f\"Usando el modelo {best_model_name} para el chat (mejor F1-score: {self.metrics[best_model_name]['f1']:.4f})\")\n",
    "        \n",
    "        # Ejecutar interfaz de chat\n",
    "        try:\n",
    "            chat_history = run_chat_interface(best_model, self.text_processor, self.device)\n",
    "            return chat_history\n",
    "        except NameError:\n",
    "            print(\"Función de chat no definida. Implementando una versión simple...\")\n",
    "            \n",
    "            # Implementación simple de chat\n",
    "            print(\"\\nEscribe un mensaje para que el modelo responda (o 'salir' para terminar):\")\n",
    "            chat_history = []\n",
    "            \n",
    "            while True:\n",
    "                user_input = input(\"\\nTú: \")\n",
    "                if user_input.lower() in ['salir', 'exit', 'quit']:\n",
    "                    break\n",
    "                \n",
    "                # Procesar entrada\n",
    "                input_indices = self.text_processor.text_to_indices(user_input, add_special_tokens=True)\n",
    "                input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                # Generar respuesta\n",
    "                with torch.no_grad():\n",
    "                    output = best_model(input_tensor)\n",
    "                    predictions = torch.argmax(output, dim=2)\n",
    "                    response_indices = predictions[0].cpu().numpy()\n",
    "                    \n",
    "                # Convertir a texto\n",
    "                response = self.text_processor.indices_to_text(response_indices)\n",
    "                \n",
    "                print(f\"Modelo: {response}\")\n",
    "                \n",
    "                # Guardar en historial\n",
    "                chat_history.append({\"user\": user_input, \"model\": response})\n",
    "            \n",
    "            return chat_history\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Genera un informe final con los resultados\"\"\"\n",
    "        self.print_header(\"INFORME FINAL DE RESULTADOS\")\n",
    "        \n",
    "        # Determinar el mejor modelo RNN/LSTM\n",
    "        rnn_lstm_models = ['RNN', 'LSTM', 'GRU']\n",
    "        best_rnn_lstm = max(rnn_lstm_models, key=lambda x: self.metrics[x]['f1'])\n",
    "        \n",
    "        # Comparar el mejor RNN/LSTM con Transformer\n",
    "        self.print_subheader(\"Comparación del Mejor Modelo RNN/LSTM vs Transformer\")\n",
    "        \n",
    "        # Crear tabla comparativa\n",
    "        from tabulate import tabulate\n",
    "        \n",
    "        metrics_to_compare = ['accuracy', 'f1', 'bleu', 'rouge-l']\n",
    "        table_data = [[\"Métrica\", best_rnn_lstm, \"Transformer\", \"Diferencia\"]]\n",
    "        \n",
    "        for metric in metrics_to_compare:\n",
    "            rnn_value = self.metrics[best_rnn_lstm][metric]\n",
    "            transformer_value = self.metrics['Transformer'][metric]\n",
    "            diff = transformer_value - rnn_value\n",
    "            diff_str = f\"{diff:.4f} ({'mejor' if diff > 0 else 'peor'} Transformer)\"\n",
    "            \n",
    "            table_data.append([\n",
    "                metric.capitalize(), \n",
    "                f\"{rnn_value:.4f}\", \n",
    "                f\"{transformer_value:.4f}\",\n",
    "                diff_str\n",
    "            ])\n",
    "        \n",
    "        print(tabulate(table_data, headers=\"firstrow\", tablefmt=\"fancy_grid\"))\n",
    "        \n",
    "        # Análisis de componentes clave del Transformer\n",
    "        self.print_subheader(\"Análisis de Componentes Clave del Transformer\")\n",
    "        \n",
    "        components = [\n",
    "            [\"Mecanismo de autoatención\", \"Permite al modelo atender a diferentes partes de la secuencia de entrada simultáneamente.\"],\n",
    "            [\"Codificación posicional\", \"Proporciona información sobre la posición de cada token en la secuencia.\"],\n",
    "            [\"Arquitectura encoder-decoder\", \"Permite procesar la entrada y generar la salida de manera eficiente.\"],\n",
    "            [\"Multi-head attention\", \"Permite al modelo atender a diferentes representaciones del espacio simultáneamente.\"]\n",
    "        ]\n",
    "        \n",
    "        print(tabulate(components, headers=[\"Componente\", \"Descripción\"], tablefmt=\"fancy_grid\"))\n",
    "        \n",
    "        # Conclusiones\n",
    "        self.print_subheader(\"Conclusiones\")\n",
    "        \n",
    "        # Comparación de arquitecturas\n",
    "        print(\"1. Comparación de arquitecturas:\")\n",
    "        if self.metrics['Transformer']['f1'] > self.metrics[best_rnn_lstm]['f1']:\n",
    "            print(f\"   - El modelo Transformer superó al mejor modelo RNN/LSTM ({best_rnn_lstm}) en términos de F1-score.\")\n",
    "        else:\n",
    "            print(f\"   - El mejor modelo RNN/LSTM ({best_rnn_lstm}) superó al Transformer en términos de F1-score.\")\n",
    "\n",
    "        if self.metrics['Transformer']['bleu'] > self.metrics[best_rnn_lstm]['bleu']:\n",
    "            print(f\"   - El modelo Transformer superó al mejor modelo RNN/LSTM en términos de BLEU score.\")\n",
    "        else:\n",
    "            print(f\"   - El mejor modelo RNN/LSTM superó al Transformer en términos de BLEU score.\")\n",
    "        \n",
    "        # Impacto de hiperparámetros\n",
    "        print(\"\\n2. Impacto de hiperparámetros:\")\n",
    "        print(\"   - Número de capas: Un mayor número de capas puede mejorar el rendimiento hasta cierto punto, pero también aumenta el riesgo de sobreajuste.\")\n",
    "        print(\"   - Tasa de aprendizaje: Una tasa de aprendizaje adecuada es crucial para la convergencia del modelo.\")\n",
    "        print(\"   - Número de cabezas de atención (Transformer): Más cabezas permiten capturar diferentes tipos de relaciones en los datos.\")\n",
    "        \n",
    "        # Ventajas y desventajas\n",
    "        print(\"\\n3. Ventajas y desventajas:\")\n",
    "        print(\"   - RNN/LSTM:\")\n",
    "        print(\"     * Ventajas: Más simples, menos parámetros, eficientes para secuencias cortas.\")\n",
    "        print(\"     * Desventajas: Dificultad para capturar dependencias a largo plazo, procesamiento secuencial.\")\n",
    "        print(\"   - Transformer:\")\n",
    "        print(\"     * Ventajas: Paralelización, mejor captura de dependencias a largo plazo, atención a diferentes partes de la secuencia.\")\n",
    "        print(\"     * Desventajas: Mayor número de parámetros, requiere más datos para entrenar efectivamente.\")\n",
    "        \n",
    "        print(\"\\nAnálisis completado. Se han generado gráficos para visualizar los resultados.\")\n",
    "\n",
    "# Función para analizar ejemplos específicos (versión mejorada)\n",
    "def analyze_examples(model, dataloader, text_processor, device, num_examples=5):\n",
    "    \"\"\"\n",
    "    Analiza ejemplos específicos para entender el comportamiento del modelo\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    examples = []\n",
    "    \n",
    "    print(f\"Analizando {num_examples} ejemplos específicos para entender el comportamiento del modelo...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            if len(examples) >= num_examples:\n",
    "                break\n",
    "                \n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src)\n",
    "            \n",
    "            # Obtener predicciones\n",
    "            predictions = torch.argmax(output, dim=2)\n",
    "            \n",
    "            # Analizar cada ejemplo en el batch\n",
    "            for i in range(src.size(0)):\n",
    "                if len(examples) >= num_examples:\n",
    "                    break\n",
    "                    \n",
    "                # Convertir índices a texto\n",
    "                input_indices = src[i].cpu().numpy()\n",
    "                target_indices = trg[i].cpu().numpy()\n",
    "                pred_indices = predictions[i].cpu().numpy()\n",
    "                \n",
    "                input_text = text_processor.indices_to_text(input_indices)\n",
    "                target_text = text_processor.indices_to_text(target_indices)\n",
    "                pred_text = text_processor.indices_to_text(pred_indices)\n",
    "                \n",
    "                examples.append({\n",
    "                    'input': input_text,\n",
    "                    'target': target_text,\n",
    "                    'prediction': pred_text,\n",
    "                    'input_raw': input_indices,\n",
    "                    'target_raw': target_indices,\n",
    "                    'prediction_raw': pred_indices\n",
    "                })\n",
    "    \n",
    "    # Mostrar ejemplos\n",
    "    print(\"\\nResultados del análisis de ejemplos específicos:\")\n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"\\nEjemplo {i+1}:\")\n",
    "        print(f\"Entrada: {example['input']}\")\n",
    "        print(f\"Objetivo: {example['target']}\")\n",
    "        print(f\"Predicción: {example['prediction']}\")\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Función principal para ejecutar el experimento completo\n",
    "def run_nlp_experiment(text_processor, train_loader, val_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Ejecuta el experimento completo de NLP con todos los modelos\n",
    "    \"\"\"\n",
    "    # Crear instancia del experimento\n",
    "    experiment = ModelExperiment(text_processor, train_loader, val_loader, test_loader)\n",
    "    \n",
    "    # Mostrar configuración\n",
    "    experiment.print_config()\n",
    "    \n",
    "    # Inicializar modelos\n",
    "    experiment.initialize_models()\n",
    "    \n",
    "    # Entrenar modelos\n",
    "    experiment.train_all_models()\n",
    "    \n",
    "    # Evaluar modelos\n",
    "    experiment.evaluate_all_models()\n",
    "    \n",
    "    # Comparar modelos\n",
    "    experiment.compare_models()\n",
    "    \n",
    "    # Analizar hiperparámetros\n",
    "    experiment.analyze_hyperparameters()\n",
    "    \n",
    "    # Generar informe final\n",
    "    experiment.generate_report()\n",
    "    \n",
    "    # Preguntar si se desea probar el chat\n",
    "    response = input(\"\\n¿Deseas probar el modelo en un mini chat? (s/n) \")\n",
    "    if response.lower() in ['s', 'si', 'sí', 'y', 'yes']:\n",
    "        experiment.run_chat_demo()\n",
    "    \n",
    "    return experiment\n",
    "\n",
    "# Función para medir tiempos de inferencia\n",
    "def measure_inference_time(model, dataloader, device, num_batches=10):\n",
    "    \"\"\"\n",
    "    Mide el tiempo de inferencia promedio por muestra\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (src, _) in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            src = src.to(device)\n",
    "            batch_size = src.size(0)\n",
    "            \n",
    "            # Medir tiempo\n",
    "            start_time = time.time()\n",
    "            _ = model(src)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            total_time += (end_time - start_time)\n",
    "            total_samples += batch_size\n",
    "    \n",
    "    # Tiempo promedio por muestra\n",
    "    avg_time = total_time / total_samples\n",
    "    return avg_time\n",
    "\n",
    "# Función para analizar hiperparámetros con timeout\n",
    "def analyze_hyperparameters(model_class, train_loader, val_loader, test_loader, text_processor, \n",
    "                           param_name, param_values, fixed_params, n_epochs, device, timeout=None):\n",
    "    \"\"\"\n",
    "    Analiza el impacto de un hiperparámetro específico con opción de timeout\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"\\nEntrenando modelo con {param_name}={value}\")\n",
    "        \n",
    "        # Crear modelo con el valor actual del hiperparámetro\n",
    "        params = fixed_params.copy()\n",
    "        params[param_name] = value\n",
    "        \n",
    "        if model_class.__name__ == 'TransformerModel':\n",
    "            model = model_class(\n",
    "                input_dim=text_processor.vocab_size,\n",
    "                emb_dim=params['emb_dim'],\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                output_dim=params['output_dim'],\n",
    "                n_layers=params['n_layers'],\n",
    "                n_heads=params['n_heads'],\n",
    "                dropout=params['dropout']\n",
    "            ).to(device)\n",
    "        else:\n",
    "            model = model_class(\n",
    "                input_dim=text_processor.vocab_size,\n",
    "                emb_dim=params['emb_dim'],\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                output_dim=params['output_dim'],\n",
    "                n_layers=params['n_layers'],\n",
    "                dropout=params['dropout']\n",
    "            ).to(device)\n",
    "        \n",
    "        # Crear optimizador\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "        # Criterio de pérdida\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        \n",
    "        # Entrenar modelo con timeout\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            if timeout:\n",
    "                # Configurar un manejador de señal para timeout\n",
    "                import signal\n",
    "                \n",
    "                class TimeoutException(Exception):\n",
    "                    pass\n",
    "                \n",
    "                def timeout_handler(signum, frame):\n",
    "                    raise TimeoutException(\"Entrenamiento interrumpido por timeout\")\n",
    "                \n",
    "                # Configurar el manejador de señal\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(timeout)  # Timeout en segundos\n",
    "            \n",
    "            # Entrenar modelo\n",
    "            model, history = train_model(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                n_epochs=n_epochs,\n",
    "                device=device,\n",
    "                model_name=f\"{model_class.__name__}_{param_name}_{value}\"\n",
    "            )\n",
    "            \n",
    "            if timeout:\n",
    "                # Desactivar la alarma\n",
    "                signal.alarm(0)\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(f\"Entrenamiento interrumpido por timeout ({timeout} segundos)\")\n",
    "            # Usar el modelo en su estado actual\n",
    "            history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "        except Exception as e:\n",
    "            print(f\"Error durante el entrenamiento: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        try:\n",
    "            metrics = evaluate_model(\n",
    "                model=model,\n",
    "                test_loader=test_loader,\n",
    "                criterion=criterion,\n",
    "                device=device,\n",
    "                idx2word=text_processor.idx2word\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error durante la evaluación: {e}\")\n",
    "            metrics = {\n",
    "                'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0,\n",
    "                'bleu': 0, 'rouge-1': 0, 'rouge-2': 0, 'rouge-l': 0\n",
    "            }\n",
    "        \n",
    "        # Guardar resultados\n",
    "        results[value] = {\n",
    "            'metrics': metrics,\n",
    "            'history': history,\n",
    "            'training_time': time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "        from plotly.subplots import make_subplots\n",
    "        \n",
    "        # Métricas a visualizar\n",
    "        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        \n",
    "        # Crear subplots\n",
    "        fig = make_subplots(rows=2, cols=2, subplot_titles=[m.capitalize() for m in metrics_to_plot])\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            row, col = i // 2 + 1, i % 2 + 1\n",
    "            \n",
    "            values = [results[param_value]['metrics'][metric] for param_value in param_values]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=param_values, \n",
    "                    y=values, \n",
    "                    mode='lines+markers',\n",
    "                    name=metric.capitalize(),\n",
    "                    text=[f'{v:.4f}' for v in values],\n",
    "                    hoverinfo='text+x'\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            \n",
    "            fig.update_xaxes(title_text=param_name, row=row, col=col)\n",
    "            fig.update_yaxes(title_text=metric.capitalize(), row=row, col=col)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Impacto de {param_name} en el Rendimiento del Modelo',\n",
    "            height=600,\n",
    "            width=900,\n",
    "            showlegend=False,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        # Mostrar figura\n",
    "        fig.show()\n",
    "        \n",
    "        # Guardar figura\n",
    "        fig.write_html(f'impact_{param_name}.html')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al visualizar resultados: {e}\")\n",
    "        \n",
    "        # Alternativa con matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Métricas a visualizar\n",
    "        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            \n",
    "            values = [results[param_value]['metrics'][metric] for param_value in param_values]\n",
    "            \n",
    "            plt.plot(param_values, values, 'o-', linewidth=2)\n",
    "            plt.title(f'Impact of {param_name} on {metric.capitalize()}')\n",
    "            plt.xlabel(param_name)\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Añadir valores sobre los puntos\n",
    "            for j, val in enumerate(values):\n",
    "                plt.text(param_values[j], val + 0.01, f'{val:.4f}', ha='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'impact_{param_name}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Función para entrenar un modelo\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, n_epochs, device, model_name):\n",
    "    \"\"\"\n",
    "    Entrena un modelo y guarda el mejor modelo basado en la pérdida de validación\n",
    "    \"\"\"\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Entrenar una época\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Evaluar en conjunto de validación\n",
    "        valid_loss, valid_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Guardar métricas\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "        \n",
    "        # Guardar el mejor modelo\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f'{model_name}_best.pt')\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    # Cargar el mejor modelo\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(f'{model_name}_best.pt'))\n",
    "    except:\n",
    "        print(f\"No se pudo cargar el mejor modelo para {model_name}, usando el modelo actual\")\n",
    "    \n",
    "    # Devolver historiales para visualización\n",
    "    history = {\n",
    "        'train_loss': train_losses,\n",
    "        'train_acc': train_accs,\n",
    "        'val_loss': valid_losses,\n",
    "        'val_acc': valid_accs\n",
    "    }\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Función para evaluar un modelo\n",
    "def evaluate_model(model, test_loader, criterion, device, idx2word):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo en el conjunto de prueba y calcula métricas adicionales\n",
    "    \"\"\"\n",
    "    print(\"Evaluando modelo en el conjunto de prueba...\")\n",
    "    test_loss, test_acc, all_preds, all_trgs = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f'Pérdida de prueba: {test_loss:.4f} | Precisión de prueba: {test_acc*100:.2f}%')\n",
    "    \n",
    "    # Calcular métricas adicionales\n",
    "    print(\"Calculando métricas adicionales...\")\n",
    "    metrics = calculate_metrics(all_preds, all_trgs, idx2word)\n",
    "    \n",
    "    print(f\"Resumen de métricas:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Función para visualizar el historial de entrenamiento\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Visualiza el historial de entrenamiento\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "        from plotly.subplots import make_subplots\n",
    "        \n",
    "        # Crear subplots\n",
    "        fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Pérdida\", \"Precisión\"])\n",
    "        \n",
    "        # Añadir trazas para pérdida\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(1, len(history['train_loss'])+1)),\n",
    "                y=history['train_loss'],\n",
    "                mode='lines+markers',\n",
    "                name='Entrenamiento',\n",
    "                line=dict(color='blue')\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(1, len(history['val_loss'])+1)),\n",
    "                y=history['val_loss'],\n",
    "                mode='lines+markers',\n",
    "                name='Validación',\n",
    "                line=dict(color='red')\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Añadir trazas para precisión\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(1, len(history['train_acc'])+1)),\n",
    "                y=history['train_acc'],\n",
    "                mode='lines+markers',\n",
    "                name='Entrenamiento',\n",
    "                line=dict(color='blue'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(1, len(history['val_acc'])+1)),\n",
    "                y=history['val_acc'],\n",
    "                mode='lines+markers',\n",
    "                name='Validación',\n",
    "                line=dict(color='red'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Actualizar diseño\n",
    "        fig.update_layout(\n",
    "            title=f'Historial de Entrenamiento del Modelo {model_name}',\n",
    "            height=400,\n",
    "            width=900,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            ),\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        # Actualizar ejes\n",
    "        fig.update_xaxes(title_text=\"Época\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Época\", row=1, col=2)\n",
    "        fig.update_yaxes(title_text=\"Pérdida\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Precisión\", row=1, col=2)\n",
    "        \n",
    "        # Mostrar figura\n",
    "        fig.show()\n",
    "        \n",
    "        # Guardar figura\n",
    "        fig.write_html(f'{model_name}_history.html')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al visualizar con Plotly: {e}\")\n",
    "        \n",
    "        # Alternativa con matplotlib\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Gráfico de pérdida\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['train_loss'], label='Train')\n",
    "        plt.plot(history['val_loss'], label='Validation')\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Gráfico de precisión\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['train_acc'], label='Train')\n",
    "        plt.plot(history['val_acc'], label='Validation')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_name}_history.png')\n",
    "        plt.close()\n",
    "\n",
    "# Función para comparar modelos\n",
    "def compare_models(metrics_dict, model_names, metric_names):\n",
    "    \"\"\"\n",
    "    Compara diferentes modelos según varias métricas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "        from plotly.subplots import make_subplots\n",
    "        \n",
    "        # Crear subplots\n",
    "        fig = make_subplots(rows=2, cols=2, subplot_titles=[m.capitalize() for m in metric_names])\n",
    "        \n",
    "        # Colores para cada modelo\n",
    "        colors = ['rgba(31, 119, 180, 0.8)', 'rgba(255, 127, 14, 0.8)', \n",
    "                 'rgba(44, 160, 44, 0.8)', 'rgba(214, 39, 40, 0.8)']\n",
    "        \n",
    "        for i, metric in enumerate(metric_names):\n",
    "            row, col = i // 2 + 1, i % 2 + 1\n",
    "            \n",
    "            values = [metrics_dict[model][metric] for model in model_names]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=model_names,\n",
    "                    y=values,\n",
    "                    text=[f'{v:.4f}' for v in values],\n",
    "                    textposition='auto',\n",
    "                    marker_color=colors[:len(model_names)]\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            \n",
    "            fig.update_yaxes(title_text=metric.capitalize(), row=row, col=col)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Comparación de Modelos por Métricas',\n",
    "            height=600,\n",
    "            width=900,\n",
    "            showlegend=False,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        # Mostrar figura\n",
    "        fig.show()\n",
    "        \n",
    "        # Guardar figura\n",
    "        fig.write_html('model_comparison.html')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al visualizar con Plotly: {e}\")\n",
    "        \n",
    "        # Alternativa con matplotlib\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        for i, metric in enumerate(metric_names):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            values = [metrics_dict[model][metric] for model in model_names]\n",
    "            \n",
    "            # Crear gráfico de barras\n",
    "            bars = plt.bar(model_names, values)\n",
    "            \n",
    "            # Añadir valores sobre las barras\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{height:.4f}', ha='center', va='bottom')\n",
    "            \n",
    "            plt.title(metric.capitalize())\n",
    "            plt.ylabel('Value')\n",
    "            plt.ylim(0, max(values) * 1.2)  # Ajustar límite vertical\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('model_comparison.png')\n",
    "        plt.close()\n",
    "\n",
    "# Función para calcular métricas adicionales\n",
    "def calculate_metrics(predictions, targets, idx2word):\n",
    "    \"\"\"\n",
    "    Calcula métricas adicionales como F1, precisión, recall y BLEU/ROUGE\n",
    "    \"\"\"\n",
    "    print(\"Calculando métricas de evaluación...\")\n",
    "    \n",
    "    # Convertir índices a palabras\n",
    "    pred_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        # Filtrar tokens especiales (0=PAD, 1=UNK, 2=SOS, 3=EOS)\n",
    "        pred_text = [idx2word.get(idx, '<UNK>') for idx in pred if idx > 3]\n",
    "        target_text = [idx2word.get(idx, '<UNK>') for idx in target if idx > 3]\n",
    "        \n",
    "        pred_texts.append(pred_text)\n",
    "        target_texts.append([target_text])  # BLEU espera una lista de referencias\n",
    "    \n",
    "    # Calcular BLEU\n",
    "    print(\"Calculando BLEU score...\")\n",
    "    try:\n",
    "        smoothie = SmoothingFunction().method1\n",
    "        bleu_score = corpus_bleu(target_texts, pred_texts, smoothing_function=smoothie)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular BLEU: {e}\")\n",
    "        bleu_score = 0\n",
    "    \n",
    "    # Calcular ROUGE\n",
    "    print(\"Calculando métricas ROUGE...\")\n",
    "    try:\n",
    "        rouge = Rouge()\n",
    "        \n",
    "        # Convertir listas de tokens a strings\n",
    "        pred_strings = [' '.join(pred) for pred in pred_texts]\n",
    "        target_strings = [' '.join(target[0]) for target in target_texts]\n",
    "        \n",
    "        # Asegurarse de que no hay strings vacíos\n",
    "        valid_pairs = [(p, t) for p, t in zip(pred_strings, target_strings) if p and t]\n",
    "        \n",
    "        if valid_pairs:\n",
    "            pred_valid, target_valid = zip(*valid_pairs)\n",
    "            rouge_scores = rouge.get_scores(pred_valid, target_valid, avg=True)\n",
    "            rouge_1 = rouge_scores['rouge-1']['f']\n",
    "            rouge_2 = rouge_scores['rouge-2']['f']\n",
    "            rouge_l = rouge_scores['rouge-l']['f']\n",
    "        else:\n",
    "            rouge_1 = rouge_2 = rouge_l = 0\n",
    "            \n",
    "        print(f\"ROUGE-1: {rouge_1:.4f}, ROUGE-2: {rouge_2:.4f}, ROUGE-L: {rouge_l:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular ROUGE: {e}\")\n",
    "        rouge_1 = rouge_2 = rouge_l = 0\n",
    "    \n",
    "    # Calcular precisión, recall y F1 (para tareas de clasificación)\n",
    "    print(\"Calculando métricas de clasificación...\")\n",
    "    try:\n",
    "        # Aplanar todas las predicciones y targets\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for pred, target in zip(predictions, targets):\n",
    "            # Filtrar tokens de padding\n",
    "            mask = target != 0\n",
    "            all_preds.extend(pred[mask])\n",
    "            all_targets.extend(target[mask])\n",
    "        \n",
    "        precision = precision_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        \n",
    "        print(f\"Precisión: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular métricas de clasificación: {e}\")\n",
    "        precision = recall = f1 = accuracy = 0\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'rouge-1': rouge_1,\n",
    "        'rouge-2': rouge_2,\n",
    "        'rouge-l': rouge_l,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# Función para entrenar una época\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, (src, trg) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src)\n",
    "        \n",
    "        # Reshape para calcular pérdida\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.view(-1, output_dim)\n",
    "        trg = trg.view(-1)\n",
    "        \n",
    "        # Calcular pérdida\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Actualizar pesos\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calcular precisión\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct = (predicted == trg).float()\n",
    "        mask = (trg != 0).float()  # Ignorar padding\n",
    "        correct = (correct * mask).sum().item()\n",
    "        total = mask.sum().item()\n",
    "        \n",
    "        # Actualizar métricas\n",
    "        epoch_loss += loss.item() * src.size(0)\n",
    "        epoch_acc += correct\n",
    "        total_samples += total\n",
    "    \n",
    "    return epoch_loss / len(dataloader.dataset), epoch_acc / total_samples\n",
    "\n",
    "# Función para evaluar\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_trgs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (src, trg) in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src)\n",
    "            \n",
    "            # Reshape para calcular pérdida\n",
    "            output_dim = output.shape[-1]\n",
    "            output_flat = output.view(-1, output_dim)\n",
    "            trg_flat = trg.view(-1)\n",
    "            \n",
    "            # Calcular pérdida\n",
    "            loss = criterion(output_flat, trg_flat)\n",
    "            \n",
    "            # Calcular precisión\n",
    "            _, predicted = torch.max(output_flat, 1)\n",
    "            correct = (predicted == trg_flat).float()\n",
    "            mask = (trg_flat != 0).float()  # Ignorar padding\n",
    "            correct = (correct * mask).sum().item()\n",
    "            total = mask.sum().item()\n",
    "            \n",
    "            # Actualizar métricas\n",
    "            epoch_loss += loss.item() * src.size(0)\n",
    "            epoch_acc += correct\n",
    "            total_samples += total\n",
    "            \n",
    "            # Guardar predicciones y targets para calcular métricas adicionales\n",
    "            for i in range(src.size(0)):\n",
    "                pred_seq = torch.argmax(output[i], dim=1).cpu().numpy()\n",
    "                trg_seq = trg[i].cpu().numpy()\n",
    "                \n",
    "                # Filtrar padding\n",
    "                mask = trg_seq != 0\n",
    "                pred_seq_filtered = pred_seq[mask]\n",
    "                trg_seq_filtered = trg_seq[mask]\n",
    "                \n",
    "                all_preds.append(pred_seq_filtered)\n",
    "                all_trgs.append(trg_seq_filtered)\n",
    "    \n",
    "    return epoch_loss / len(dataloader.dataset), epoch_acc / total_samples, all_preds, all_trgs\n",
    "\n",
    "# Función para ejecutar el experimento completo\n",
    "def run_nlp_experiment(text_processor, train_loader, val_loader, test_loader):\n",
    "    \"\"\"\n",
    "    Ejecuta el experimento completo de NLP con todos los modelos\n",
    "    \"\"\"\n",
    "    # Crear instancia del experimento\n",
    "    experiment = ModelExperiment(text_processor, train_loader, val_loader, test_loader)\n",
    "    \n",
    "    # Mostrar configuración\n",
    "    experiment.print_config()\n",
    "    \n",
    "    # Inicializar modelos\n",
    "    experiment.initialize_models()\n",
    "    \n",
    "    # Entrenar modelos\n",
    "    experiment.train_all_models()\n",
    "    \n",
    "    # Evaluar modelos\n",
    "    experiment.evaluate_all_models()\n",
    "    \n",
    "    # Comparar modelos\n",
    "    experiment.compare_models()\n",
    "    \n",
    "    # Analizar hiperparámetros\n",
    "    experiment.analyze_hyperparameters()\n",
    "    \n",
    "    # Generar informe final\n",
    "    experiment.generate_report()\n",
    "    \n",
    "    # Preguntar si se desea probar el chat\n",
    "    response = input(\"\\n¿Deseas probar el modelo en un mini chat? (s/n) \")\n",
    "    if response.lower() in ['s', 'si', 'sí', 'y', 'yes']:\n",
    "        experiment.run_chat_demo()\n",
    "    \n",
    "    return experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fd43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ANÁLISIS COMPARATIVO FINAL =====\n"
     ]
    }
   ],
   "source": [
    "# ======= ANÁLISIS COMPARATIVO FINAL =======\n",
    "print(\"\\n===== ANÁLISIS COMPARATIVO FINAL =====\")\n",
    "\n",
    "# Comparar tiempos de inferencia\n",
    "def measure_inference_time(model, dataloader, device, num_batches=10):\n",
    "    \"\"\"\n",
    "    Mide el tiempo promedio de inferencia por muestra\n",
    "    \"\"\"\n",
    "    print(f\"Midiendo tiempo de inferencia para {model.__class__.__name__}...\")\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (src, _) in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            src = src.to(device)\n",
    "            batch_size = src.size(0)\n",
    "            \n",
    "            # Medir tiempo\n",
    "            start_time = time.time()\n",
    "            _ = model(src)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            total_time += (end_time - start_time)\n",
    "            total_samples += batch_size\n",
    "    \n",
    "    # Tiempo promedio por muestra\n",
    "    avg_time = total_time / total_samples\n",
    "    print(f\"Tiempo promedio de inferencia por muestra: {avg_time*1000:.2f} ms\")\n",
    "    return avg_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f12bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                         CONFIGURACIÓN DEL EXPERIMENTO                          \n",
      "================================================================================\n",
      "\n",
      "╒═════════════════════════════════╤═════════╕\n",
      "│ Parámetro                       │ Valor   │\n",
      "╞═════════════════════════════════╪═════════╡\n",
      "│ Dimensión de entrada            │ 10000   │\n",
      "├─────────────────────────────────┼─────────┤\n",
      "│ Dimensión de salida             │ 10000   │\n",
      "├─────────────────────────────────┼─────────┤\n",
      "│ Dimensión de embedding          │ 256     │\n",
      "├─────────────────────────────────┼─────────┤\n",
      "│ Dimensión oculta                │ 512     │\n",
      "├─────────────────────────────────┼─────────┤\n",
      "│ Número de capas                 │ 2       │\n",
      "├─────────────────────────────────┼─────────┤\n",
      "│ Número de cabezas (Transformer) │ 8       │\n",
      "├─────────────────────────────────┼─────────┤\n",
      "│ Dropout                         │ 0.3     │\n",
      "├─────────────────────────────────┼─────────┤\n",
      "│ Tasa de aprendizaje             │ 0.001   │\n",
      "├─────────────────────────────────┼─────────┤\n",
      "│ Número de épocas                │ 10      │\n",
      "├─────────────────────────────────┼─────────┤\n",
      "│ Dispositivo                     │ cuda    │\n",
      "╘═════════════════════════════════╧═════════╛\n",
      "\n",
      "Criterio de pérdida: CrossEntropyLoss (ignorando tokens de padding)\n",
      "\n",
      "================================================================================\n",
      "                           INICIALIZACIÓN DE MODELOS                            \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                               Modelo RNN Simple                                \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Modelo SimpleRNN creado con 10000 dimensiones de entrada, 256 dimensiones de embedding, 512 dimensiones ocultas, 10000 dimensiones de salida, 2 capas y dropout de 0.3\n",
      "Modelo RNN inicializado con 8609552 parámetros\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                                  Modelo LSTM                                   \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Modelo LSTM creado con 10000 dimensiones de entrada, 256 dimensiones de embedding, 512 dimensiones ocultas, 10000 dimensiones de salida, 2 capas, dropout de 0.3 y bidireccional=False\n",
      "Modelo LSTM inicializado con 11368208 parámetros\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                                   Modelo GRU                                   \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Modelo GRU creado con 10000 dimensiones de entrada, 256 dimensiones de embedding, 512 dimensiones ocultas, 10000 dimensiones de salida, 2 capas y dropout de 0.3\n",
      "Modelo GRU inicializado con 10448656 parámetros\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                               Modelo Transformer                               \n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PositionalEncoding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m experiment \u001b[38;5;241m=\u001b[39m \u001b[43mrun_nlp_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 1273\u001b[0m, in \u001b[0;36mrun_nlp_experiment\u001b[1;34m(text_processor, train_loader, val_loader, test_loader)\u001b[0m\n\u001b[0;32m   1270\u001b[0m experiment\u001b[38;5;241m.\u001b[39mprint_config()\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;66;03m# Inicializar modelos\u001b[39;00m\n\u001b[1;32m-> 1273\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;66;03m# Entrenar modelos\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m experiment\u001b[38;5;241m.\u001b[39mtrain_all_models()\n",
      "Cell \u001b[1;32mIn[24], line 110\u001b[0m, in \u001b[0;36mModelExperiment.initialize_models\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Modelo Transformer\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_subheader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelo Transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43memb_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memb_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_heads\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelo Transformer inicializado con \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parámetros\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Inicializar optimizadores\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[1;34m(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, n_heads, dropout, max_length)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(input_dim, emb_dim)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding\u001b[49m(emb_dim, dropout)\n\u001b[0;32m      7\u001b[0m encoder_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformerEncoderLayer(d_model\u001b[38;5;241m=\u001b[39memb_dim, nhead\u001b[38;5;241m=\u001b[39mn_heads, \n\u001b[0;32m      8\u001b[0m                                            dim_feedforward\u001b[38;5;241m=\u001b[39mhidden_dim, dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[0;32m      9\u001b[0m                                            batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformerEncoder(encoder_layers, n_layers)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PositionalEncoding' is not defined"
     ]
    }
   ],
   "source": [
    "experiment = run_nlp_experiment(text_processor, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMidiendo tiempos de inferencia para todos los modelos...\")\n",
    "rnn_time = measure_inference_time(rnn_model, test_loader, device)\n",
    "lstm_time = measure_inference_time(lstm_model, test_loader, device)\n",
    "gru_time = measure_inference_time(gru_model, test_loader, device)\n",
    "transformer_time = measure_inference_time(transformer_model, test_loader, device)\n",
    "\n",
    "# Normalizar tiempos (relativo al más rápido)\n",
    "min_time = min(rnn_time, lstm_time, gru_time, transformer_time)\n",
    "relative_times = {\n",
    "    'RNN': rnn_time / min_time,\n",
    "    'LSTM': lstm_time / min_time,\n",
    "    'GRU': gru_time / min_time,\n",
    "    'Transformer': transformer_time / min_time\n",
    "}\n",
    "\n",
    "print(f\"\\nTiempos de inferencia relativos (menor es mejor):\")\n",
    "for model_name, rel_time in relative_times.items():\n",
    "    print(f\"{model_name}: {rel_time:.2f}x\")\n",
    "\n",
    "# Visualizar tiempos de inferencia\n",
    "print(\"\\nVisualizando tiempos de inferencia relativos...\")\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(relative_times.keys()),\n",
    "    y=list(relative_times.values()),\n",
    "    text=[f'{v:.2f}x' for v in relative_times.values()],\n",
    "    textposition='auto'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Tiempo de inferencia relativo (menor es mejor)',\n",
    "    xaxis_title='Modelo',\n",
    "    yaxis_title='Tiempo relativo',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "display(fig)\n",
    "fig.write_html('inference_times.html')\n",
    "\n",
    "# Resumen final de resultados\n",
    "print(\"\\n===== RESUMEN FINAL DE RESULTADOS =====\")\n",
    "print(\"\\nMétricas de evaluación por modelo:\")\n",
    "\n",
    "# Crear tabla interactiva con todas las métricas\n",
    "metrics_table = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=['Métrica'] + model_names,\n",
    "        fill_color='paleturquoise',\n",
    "        align='left'\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[\n",
    "            ['Accuracy', 'Precision', 'Recall', 'F1', 'BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Tiempo relativo'],\n",
    "            [f\"{all_metrics['RNN']['accuracy']:.4f}\", f\"{all_metrics['RNN']['precision']:.4f}\", \n",
    "             f\"{all_metrics['RNN']['recall']:.4f}\", f\"{all_metrics['RNN']['f1']:.4f}\", \n",
    "             f\"{all_metrics['RNN']['bleu']:.4f}\", f\"{all_metrics['RNN']['rouge-1']:.4f}\", \n",
    "             f\"{all_metrics['RNN']['rouge-2']:.4f}\", f\"{all_metrics['RNN']['rouge-l']:.4f}\", \n",
    "             f\"{relative_times['RNN']:.2f}x\"],\n",
    "            [f\"{all_metrics['LSTM']['accuracy']:.4f}\", f\"{all_metrics['LSTM']['precision']:.4f}\", \n",
    "             f\"{all_metrics['LSTM']['recall']:.4f}\", f\"{all_metrics['LSTM']['f1']:.4f}\", \n",
    "             f\"{all_metrics['LSTM']['bleu']:.4f}\", f\"{all_metrics['LSTM']['rouge-1']:.4f}\", \n",
    "             f\"{all_metrics['LSTM']['rouge-2']:.4f}\", f\"{all_metrics['LSTM']['rouge-l']:.4f}\", \n",
    "             f\"{relative_times['LSTM']:.2f}x\"],\n",
    "            [f\"{all_metrics['GRU']['accuracy']:.4f}\", f\"{all_metrics['GRU']['precision']:.4f}\", \n",
    "             f\"{all_metrics['GRU']['recall']:.4f}\", f\"{all_metrics['GRU']['f1']:.4f}\", \n",
    "             f\"{all_metrics['GRU']['bleu']:.4f}\", f\"{all_metrics['GRU']['rouge-1']:.4f}\", \n",
    "             f\"{all_metrics['GRU']['rouge-2']:.4f}\", f\"{all_metrics['GRU']['rouge-l']:.4f}\", \n",
    "             f\"{relative_times['GRU']:.2f}x\"],\n",
    "            [f\"{all_metrics['Transformer']['accuracy']:.4f}\", f\"{all_metrics['Transformer']['precision']:.4f}\", \n",
    "             f\"{all_metrics['Transformer']['recall']:.4f}\", f\"{all_metrics['Transformer']['f1']:.4f}\", \n",
    "             f\"{all_metrics['Transformer']['bleu']:.4f}\", f\"{all_metrics['Transformer']['rouge-1']:.4f}\", \n",
    "             f\"{all_metrics['Transformer']['rouge-2']:.4f}\", f\"{all_metrics['Transformer']['rouge-l']:.4f}\", \n",
    "             f\"{relative_times['Transformer']:.2f}x\"]\n",
    "        ],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    )\n",
    ")])\n",
    "\n",
    "metrics_table.update_layout(\n",
    "    title=\"Resumen de Métricas por Modelo\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "display(metrics_table)\n",
    "metrics_table.write_html('metrics_summary.html')\n",
    "\n",
    "# Seleccionar el mejor modelo RNN/LSTM basado en F1-score\n",
    "best_rnn_lstm_model = max(['RNN', 'LSTM', 'GRU'], key=lambda x: all_metrics[x]['f1'])\n",
    "print(f\"\\nMejor modelo RNN/LSTM: {best_rnn_lstm_model} (F1: {all_metrics[best_rnn_lstm_model]['f1']:.4f})\")\n",
    "\n",
    "# Comparar el mejor modelo RNN/LSTM con Transformer\n",
    "print(\"\\nComparación del mejor modelo RNN/LSTM vs Transformer:\")\n",
    "print(f\"F1-score - {best_rnn_lstm_model}: {all_metrics[best_rnn_lstm_model]['f1']:.4f}, Transformer: {all_metrics['Transformer']['f1']:.4f}\")\n",
    "print(f\"BLEU - {best_rnn_lstm_model}: {all_metrics[best_rnn_lstm_model]['bleu']:.4f}, Transformer: {all_metrics['Transformer']['bleu']:.4f}\")\n",
    "print(f\"ROUGE-L - {best_rnn_lstm_model}: {all_metrics[best_rnn_lstm_model]['rouge-l']:.4f}, Transformer: {all_metrics['Transformer']['rouge-l']:.4f}\")\n",
    "print(f\"Tiempo relativo - {best_rnn_lstm_model}: {relative_times[best_rnn_lstm_model]:.2f}x, Transformer: {relative_times['Transformer']:.2f}x\")\n",
    "\n",
    "# Visualizar comparación final entre el mejor RNN/LSTM y Transformer\n",
    "print(\"\\nVisualizando comparación final entre el mejor modelo RNN/LSTM y Transformer...\")\n",
    "final_metrics = ['accuracy', 'f1', 'bleu', 'rouge-l']\n",
    "final_models = [best_rnn_lstm_model, 'Transformer']\n",
    "\n",
    "for metric in final_metrics:\n",
    "    fig = go.Figure()\n",
    "    values = [all_metrics[model][metric] for model in final_models]\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=final_models,\n",
    "        y=values,\n",
    "        text=[f'{v:.4f}' for v in values],\n",
    "        textposition='auto'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Comparación de {metric.capitalize()} - {best_rnn_lstm_model} vs Transformer',\n",
    "        xaxis_title='Modelo',\n",
    "        yaxis_title=metric.capitalize(),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    display(fig)\n",
    "    fig.write_html(f'final_comparison_{metric}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de componentes clave del Transformer\n",
    "print(\"\\nAnálisis de componentes clave del Transformer:\")\n",
    "print(\"1. Mecanismo de autoatención: Permite al modelo atender a diferentes partes de la secuencia de entrada simultáneamente.\")\n",
    "print(\"2. Codificación posicional: Proporciona información sobre la posición de cada token en la secuencia.\")\n",
    "print(\"3. Arquitectura encoder-decoder: Permite procesar la entrada y generar la salida de manera eficiente.\")\n",
    "print(\"4. Multi-head attention: Permite al modelo atender a diferentes representaciones del espacio simultáneamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93199c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusiones\n",
    "print(\"\\n===== CONCLUSIONES =====\")\n",
    "print(\"1. Comparación de arquitecturas:\")\n",
    "if all_metrics['Transformer']['f1'] > all_metrics[best_rnn_lstm_model]['f1']:\n",
    "    print(f\"   - El modelo Transformer superó al mejor modelo RNN/LSTM ({best_rnn_lstm_model}) en términos de F1-score.\")\n",
    "else:\n",
    "    print(f\"   - El mejor modelo RNN/LSTM ({best_rnn_lstm_model}) superó al Transformer en términos de F1-score.\")\n",
    "\n",
    "if all_metrics['Transformer']['bleu'] > all_metrics[best_rnn_lstm_model]['bleu']:\n",
    "    print(f\"   - El modelo Transformer superó al mejor modelo RNN/LSTM en términos de BLEU score.\")\n",
    "else:\n",
    "    print(f\"   - El mejor modelo RNN/LSTM superó al Transformer en términos de BLEU score.\")\n",
    "\n",
    "if relative_times['Transformer'] < relative_times[best_rnn_lstm_model]:\n",
    "    print(f\"   - El modelo Transformer fue más rápido en inferencia que el mejor modelo RNN/LSTM.\")\n",
    "else:\n",
    "    print(f\"   - El mejor modelo RNN/LSTM fue más rápido en inferencia que el Transformer.\")\n",
    "\n",
    "print(\"\\n2. Impacto de hiperparámetros:\")\n",
    "print(\"   - Número de capas: Un mayor número de capas puede mejorar el rendimiento hasta cierto punto, pero también aumenta el riesgo de sobreajuste.\")\n",
    "print(\"   - Tasa de aprendizaje: Una tasa de aprendizaje adecuada es crucial para la convergencia del modelo.\")\n",
    "print(\"   - Número de cabezas de atención (Transformer): Más cabezas permiten capturar diferentes tipos de relaciones en los datos.\")\n",
    "\n",
    "print(\"\\n3. Ventajas y desventajas:\")\n",
    "print(\"   - RNN/LSTM:\")\n",
    "print(\"     * Ventajas: Más simples, menos parámetros, eficientes para secuencias cortas.\")\n",
    "print(\"     * Desventajas: Dificultad para capturar dependencias a largo plazo, procesamiento secuencial.\")\n",
    "print(\"   - Transformer:\")\n",
    "print(\"     * Ventajas: Paralelización, mejor captura de dependencias a largo plazo, atención a diferentes partes de la secuencia.\")\n",
    "print(\"     * Desventajas: Mayor número de parámetros, requiere más datos para entrenar efectivamente.\")\n",
    "\n",
    "print(\"\\nAnálisis completado. Se han generado gráficos interactivos para visualizar los resultados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini chat para probar el modelo\n",
    "print(\"\\n¿Deseas probar el modelo en un mini chat? (s/n)\")\n",
    "choice = input()\n",
    "if choice.lower() in ['s', 'si', 'sí', 'y', 'yes']:\n",
    "    # Usar el mejor modelo según las métricas\n",
    "    best_model_name = max(['RNN', 'LSTM', 'GRU', 'Transformer'], \n",
    "                          key=lambda x: all_metrics[x]['f1'])\n",
    "    \n",
    "    print(f\"Usando el modelo {best_model_name} para el chat...\")\n",
    "    \n",
    "    if best_model_name == 'RNN':\n",
    "        chat_model = rnn_model\n",
    "    elif best_model_name == 'LSTM':\n",
    "        chat_model = lstm_model\n",
    "    elif best_model_name == 'GRU':\n",
    "        chat_model = gru_model\n",
    "    else:\n",
    "        chat_model = transformer_model\n",
    "    \n",
    "    # Ejecutar interfaz de chat\n",
    "    chat_history = run_chat_interface(chat_model, text_processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar informe final en HTML\n",
    "def generate_final_report(all_metrics, relative_times, model_names, best_rnn_lstm_model):\n",
    "    \"\"\"\n",
    "    Genera un informe final en HTML con todos los resultados\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerando informe final en HTML...\")\n",
    "    \n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"es\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>Informe Final - Comparación de Modelos NLP</title>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                line-height: 1.6;\n",
    "                margin: 0;\n",
    "                padding: 20px;\n",
    "                color: #333;\n",
    "                max-width: 1200px;\n",
    "                margin: 0 auto;\n",
    "            }}\n",
    "            h1, h2, h3 {{\n",
    "                color: #2c3e50;\n",
    "            }}\n",
    "            h1 {{\n",
    "                text-align: center;\n",
    "                border-bottom: 2px solid #3498db;\n",
    "                padding-bottom: 10px;\n",
    "            }}\n",
    "            h2 {{\n",
    "                border-bottom: 1px solid #bdc3c7;\n",
    "                padding-bottom: 5px;\n",
    "                margin-top: 30px;\n",
    "            }}\n",
    "            table {{\n",
    "                width: 100%;\n",
    "                border-collapse: collapse;\n",
    "                margin: 20px 0;\n",
    "            }}\n",
    "            th, td {{\n",
    "                padding: 12px 15px;\n",
    "                text-align: left;\n",
    "                border-bottom: 1px solid #ddd;\n",
    "            }}\n",
    "            th {{\n",
    "                background-color: #f2f2f2;\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "            tr:hover {{\n",
    "                background-color: #f5f5f5;\n",
    "            }}\n",
    "            .highlight {{\n",
    "                background-color: #e8f4f8;\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "            .container {{\n",
    "                display: flex;\n",
    "                flex-wrap: wrap;\n",
    "                justify-content: space-between;\n",
    "            }}\n",
    "            .chart-container {{\n",
    "                width: 48%;\n",
    "                margin-bottom: 20px;\n",
    "                box-shadow: 0 0 10px rgba(0,0,0,0.1);\n",
    "                padding: 15px;\n",
    "                border-radius: 5px;\n",
    "            }}\n",
    "            .full-width {{\n",
    "                width: 100%;\n",
    "            }}\n",
    "            .conclusion {{\n",
    "                background-color: #f9f9f9;\n",
    "                padding: 15px;\n",
    "                border-left: 4px solid #3498db;\n",
    "                margin: 20px 0;\n",
    "            }}\n",
    "            .footer {{\n",
    "                text-align: center;\n",
    "                margin-top: 50px;\n",
    "                padding-top: 20px;\n",
    "                border-top: 1px solid #ddd;\n",
    "                color: #7f8c8d;\n",
    "            }}\n",
    "            .advantage {{\n",
    "                color: #27ae60;\n",
    "            }}\n",
    "            .disadvantage {{\n",
    "                color: #e74c3c;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Informe Final: Comparación de Modelos RNN/LSTM y Transformer para NLP</h1>\n",
    "        \n",
    "        <h2>1. Resumen Ejecutivo</h2>\n",
    "        <p>\n",
    "            Este informe presenta un análisis comparativo entre diferentes arquitecturas de redes neuronales\n",
    "            para el procesamiento de lenguaje natural (NLP): RNN simple, LSTM, GRU y Transformer.\n",
    "            Se evaluaron estos modelos en términos de precisión, métricas específicas de NLP y eficiencia computacional.\n",
    "        </p>\n",
    "        \n",
    "        <h2>2. Configuración Experimental</h2>\n",
    "        <p>\n",
    "            <strong>Parámetros de los modelos:</strong>\n",
    "            <ul>\n",
    "                <li>Dimensión de entrada/salida: {INPUT_DIM}</li>\n",
    "                <li>Dimensión de embedding: {EMB_DIM}</li>\n",
    "                <li>Dimensión oculta: {HIDDEN_DIM}</li>\n",
    "                <li>Número de capas: {N_LAYERS}</li>\n",
    "                <li>Número de cabezas (Transformer): {N_HEADS}</li>\n",
    "                <li>Dropout: {DROPOUT}</li>\n",
    "                <li>Tasa de aprendizaje: {LEARNING_RATE}</li>\n",
    "                <li>Épocas de entrenamiento: {N_EPOCHS}</li>\n",
    "            </ul>\n",
    "        </p>\n",
    "        \n",
    "        <h2>3. Resultados Comparativos</h2>\n",
    "        \n",
    "        <h3>3.1. Tabla de Métricas</h3>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Métrica</th>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Añadir encabezados de columnas para cada modelo\n",
    "    for model in model_names:\n",
    "        html_content += f\"<th>{model}</th>\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "            </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Añadir filas para cada métrica\n",
    "    metrics_list = ['accuracy', 'precision', 'recall', 'f1', 'bleu', 'rouge-1', 'rouge-2', 'rouge-l']\n",
    "    metrics_display = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "    \n",
    "    for i, metric in enumerate(metrics_list):\n",
    "        html_content += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{metrics_display[i]}</td>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Encontrar el mejor valor para esta métrica\n",
    "        best_value = max([all_metrics[model][metric] for model in model_names])\n",
    "        \n",
    "        for model in model_names:\n",
    "            value = all_metrics[model][metric]\n",
    "            if value == best_value:\n",
    "                html_content += f\"<td class='highlight'>{value:.4f}</td>\"\n",
    "            else:\n",
    "                html_content += f\"<td>{value:.4f}</td>\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Añadir fila para tiempo de inferencia\n",
    "    html_content += \"\"\"\n",
    "            <tr>\n",
    "                <td>Tiempo de inferencia relativo</td>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encontrar el mejor tiempo (menor valor)\n",
    "    best_time = min([relative_times[model] for model in model_names])\n",
    "    \n",
    "    for model in model_names:\n",
    "        time_value = relative_times[model]\n",
    "        if time_value == best_time:\n",
    "            html_content += f\"<td class='highlight'>{time_value:.2f}x</td>\"\n",
    "        else:\n",
    "            html_content += f\"<td>{time_value:.2f}x</td>\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "            </tr>\n",
    "        </table>\n",
    "        \n",
    "        <h3>3.2. Visualizaciones</h3>\n",
    "        \n",
    "        <div class=\"container\">\n",
    "            <div class=\"chart-container\">\n",
    "                <h4>Comparación de Accuracy</h4>\n",
    "                <img src=\"comparison_accuracy.png\" alt=\"Comparación de Accuracy\" width=\"100%\">\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"chart-container\">\n",
    "                <h4>Comparación de F1-Score</h4>\n",
    "                <img src=\"comparison_f1.png\" alt=\"Comparación de F1-Score\" width=\"100%\">\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"chart-container\">\n",
    "                <h4>Comparación de BLEU</h4>\n",
    "                <img src=\"comparison_bleu.png\" alt=\"Comparación de BLEU\" width=\"100%\">\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"chart-container\">\n",
    "                <h4>Comparación de ROUGE-L</h4>\n",
    "                <img src=\"comparison_rouge-l.png\" alt=\"Comparación de ROUGE-L\" width=\"100%\">\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"chart-container full-width\">\n",
    "                <h4>Tiempos de Inferencia Relativos</h4>\n",
    "                <img src=\"inference_times.png\" alt=\"Tiempos de Inferencia\" width=\"100%\">\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <h3>3.3. Análisis de Hiperparámetros</h3>\n",
    "        \n",
    "        <div class=\"container\">\n",
    "            <div class=\"chart-container\">\n",
    "                <h4>Impacto del Número de Capas (LSTM)</h4>\n",
    "                <img src=\"impact_n_layers_f1.png\" alt=\"Impacto del Número de Capas\" width=\"100%\">\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"chart-container\">\n",
    "                <h4>Impacto de la Tasa de Aprendizaje (LSTM)</h4>\n",
    "                <img src=\"impact_learning_rate_f1.png\" alt=\"Impacto de la Tasa de Aprendizaje\" width=\"100%\">\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"chart-container\">\n",
    "                <h4>Impacto del Número de Capas (Transformer)</h4>\n",
    "                <img src=\"impact_n_layers_f1.png\" alt=\"Impacto del Número de Capas (Transformer)\" width=\"100%\">\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"chart-container\">\n",
    "                <h4>Impacto del Número de Cabezas (Transformer)</h4>\n",
    "                <img src=\"impact_n_heads_f1.png\" alt=\"Impacto del Número de Cabezas\" width=\"100%\">\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <h2>4. Análisis Comparativo</h2>\n",
    "        \n",
    "        <h3>4.1. Mejor Modelo RNN/LSTM vs Transformer</h3>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Comparación del mejor modelo RNN/LSTM vs Transformer\n",
    "    html_content += f\"\"\"\n",
    "        <p>\n",
    "            El mejor modelo de la familia RNN/LSTM fue <strong>{best_rnn_lstm_model}</strong> con un F1-score de {all_metrics[best_rnn_lstm_model]['f1']:.4f}.\n",
    "            En comparación, el modelo Transformer obtuvo un F1-score de {all_metrics['Transformer']['f1']:.4f}.\n",
    "        </p>\n",
    "        \n",
    "        <div class=\"container\">\n",
    "            <div class=\"chart-container full-width\">\n",
    "                <h4>Comparación Final: {best_rnn_lstm_model} vs Transformer</h4>\n",
    "                <img src=\"final_comparison_f1.png\" alt=\"Comparación Final\" width=\"100%\">\n",
    "            </div>\n",
    "        </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Conclusiones basadas en los resultados\n",
    "    html_content += \"\"\"\n",
    "        <h3>4.2. Componentes Clave del Transformer</h3>\n",
    "        <p>\n",
    "            El modelo Transformer se distingue por varios componentes clave que contribuyen a su rendimiento:\n",
    "        </p>\n",
    "        <ul>\n",
    "            <li><strong>Mecanismo de Autoatención:</strong> Permite al modelo atender a diferentes partes de la secuencia de entrada simultáneamente, facilitando la captura de dependencias a larga distancia.</li>\n",
    "            <li><strong>Codificación Posicional:</strong> Proporciona información sobre la posición de cada token en la secuencia, compensando la falta de recurrencia.</li>\n",
    "            <li><strong>Arquitectura Encoder-Decoder:</strong> Permite procesar la entrada y generar la salida de manera eficiente, con un flujo de información bien estructurado.</li>\n",
    "            <li><strong>Multi-Head Attention:</strong> Permite al modelo atender a diferentes representaciones del espacio simultáneamente, capturando diferentes tipos de relaciones.</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h2>5. Conclusiones</h2>\n",
    "        \n",
    "        <div class=\"conclusion\">\n",
    "    \"\"\"\n",
    "    \n",
    "    # Conclusiones basadas en los resultados\n",
    "    if all_metrics['Transformer']['f1'] > all_metrics[best_rnn_lstm_model]['f1']:\n",
    "        html_content += f\"\"\"\n",
    "            <p>\n",
    "                <strong>1. Comparación de Arquitecturas:</strong><br>\n",
    "                El modelo Transformer superó al mejor modelo RNN/LSTM ({best_rnn_lstm_model}) en términos de F1-score,\n",
    "                demostrando su capacidad superior para capturar relaciones complejas en los datos.\n",
    "            </p>\n",
    "        \"\"\"\n",
    "    else:\n",
    "        html_content += f\"\"\"\n",
    "            <p>\n",
    "                <strong>1. Comparación de Arquitecturas:</strong><br>\n",
    "                El mejor modelo RNN/LSTM ({best_rnn_lstm_model}) superó al Transformer en términos de F1-score,\n",
    "                lo que sugiere que para este conjunto de datos específico, las arquitecturas recurrentes pueden ser más adecuadas.\n",
    "            </p>\n",
    "        \"\"\"\n",
    "    \n",
    "    if all_metrics['Transformer']['bleu'] > all_metrics[best_rnn_lstm_model]['bleu']:\n",
    "        html_content += f\"\"\"\n",
    "            <p>\n",
    "                En términos de BLEU score, el Transformer también mostró un mejor rendimiento, indicando su superioridad\n",
    "                para tareas de generación de texto.\n",
    "            </p>\n",
    "        \"\"\"\n",
    "    else:\n",
    "        html_content += f\"\"\"\n",
    "            <p>\n",
    "                En términos de BLEU score, el modelo {best_rnn_lstm_model} mostró un mejor rendimiento, lo que sugiere\n",
    "                que puede ser más adecuado para ciertas tareas de generación de texto en este contexto.\n",
    "            </p>\n",
    "        \"\"\"\n",
    "    \n",
    "    if relative_times['Transformer'] < relative_times[best_rnn_lstm_model]:\n",
    "        html_content += f\"\"\"\n",
    "            <p>\n",
    "                En cuanto a eficiencia computacional, el Transformer fue más rápido en inferencia que el mejor modelo RNN/LSTM,\n",
    "                lo que destaca otra ventaja de su arquitectura paralela.\n",
    "            </p>\n",
    "        \"\"\"\n",
    "    else:\n",
    "        html_content += f\"\"\"\n",
    "            <p>\n",
    "                En cuanto a eficiencia computacional, el modelo {best_rnn_lstm_model} fue más rápido en inferencia que el Transformer,\n",
    "                lo que puede ser una consideración importante para aplicaciones con restricciones de recursos.\n",
    "            </p>\n",
    "        \"\"\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "            <p>\n",
    "                <strong>2. Impacto de Hiperparámetros:</strong><br>\n",
    "                - Número de capas: Un mayor número de capas puede mejorar el rendimiento hasta cierto punto, pero también aumenta el riesgo de sobreajuste.<br>\n",
    "                - Tasa de aprendizaje: Una tasa de aprendizaje adecuada es crucial para la convergencia del modelo.<br>\n",
    "                - Número de cabezas de atención (Transformer): Más cabezas permiten capturar diferentes tipos de relaciones en los datos.\n",
    "            </p>\n",
    "            \n",
    "            <p>\n",
    "                <strong>3. Ventajas y Desventajas:</strong><br>\n",
    "                - RNN/LSTM:\n",
    "                <ul>\n",
    "                    <li class=\"advantage\">Ventajas: Más simples, menos parámetros, eficientes para secuencias cortas.</li>\n",
    "                    <li class=\"disadvantage\">Desventajas: Dificultad para capturar dependencias a largo plazo, procesamiento secuencial.</li>\n",
    "                </ul>\n",
    "                \n",
    "                - Transformer:\n",
    "                <ul>\n",
    "                    <li class=\"advantage\">Ventajas: Paralelización, mejor captura de dependencias a largo plazo, atención a diferentes partes de la secuencia.</li>\n",
    "                    <li class=\"disadvantage\">Desventajas: Mayor número de parámetros, requiere más datos para entrenar efectivamente.</li>\n",
    "                </ul>\n",
    "            </p>\n",
    "        </div>\n",
    "        \n",
    "        <h2>6. Recomendaciones</h2>\n",
    "        <p>\n",
    "            Basado en los resultados de este estudio, se pueden hacer las siguientes recomendaciones:\n",
    "        </p>\n",
    "        <ul>\n",
    "            <li>Para tareas de NLP con secuencias largas y dependencias a distancia, considerar el uso de Transformers.</li>\n",
    "            <li>Para aplicaciones con recursos limitados o conjuntos de datos pequeños, los modelos LSTM/GRU pueden ser más adecuados.</li>\n",
    "            <li>La selección del modelo debe considerar no solo la precisión, sino también la eficiencia computacional según los requisitos específicos.</li>\n",
    "            <li>Es recomendable realizar un ajuste cuidadoso de hiperparámetros, especialmente el número de capas y la tasa de aprendizaje.</li>\n",
    "            <li>Para aplicaciones en tiempo real, considerar el equilibrio entre precisión y tiempo de inferencia.</li>\n",
    "        </ul>\n",
    "        \n",
    "        <div class=\"footer\">\n",
    "            <p>Informe generado automáticamente - Análisis de Modelos RNN/LSTM y Transformer para NLP</p>\n",
    "            <p>Fecha: \"\"\" + time.strftime(\"%d/%m/%Y\") + \"\"\"</p>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Guardar el informe HTML\n",
    "    with open('informe_final.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(\"Informe final generado como 'informe_final.html'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar informe final\n",
    "generate_final_report(all_metrics, relative_times, model_names, best_rnn_lstm_model)\n",
    "\n",
    "# Función para generar respuestas con el modelo\n",
    "def generate_response(model, text_processor, input_text, device, temperature=0.8, beam_size=3):\n",
    "    \"\"\"\n",
    "    Genera una respuesta utilizando el modelo entrenado\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Convertir texto de entrada a índices\n",
    "    input_indices = text_processor.text_to_indices(input_text, add_special_tokens=True)\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Para beam search\n",
    "    if beam_size > 1:\n",
    "        return beam_search_decode(model, input_tensor, text_processor, beam_size, max_length=50)\n",
    "    \n",
    "    # Para generación greedy o con temperatura\n",
    "    with torch.no_grad():\n",
    "        # Inicializar con token SOS\n",
    "        output_indices = [text_processor.word2idx['<SOS>']]\n",
    "        \n",
    "        # Generar tokens uno a uno\n",
    "        for _ in range(50):  # Limitar a 50 tokens como máximo\n",
    "            # Convertir secuencia actual a tensor\n",
    "            output_tensor = torch.tensor([output_indices], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Obtener predicción del modelo\n",
    "            with torch.no_grad():\n",
    "                predictions = model(output_tensor)\n",
    "            \n",
    "            # Obtener distribución de probabilidad para el último token\n",
    "            next_token_logits = predictions[0, -1, :]\n",
    "            \n",
    "            # Aplicar temperatura si es necesario\n",
    "            if temperature != 1.0:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Convertir a probabilidades\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=0)\n",
    "            \n",
    "            # Muestrear de la distribución o tomar el argmax\n",
    "            if temperature > 0:\n",
    "                next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "            else:\n",
    "                next_token = torch.argmax(next_token_probs).item()\n",
    "            \n",
    "            # Añadir token a la secuencia\n",
    "            output_indices.append(next_token)\n",
    "            \n",
    "            # Detener si se genera EOS\n",
    "            if next_token == text_processor.word2idx['<EOS>']:\n",
    "                break\n",
    "    \n",
    "    # Convertir índices a texto\n",
    "    response = text_processor.indices_to_text(output_indices)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ced96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, input_tensor, text_processor, beam_size=3, max_length=50):\n",
    "    \"\"\"\n",
    "    Implementa beam search para generar respuestas de mayor calidad\n",
    "    \"\"\"\n",
    "    # Inicializar con token SOS\n",
    "    sequences = [[text_processor.word2idx['<SOS>']], 0.0]\n",
    "    \n",
    "    # Expandir secuencias\n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []\n",
    "        \n",
    "        # Expandir cada secuencia actual\n",
    "        for seq, score in sequences:\n",
    "            if seq[-1] == text_processor.word2idx['<EOS>']:\n",
    "                # Si la secuencia ya terminó, mantenerla como está\n",
    "                all_candidates.append([seq, score])\n",
    "                continue\n",
    "            \n",
    "            # Preparar entrada para el modelo\n",
    "            output_tensor = torch.tensor([seq], dtype=torch.long).to(input_tensor.device)\n",
    "            \n",
    "            # Obtener predicción del modelo\n",
    "            with torch.no_grad():\n",
    "                predictions = model(output_tensor)\n",
    "            \n",
    "            # Obtener distribución para el último token\n",
    "            next_token_logits = predictions[0, -1, :]\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=0)\n",
    "            \n",
    "            # Obtener los top-k tokens\n",
    "            topk_probs, topk_indices = torch.topk(next_token_probs, beam_size)\n",
    "            \n",
    "            # Crear nuevas secuencias candidatas\n",
    "            for i in range(beam_size):\n",
    "                next_token = topk_indices[i].item()\n",
    "                next_score = score - torch.log(topk_probs[i]).item()  # Usar log-probabilidad negativa\n",
    "                \n",
    "                all_candidates.append([seq + [next_token], next_score])\n",
    "        \n",
    "        # Ordenar candidatos por puntuación\n",
    "        all_candidates.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Seleccionar los mejores beam_size candidatos\n",
    "        sequences = all_candidates[:beam_size]\n",
    "        \n",
    "        # Verificar si todas las secuencias han terminado\n",
    "        if all(seq[-1] == text_processor.word2idx['<EOS>'] for seq, _ in sequences):\n",
    "            break\n",
    "    \n",
    "    # Tomar la mejor secuencia\n",
    "    best_seq = sequences[0][0]\n",
    "    \n",
    "    # Convertir a texto\n",
    "    response = text_processor.indices_to_text(best_seq)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc199bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para visualizar la atención del Transformer\n",
    "def visualize_attention(model, text_processor, input_text, device):\n",
    "    \"\"\"\n",
    "    Visualiza los pesos de atención del modelo Transformer\n",
    "    \"\"\"\n",
    "    if not isinstance(model, TransformerModel):\n",
    "        print(\"Esta función solo es compatible con modelos Transformer\")\n",
    "        return\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Convertir texto de entrada a índices\n",
    "    input_indices = text_processor.text_to_indices(input_text, add_special_tokens=True)\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Obtener tokens de entrada como texto\n",
    "    input_tokens = [text_processor.idx2word.get(idx, '<UNK>') for idx in input_indices]\n",
    "    \n",
    "    # Registrar hooks para capturar la atención\n",
    "    attention_maps = []\n",
    "    \n",
    "    def get_attention(module, input, output):\n",
    "        attention_maps.append(output[1].detach().cpu())\n",
    "    \n",
    "    # Registrar hooks en las capas de atención\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        if \"multihead_attn\" in name:\n",
    "            hook = module.register_forward_hook(get_attention)\n",
    "            hooks.append(hook)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_tensor)\n",
    "    \n",
    "    # Eliminar hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Visualizar mapas de atención\n",
    "    if attention_maps:\n",
    "        n_layers = len(attention_maps)\n",
    "        n_heads = attention_maps[0].size(1)\n",
    "        \n",
    "        fig, axes = plt.subplots(n_layers, n_heads, figsize=(n_heads*3, n_layers*3))\n",
    "        \n",
    "        if n_layers == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, layer_attention in enumerate(attention_maps):\n",
    "            for j in range(n_heads):\n",
    "                ax = axes[i][j] if n_heads > 1 else axes[i]\n",
    "                \n",
    "                # Obtener mapa de atención para esta cabeza\n",
    "                attn = layer_attention[0, j].numpy()\n",
    "                \n",
    "                # Crear heatmap\n",
    "                im = ax.imshow(attn, cmap='viridis')\n",
    "                \n",
    "                # Configurar etiquetas\n",
    "                ax.set_xticks(range(len(input_tokens)))\n",
    "                ax.set_yticks(range(len(input_tokens)))\n",
    "                ax.set_xticklabels(input_tokens, rotation=90)\n",
    "                ax.set_yticklabels(input_tokens)\n",
    "                \n",
    "                ax.set_title(f\"Layer {i+1}, Head {j+1}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('attention_visualization.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Visualización de atención guardada como 'attention_visualization.png'\")\n",
    "    else:\n",
    "        print(\"No se pudieron capturar mapas de atención\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb26354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para analizar errores comunes\n",
    "def analyze_errors(model, dataloader, text_processor, device, num_examples=10):\n",
    "    \"\"\"\n",
    "    Analiza los errores más comunes del modelo\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalizando errores comunes del modelo...\")\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            if len(errors) >= num_examples:\n",
    "                break\n",
    "                \n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src)\n",
    "            \n",
    "            # Obtener predicciones\n",
    "            predictions = torch.argmax(output, dim=2)\n",
    "            \n",
    "            # Analizar cada ejemplo en el batch\n",
    "            for i in range(src.size(0)):\n",
    "                if len(errors) >= num_examples:\n",
    "                    break\n",
    "                \n",
    "                # Convertir a listas de tokens\n",
    "                input_tokens = [text_processor.idx2word.get(idx.item(), '<UNK>') for idx in src[i] if idx.item() > 0]\n",
    "                target_tokens = [text_processor.idx2word.get(idx.item(), '<UNK>') for idx in trg[i] if idx.item() > 0]\n",
    "                pred_tokens = [text_processor.idx2word.get(idx.item(), '<UNK>') for idx in predictions[i] if idx.item() > 0]\n",
    "                \n",
    "                # Calcular similitud\n",
    "                target_text = ' '.join(target_tokens)\n",
    "                pred_text = ' '.join(pred_tokens)\n",
    "                \n",
    "                # Si hay diferencia, registrar como error\n",
    "                if target_text != pred_text:\n",
    "                    # Encontrar tokens incorrectos\n",
    "                    incorrect_tokens = []\n",
    "                    for j, (t, p) in enumerate(zip(target_tokens, pred_tokens)):\n",
    "                        if t != p:\n",
    "                            incorrect_tokens.append((j, t, p))\n",
    "                    \n",
    "                    errors.append({\n",
    "                        'input': ' '.join(input_tokens),\n",
    "                        'target': target_text,\n",
    "                        'prediction': pred_text,\n",
    "                        'incorrect_tokens': incorrect_tokens\n",
    "                    })\n",
    "    \n",
    "    # Mostrar errores\n",
    "    print(\"\\nAnálisis de errores comunes:\")\n",
    "    for i, error in enumerate(errors):\n",
    "        print(f\"\\nError {i+1}:\")\n",
    "        print(f\"Entrada: {error['input']}\")\n",
    "        print(f\"Objetivo: {error['target']}\")\n",
    "        print(f\"Predicción: {error['prediction']}\")\n",
    "        print(\"Tokens incorrectos:\")\n",
    "        for pos, target, pred in error['incorrect_tokens']:\n",
    "            print(f\"  Posición {pos}: '{target}' -> '{pred}'\")\n",
    "    \n",
    "    # Analizar patrones de error\n",
    "    error_patterns = {}\n",
    "    for error in errors:\n",
    "        for _, target, pred in error['incorrect_tokens']:\n",
    "            pattern = f\"{target} -> {pred}\"\n",
    "            if pattern in error_patterns:\n",
    "                error_patterns[pattern] += 1\n",
    "            else:\n",
    "                error_patterns[pattern] = 1\n",
    "    \n",
    "    # Mostrar patrones más comunes\n",
    "    print(\"\\nPatrones de error más comunes:\")\n",
    "    sorted_patterns = sorted(error_patterns.items(), key=lambda x: x[1], reverse=True)\n",
    "    for pattern, count in sorted_patterns[:5]:\n",
    "        print(f\"  {pattern}: {count} ocurrencias\")\n",
    "    \n",
    "    # Crear visualización\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(\n",
    "            values=['Error', 'Entrada', 'Objetivo', 'Predicción', 'Tokens Incorrectos'],\n",
    "            fill_color='paleturquoise',\n",
    "            align='left'\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[\n",
    "                list(range(1, len(errors) + 1)),\n",
    "                [error['input'] for error in errors],\n",
    "                [error['target'] for error in errors],\n",
    "                [error['prediction'] for error in errors],\n",
    "                [', '.join([f\"'{t}'->{p}\" for _, t, p in error['incorrect_tokens']]) for error in errors]\n",
    "            ],\n",
    "            fill_color='lavender',\n",
    "            align='left'\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Análisis de Errores\",\n",
    "        height=125 * len(errors)\n",
    "    )\n",
    "    \n",
    "    display(fig)\n",
    "    fig.write_html('error_analysis.html')\n",
    "    print(\"Análisis de errores guardado como 'error_analysis.html'\")\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226af1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar análisis de errores para el mejor modelo\n",
    "print(\"\\n¿Deseas realizar un análisis de errores del mejor modelo? (s/n)\")\n",
    "choice = input()\n",
    "if choice.lower() in ['s', 'si', 'sí', 'y', 'yes']:\n",
    "    # Usar el mejor modelo según las métricas\n",
    "    best_model_name = max(['RNN', 'LSTM', 'GRU', 'Transformer'], \n",
    "                          key=lambda x: all_metrics[x]['f1'])\n",
    "    \n",
    "    print(f\"Realizando análisis de errores del modelo {best_model_name}...\")\n",
    "    \n",
    "    if best_model_name == 'RNN':\n",
    "        best_model = rnn_model\n",
    "    elif best_model_name == 'LSTM':\n",
    "        best_model = lstm_model\n",
    "    elif best_model_name == 'GRU':\n",
    "        best_model = gru_model\n",
    "    else:\n",
    "        best_model = transformer_model\n",
    "    \n",
    "    # Ejecutar análisis de errores\n",
    "    error_analysis = analyze_errors(best_model, test_loader, text_processor, device)\n",
    "\n",
    "# Si el mejor modelo es un Transformer, visualizar la atención\n",
    "if best_model_name == 'Transformer':\n",
    "    print(\"\\n¿Deseas visualizar los mapas de atención del Transformer? (s/n)\")\n",
    "    choice = input()\n",
    "    if choice.lower() in ['s', 'si', 'sí', 'y', 'yes']:\n",
    "        print(\"Ingresa un texto para visualizar la atención:\")\n",
    "        input_text = input()\n",
    "        visualize_attention(transformer_model, text_processor, input_text, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0406285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar la robustez del modelo\n",
    "def evaluate_robustness(model, text_processor, device, num_examples=5):\n",
    "    \"\"\"\n",
    "    Evalúa la robustez del modelo frente a perturbaciones en la entrada\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluando robustez del modelo frente a perturbaciones...\")\n",
    "    \n",
    "    # Obtener algunos ejemplos del conjunto de prueba\n",
    "    examples = []\n",
    "    for src, trg in test_loader:\n",
    "        if len(examples) >= num_examples:\n",
    "            break\n",
    "        \n",
    "        for i in range(min(src.size(0), num_examples - len(examples))):\n",
    "            input_text = text_processor.indices_to_text(src[i].numpy())\n",
    "            target_text = text_processor.indices_to_text(trg[i].numpy())\n",
    "            \n",
    "            if len(input_text.split()) > 5:  # Asegurar que el texto tenga suficientes palabras\n",
    "                examples.append({\n",
    "                    'input': input_text,\n",
    "                    'target': target_text\n",
    "                })\n",
    "    \n",
    "    # Tipos de perturbaciones\n",
    "    perturbations = [\n",
    "        ('original', lambda x: x),\n",
    "        ('eliminar_palabra', lambda x: ' '.join(x.split()[1:] if len(x.split()) > 1 else x.split())),\n",
    "        ('cambiar_orden', lambda x: ' '.join(x.split()[::-1]) if len(x.split()) > 1 else x),\n",
    "        ('duplicar_palabra', lambda x: x + ' ' + x.split()[0] if len(x.split()) > 0 else x),\n",
    "        ('añadir_ruido', lambda x: x + ' xyz123')\n",
    "    ]\n",
    "    \n",
    "    # Evaluar cada ejemplo con diferentes perturbaciones\n",
    "    results = []\n",
    "    \n",
    "    for example in examples:\n",
    "        example_results = {'original': example}\n",
    "        \n",
    "        for name, perturb_func in perturbations:\n",
    "            if name == 'original':\n",
    "                perturbed_input = example['input']\n",
    "            else:\n",
    "                perturbed_input = perturb_func(example['input'])\n",
    "            \n",
    "            # Generar respuesta con el modelo\n",
    "            model_output = generate_response(model, text_processor, perturbed_input, device)\n",
    "            \n",
    "            example_results[name] = {\n",
    "                'input': perturbed_input,\n",
    "                'output': model_output\n",
    "            }\n",
    "        \n",
    "        results.append(example_results)\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(\"\\nResultados de la evaluación de robustez:\")\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nEjemplo {i+1}:\")\n",
    "        print(f\"Original - Entrada: {result['original']['input']}\")\n",
    "        print(f\"Original - Objetivo: {result['original']['target']}\")\n",
    "        \n",
    "        for name in [p[0] for p in perturbations]:\n",
    "            if name != 'original':\n",
    "                print(f\"\\n{name.capitalize()} - Entrada: {result[name]['input']}\")\n",
    "                print(f\"{name.capitalize()} - Salida: {result[name]['output']}\")\n",
    "    \n",
    "    # Crear visualización\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(\n",
    "            values=['Ejemplo', 'Tipo', 'Entrada', 'Salida'],\n",
    "            fill_color='paleturquoise',\n",
    "            align='left'\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[\n",
    "                [f\"Ejemplo {i+1}\" for i in range(len(results)) for _ in range(len(perturbations))],\n",
    "                [p[0].capitalize() for _ in range(len(results)) for p in perturbations],\n",
    "                [result[p[0]]['input'] for result in results for p in perturbations],\n",
    "                [result[p[0]].get('output', result[p[0]].get('target', '')) for result in results for p in perturbations]\n",
    "            ],\n",
    "            fill_color='lavender',\n",
    "            align='left'\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Evaluación de Robustez\",\n",
    "        height=125 * len(results) * len(perturbations)\n",
    "    )\n",
    "    \n",
    "    display(fig)\n",
    "    fig.write_html('robustness_evaluation.html')\n",
    "    print(\"Evaluación de robustez guardada como 'robustness_evaluation.html'\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar evaluación de robustez para el mejor modelo\n",
    "print(\"\\n¿Deseas realizar una evaluación de robustez del mejor modelo? (s/n)\")\n",
    "choice = input()\n",
    "if choice.lower() in ['s', 'si', 'sí', 'y', 'yes']:\n",
    "    # Usar el mejor modelo según las métricas\n",
    "    best_model_name = max(['RNN', 'LSTM', 'GRU', 'Transformer'], \n",
    "                          key=lambda x: all_metrics[x]['f1'])\n",
    "    \n",
    "    print(f\"Realizando evaluación de robustez del modelo {best_model_name}...\")\n",
    "    \n",
    "    if best_model_name == 'RNN':\n",
    "        best_model = rnn_model\n",
    "    elif best_model_name == 'LSTM':\n",
    "        best_model = lstm_model\n",
    "    elif best_model_name == 'GRU':\n",
    "        best_model = gru_model\n",
    "    else:\n",
    "        best_model = transformer_model\n",
    "    \n",
    "    # Ejecutar evaluación de robustez\n",
    "    robustness_results = evaluate_robustness(best_model, text_processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ed56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para comparar la eficiencia de memoria\n",
    "def compare_memory_usage(models, model_names, device):\n",
    "    \"\"\"\n",
    "    Compara el uso de memoria de diferentes modelos\n",
    "    \"\"\"\n",
    "    print(\"\\nComparando uso de memoria de los modelos...\")\n",
    "    \n",
    "    memory_usage = {}\n",
    "    \n",
    "    for model, name in zip(models, model_names):\n",
    "        # Mover modelo a CPU para medición precisa\n",
    "        model = model.to('cpu')\n",
    "        \n",
    "        # Calcular número de parámetros\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # Estimar uso de memoria (en MB)\n",
    "        memory_estimate = num_params * 4 / (1024 * 1024)  # 4 bytes por parámetro (float32)\n",
    "        \n",
    "        memory_usage[name] = {\n",
    "            'params': num_params,\n",
    "            'memory_mb': memory_estimate\n",
    "        }\n",
    "        \n",
    "        # Devolver modelo al dispositivo original\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(\"\\nUso de memoria por modelo:\")\n",
    "    for name, usage in memory_usage.items():\n",
    "        print(f\"{name}: {usage['params']:,} parámetros, {usage['memory_mb']:.2f} MB\")\n",
    "    \n",
    "    # Crear visualización\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Gráfico de barras para parámetros\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=list(memory_usage.keys()),\n",
    "        y=[usage['params'] for usage in memory_usage.values()],\n",
    "        name='Número de Parámetros',\n",
    "        text=[f\"{usage['params']:,}\" for usage in memory_usage.values()],\n",
    "        textposition='auto'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Comparación de Número de Parámetros',\n",
    "        xaxis_title='Modelo',\n",
    "        yaxis_title='Número de Parámetros',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    display(fig)\n",
    "    fig.write_html('memory_comparison.html')\n",
    "    \n",
    "    # Gráfico de barras para memoria\n",
    "    fig2 = go.Figure()\n",
    "    fig2.add_trace(go.Bar(\n",
    "        x=list(memory_usage.keys()),\n",
    "        y=[usage['memory_mb'] for usage in memory_usage.values()],\n",
    "        name='Memoria (MB)',\n",
    "        text=[f\"{usage['memory_mb']:.2f} MB\" for usage in memory_usage.values()],\n",
    "        textposition='auto'\n",
    "    ))\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        title='Comparación de Uso de Memoria',\n",
    "        xaxis_title='Modelo',\n",
    "        yaxis_title='Memoria (MB)',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    display(fig2)\n",
    "    fig2.write_html('memory_comparison_mb.html')\n",
    "    \n",
    "    print(\"Comparación de memoria guardada como 'memory_comparison.html' y 'memory_comparison_mb.html'\")\n",
    "    \n",
    "    return memory_usage\n",
    "\n",
    "# Ejecutar comparación de memoria\n",
    "print(\"\\n¿Deseas comparar el uso de memoria de los modelos? (s/n)\")\n",
    "choice = input()\n",
    "if choice.lower() in ['s', 'si', 'sí', 'y', 'yes']:\n",
    "    memory_usage = compare_memory_usage(\n",
    "        [rnn_model, lstm_model, gru_model, transformer_model],\n",
    "        ['RNN', 'LSTM', 'GRU', 'Transformer'],\n",
    "        device\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar un resumen ejecutivo\n",
    "def generate_executive_summary(all_metrics, relative_times, memory_usage, best_model_name):\n",
    "    \"\"\"\n",
    "    Genera un resumen ejecutivo con los principales hallazgos\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerando resumen ejecutivo...\")\n",
    "    \n",
    "    summary = \"\"\"\n",
    "# Resumen Ejecutivo: Comparación de Modelos RNN/LSTM y Transformer para NLP\n",
    "\n",
    "## Objetivo\n",
    "Este estudio comparó diferentes arquitecturas de redes neuronales para procesamiento de lenguaje natural (NLP): RNN simple, LSTM, GRU y Transformer, evaluando su rendimiento, eficiencia y características.\n",
    "\n",
    "## Metodología\n",
    "Se entrenaron y evaluaron los modelos utilizando métricas estándar de NLP como precisión, F1-score, BLEU y ROUGE, además de medir tiempos de inferencia y uso de memoria.\n",
    "\n",
    "## Principales Hallazgos\n",
    "\"\"\"\n",
    "    \n",
    "    # Mejor modelo según F1-score\n",
    "    best_f1_model = max(model_names, key=lambda x: all_metrics[x]['f1'])\n",
    "    summary += f\"1. **Rendimiento**: El modelo {best_f1_model} obtuvo el mejor F1-score ({all_metrics[best_f1_model]['f1']:.4f}), \"\n",
    "    \n",
    "    # Mejor modelo según BLEU\n",
    "    best_bleu_model = max(model_names, key=lambda x: all_metrics[x]['bleu'])\n",
    "    if best_bleu_model == best_f1_model:\n",
    "        summary += f\"y también el mejor BLEU score ({all_metrics[best_bleu_model]['bleu']:.4f}).\\n\"\n",
    "    else:\n",
    "        summary += f\"mientras que {best_bleu_model} obtuvo el mejor BLEU score ({all_metrics[best_bleu_model]['bleu']:.4f}).\\n\"\n",
    "    \n",
    "    # Modelo más rápido\n",
    "    fastest_model = min(model_names, key=lambda x: relative_times[x])\n",
    "    summary += f\"2. **Eficiencia**: {fastest_model} fue el modelo más rápido en inferencia, \"\n",
    "    \n",
    "    # Modelo con menos memoria\n",
    "    if memory_usage:\n",
    "        smallest_model = min(model_names, key=lambda x: memory_usage[x]['memory_mb'])\n",
    "        summary += f\"y {smallest_model} utilizó la menor cantidad de memoria ({memory_usage[smallest_model]['memory_mb']:.2f} MB).\\n\"\n",
    "    else:\n",
    "        summary += \"siendo 1.0x más rápido que el segundo mejor.\\n\"\n",
    "    \n",
    "    # Comparación RNN/LSTM vs Transformer\n",
    "    summary += \"3. **Comparación RNN/LSTM vs Transformer**:\\n\"\n",
    "    \n",
    "    if all_metrics['Transformer']['f1'] > all_metrics[best_rnn_lstm_model]['f1']:\n",
    "        summary += f\"   - El Transformer superó al mejor modelo RNN/LSTM en F1-score ({all_metrics['Transformer']['f1']:.4f} vs {all_metrics[best_rnn_lstm_model]['f1']:.4f}).\\n\"\n",
    "    else:\n",
    "        summary += f\"   - El mejor modelo RNN/LSTM ({best_rnn_lstm_model}) superó al Transformer en F1-score ({all_metrics[best_rnn_lstm_model]['f1']:.4f} vs {all_metrics['Transformer']['f1']:.4f}).\\n\"\n",
    "    \n",
    "    if relative_times['Transformer'] < relative_times[best_rnn_lstm_model]:\n",
    "        summary += f\"   - El Transformer fue más rápido en inferencia que el mejor modelo RNN/LSTM ({relative_times['Transformer']:.2f}x vs {relative_times[best_rnn_lstm_model]:.2f}x).\\n\"\n",
    "    else:\n",
    "        summary += f\"   - El mejor modelo RNN/LSTM fue más rápido en inferencia que el Transformer ({relative_times[best_rnn_lstm_model]:.2f}x vs {relative_times['Transformer']:.2f}x).\\n\"\n",
    "    \n",
    "    if memory_usage:\n",
    "        if memory_usage['Transformer']['memory_mb'] < memory_usage[best_rnn_lstm_model]['memory_mb']:\n",
    "            summary += f\"   - El Transformer utilizó menos memoria que el mejor modelo RNN/LSTM ({memory_usage['Transformer']['memory_mb']:.2f} MB vs {memory_usage[best_rnn_lstm_model]['memory_mb']:.2f} MB).\\n\"\n",
    "        else:\n",
    "            summary += f\"   - El mejor modelo RNN/LSTM utilizó menos memoria que el Transformer ({memory_usage[best_rnn_lstm_model]['memory_mb']:.2f} MB vs {memory_usage['Transformer']['memory_mb']:.2f} MB).\\n\"\n",
    "    \n",
    "    # Conclusiones\n",
    "    summary += \"\"\"\n",
    "## Conclusiones\n",
    "1. **Selección de modelo**: La elección entre arquitecturas RNN/LSTM y Transformer debe considerar el equilibrio entre rendimiento, velocidad y uso de recursos según los requisitos específicos de la aplicación.\n",
    "\n",
    "2. **Ventajas y desventajas**:\n",
    "   - **RNN/LSTM**: Más simples, menos parámetros, eficientes para secuencias cortas. Sin embargo, tienen dificultad para capturar dependencias a largo plazo.\n",
    "   - **Transformer**: Mejor paralelización y captura de dependencias a largo plazo, pero requieren más datos para entrenar efectivamente.\n",
    "\n",
    "3. **Recomendaciones**:\n",
    "   - Para tareas con secuencias largas y dependencias a distancia: considerar Transformers.\n",
    "   - Para aplicaciones con recursos limitados o conjuntos de datos pequeños: considerar LSTM/GRU.\n",
    "   - Realizar un ajuste cuidadoso de hiperparámetros, especialmente el número de capas y la tasa de aprendizaje.\n",
    "\"\"\"\n",
    "    \n",
    "    # Guardar resumen\n",
    "    with open('resumen_ejecutivo.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(summary)\n",
    "    \n",
    "    print(\"Resumen ejecutivo guardado como 'resumen_ejecutivo.md'\")\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f661499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar resumen ejecutivo\n",
    "print(\"\\n¿Deseas generar un resumen ejecutivo de los resultados? (s/n)\")\n",
    "choice = input()\n",
    "if choice.lower() in ['s', 'si', 'sí', 'y', 'yes']:\n",
    "    memory_usage_data = memory_usage if 'memory_usage' in locals() else None\n",
    "    executive_summary = generate_executive_summary(all_metrics, relative_times, memory_usage_data, best_model_name)\n",
    "    print(\"\\nResumen Ejecutivo:\")\n",
    "    print(executive_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72377e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear una interfaz de chat simple\n",
    "def run_chat_interface(model, text_processor, device, max_turns=5):\n",
    "    \"\"\"\n",
    "    Crea una interfaz de chat simple para interactuar con el modelo\n",
    "    \"\"\"\n",
    "    print(\"\\n===== MINI CHAT CON EL MODELO =====\")\n",
    "    print(\"(Escribe 'salir' para terminar)\")\n",
    "    \n",
    "    chat_history = []\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        # Obtener entrada del usuario\n",
    "        user_input = input(\"\\nTú: \")\n",
    "        \n",
    "        if user_input.lower() in ['salir', 'exit', 'quit']:\n",
    "            break\n",
    "        \n",
    "        # Añadir a historial\n",
    "        chat_history.append(f\"Usuario: {user_input}\")\n",
    "        \n",
    "        # Generar respuesta\n",
    "        if len(chat_history) > 1:\n",
    "            # Usar todo el historial como contexto\n",
    "            context = \" \".join(chat_history)\n",
    "        else:\n",
    "            context = user_input\n",
    "        \n",
    "        # Generar respuesta con beam search\n",
    "        model_response = generate_response(model, text_processor, context, device, beam_size=3)\n",
    "        \n",
    "        # Mostrar respuesta\n",
    "        print(f\"Modelo: {model_response}\")\n",
    "        \n",
    "        # Añadir a historial\n",
    "        chat_history.append(f\"Modelo: {model_response}\")\n",
    "    \n",
    "    print(\"\\nFin del chat.\")\n",
    "    return chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar interfaz de chat con el mejor modelo\n",
    "print(\"\\n¿Deseas probar el mejor modelo en un mini chat? (s/n)\")\n",
    "choice = input()\n",
    "if choice.lower() in ['s', 'si', 'sí', 'y', 'yes']:\n",
    "    # Usar el mejor modelo según las métricas\n",
    "    best_model_name = max(['RNN', 'LSTM', 'GRU', 'Transformer'], \n",
    "                          key=lambda x: all_metrics[x]['f1'])\n",
    "    \n",
    "    print(f\"Iniciando chat con el modelo {best_model_name}...\")\n",
    "    \n",
    "    if best_model_name == 'RNN':\n",
    "        chat_model = rnn_model\n",
    "    elif best_model_name == 'LSTM':\n",
    "        chat_model = lstm_model\n",
    "    elif best_model_name == 'GRU':\n",
    "        chat_model = gru_model\n",
    "    else:\n",
    "        chat_model = transformer_model\n",
    "    \n",
    "    # Ejecutar interfaz de chat\n",
    "    chat_history = run_chat_interface(chat_model, text_processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para guardar los modelos entrenados\n",
    "def save_models(models_dict, save_dir='modelos_entrenados'):\n",
    "    \"\"\"\n",
    "    Guarda los modelos entrenados para uso futuro\n",
    "    \"\"\"\n",
    "    print(f\"\\nGuardando modelos entrenados en '{save_dir}'...\")\n",
    "    \n",
    "    # Crear directorio si no existe\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Guardar cada modelo\n",
    "    for name, model in models_dict.items():\n",
    "        model_path = os.path.join(save_dir, f\"{name}_model.pt\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Modelo {name} guardado en {model_path}\")\n",
    "    \n",
    "    # Guardar el procesador de texto\n",
    "    text_processor_path = os.path.join(save_dir, \"text_processor.pkl\")\n",
    "    with open(text_processor_path, 'wb') as f:\n",
    "        import pickle\n",
    "        pickle.dump(text_processor, f)\n",
    "    \n",
    "    print(f\"Procesador de texto guardado en {text_processor_path}\")\n",
    "    \n",
    "    # Guardar configuración\n",
    "    config = {\n",
    "        'INPUT_DIM': INPUT_DIM,\n",
    "        'OUTPUT_DIM': OUTPUT_DIM,\n",
    "        'EMB_DIM': EMB_DIM,\n",
    "        'HIDDEN_DIM': HIDDEN_DIM,\n",
    "        'N_LAYERS': N_LAYERS,\n",
    "        'N_HEADS': N_HEADS,\n",
    "        'DROPOUT': DROPOUT,\n",
    "        'LEARNING_RATE': LEARNING_RATE,\n",
    "        'N_EPOCHS': N_EPOCHS,\n",
    "        'metrics': {name: {k: float(v) for k, v in metrics.items()} for name, metrics in all_metrics.items()},\n",
    "        'relative_times': {k: float(v) for k, v in relative_times.items()}\n",
    "    }\n",
    "    \n",
    "    config_path = os.path.join(save_dir, \"config.json\")\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(f\"Configuración guardada en {config_path}\")\n",
    "    \n",
    "    return save_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c989610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preguntar si se desean guardar los modelos\n",
    "print(\"\\n¿Deseas guardar los modelos entrenados? (s/n)\")\n",
    "choice = input()\n",
    "if choice.lower() in ['s', 'si', 'sí', 'y', 'yes']:\n",
    "    models_dict = {\n",
    "        'RNN': rnn_model,\n",
    "        'LSTM': lstm_model,\n",
    "        'GRU': gru_model,\n",
    "        'Transformer': transformer_model\n",
    "    }\n",
    "    \n",
    "    save_dir = save_models(models_dict)\n",
    "    print(f\"Todos los modelos guardados en '{save_dir}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
