{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08afbfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizando dispositivo: cuda\n",
      "Rúbrica de evaluación cargada correctamente\n",
      "Cargando datos...\n",
      "Datos cargados: 11118 ejemplos de entrenamiento, 1000 de validación, 1000 de prueba\n",
      "\n",
      "Estructura de los datos de entrenamiento:\n",
      "                                              dialog                    act  \\\n",
      "0  [Say , Jim , how about going for a few beers a...  [3 4 2 2 2 3 4 1 3 4]   \n",
      "1  [Can you do push-ups ?  Of course I can . It's...          [2 1 2 2 1 1]   \n",
      "2  [Can you study with the radio on ?  No , I lis...            [2 1 2 1 1]   \n",
      "3  [Are you all right ?  I will be all right soon...              [2 1 1 1]   \n",
      "4  [Hey John , nice skates . Are they new ?  Yeah...    [2 1 2 1 1 2 1 3 4]   \n",
      "\n",
      "                 emotion  num_utterances  \\\n",
      "0  [0 0 0 0 0 0 4 4 4 4]               1   \n",
      "1          [0 0 6 0 0 0]               1   \n",
      "2            [0 0 0 0 0]               1   \n",
      "3              [0 0 0 0]               1   \n",
      "4    [0 0 0 0 0 6 0 6 0]               1   \n",
      "\n",
      "                                         dialog_text  \\\n",
      "0  Say , Jim , how about going for a few beers af...   \n",
      "1  Can you do push-ups ?  Of course I can . It's ...   \n",
      "2  Can you study with the radio on ?  No , I list...   \n",
      "3  Are you all right ?  I will be all right soon ...   \n",
      "4  Hey John , nice skates . Are they new ?  Yeah ...   \n",
      "\n",
      "                                     first_utterance  \\\n",
      "0  Say , Jim , how about going for a few beers af...   \n",
      "1  Can you do push-ups ?  Of course I can . It's ...   \n",
      "2  Can you study with the radio on ?  No , I list...   \n",
      "3  Are you all right ?  I will be all right soon ...   \n",
      "4  Hey John , nice skates . Are they new ?  Yeah ...   \n",
      "\n",
      "                                      last_utterance  act_counts  \\\n",
      "0  Say , Jim , how about going for a few beers af...          21   \n",
      "1  Can you do push-ups ?  Of course I can . It's ...          13   \n",
      "2  Can you study with the radio on ?  No , I list...          11   \n",
      "3  Are you all right ?  I will be all right soon ...           9   \n",
      "4  Hey John , nice skates . Are they new ?  Yeah ...          19   \n",
      "\n",
      "   emotion_counts  lengths_match most_common_act most_common_emotion  \\\n",
      "0              21          False                                       \n",
      "1              13          False                                   0   \n",
      "2              11          False                                   0   \n",
      "3               9          False                                   0   \n",
      "4              19          False                                       \n",
      "\n",
      "   most_common_act_encoded  most_common_emotion_encoded  dialog_length  \\\n",
      "0                        0                            0            728   \n",
      "1                        0                            1            253   \n",
      "2                        0                            1            191   \n",
      "3                        0                            1            146   \n",
      "4                        0                            0            515   \n",
      "\n",
      "   word_count  avg_word_length  \n",
      "0         161         3.465839  \n",
      "1          59         3.203390  \n",
      "2          41         3.560976  \n",
      "3          33         3.333333  \n",
      "4         128         2.960938  \n",
      "\n",
      "Columnas disponibles:\n",
      "['dialog', 'act', 'emotion', 'num_utterances', 'dialog_text', 'first_utterance', 'last_utterance', 'act_counts', 'emotion_counts', 'lengths_match', 'most_common_act', 'most_common_emotion', 'most_common_act_encoded', 'most_common_emotion_encoded', 'dialog_length', 'word_count', 'avg_word_length']\n",
      "Preparando los datos...\n",
      "Usando dialog como entrada y act como salida\n",
      "Vocabulario construido con 10000 palabras\n",
      "Dataloaders creados con batch_size=32\n",
      "\n",
      "===== PARTE 1: MODELOS RNN/LSTM =====\n",
      "\n",
      "Creando modelos RNN/LSTM...\n",
      "\n",
      "Entrenando modelo RNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.43it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0.0m 44.20s\n",
      "\tTrain Loss: 1.381 | Train Acc: 48.70%\n",
      "\tValid Loss: 1.309 | Valid Acc: 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.18it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:03<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0.0m 45.73s\n",
      "\tTrain Loss: 1.257 | Train Acc: 49.05%\n",
      "\tValid Loss: 1.319 | Valid Acc: 44.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:38<00:00,  9.14it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0.0m 40.50s\n",
      "\tTrain Loss: 1.237 | Train Acc: 49.13%\n",
      "\tValid Loss: 1.311 | Valid Acc: 45.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:37<00:00,  9.37it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0.0m 39.63s\n",
      "\tTrain Loss: 1.225 | Train Acc: 49.25%\n",
      "\tValid Loss: 1.270 | Valid Acc: 46.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:36<00:00,  9.47it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0.0m 39.27s\n",
      "\tTrain Loss: 1.217 | Train Acc: 49.41%\n",
      "\tValid Loss: 1.289 | Valid Acc: 46.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:36<00:00,  9.47it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0.0m 39.34s\n",
      "\tTrain Loss: 1.211 | Train Acc: 49.84%\n",
      "\tValid Loss: 1.260 | Valid Acc: 46.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:37<00:00,  9.38it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0.0m 39.62s\n",
      "\tTrain Loss: 1.200 | Train Acc: 50.38%\n",
      "\tValid Loss: 1.256 | Valid Acc: 46.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:37<00:00,  9.40it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0.0m 39.58s\n",
      "\tTrain Loss: 1.196 | Train Acc: 50.42%\n",
      "\tValid Loss: 1.270 | Valid Acc: 47.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:37<00:00,  9.32it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0.0m 39.90s\n",
      "\tTrain Loss: 1.190 | Train Acc: 50.79%\n",
      "\tValid Loss: 1.255 | Valid Acc: 46.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:37<00:00,  9.26it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0.0m 40.04s\n",
      "\tTrain Loss: 1.183 | Train Acc: 51.12%\n",
      "\tValid Loss: 1.260 | Valid Acc: 47.01%\n",
      "\n",
      "Entrenando modelo LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.38it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0.0m 44.10s\n",
      "\tTrain Loss: 1.449 | Train Acc: 47.98%\n",
      "\tValid Loss: 1.309 | Valid Acc: 43.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.42it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0.0m 43.87s\n",
      "\tTrain Loss: 1.259 | Train Acc: 48.94%\n",
      "\tValid Loss: 1.306 | Valid Acc: 45.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.44it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0.0m 43.72s\n",
      "\tTrain Loss: 1.249 | Train Acc: 49.30%\n",
      "\tValid Loss: 1.304 | Valid Acc: 45.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.43it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0.0m 43.87s\n",
      "\tTrain Loss: 1.240 | Train Acc: 49.32%\n",
      "\tValid Loss: 1.283 | Valid Acc: 45.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.44it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0.0m 43.78s\n",
      "\tTrain Loss: 1.220 | Train Acc: 49.80%\n",
      "\tValid Loss: 1.259 | Valid Acc: 46.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.35it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0.0m 44.15s\n",
      "\tTrain Loss: 1.209 | Train Acc: 49.96%\n",
      "\tValid Loss: 1.267 | Valid Acc: 46.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.27it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0.0m 44.74s\n",
      "\tTrain Loss: 1.198 | Train Acc: 50.36%\n",
      "\tValid Loss: 1.259 | Valid Acc: 47.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.28it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0.0m 44.51s\n",
      "\tTrain Loss: 1.188 | Train Acc: 50.73%\n",
      "\tValid Loss: 1.274 | Valid Acc: 47.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.28it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0.0m 44.67s\n",
      "\tTrain Loss: 1.176 | Train Acc: 51.13%\n",
      "\tValid Loss: 1.258 | Valid Acc: 47.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.27it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0.0m 44.71s\n",
      "\tTrain Loss: 1.163 | Train Acc: 51.76%\n",
      "\tValid Loss: 1.259 | Valid Acc: 46.94%\n",
      "\n",
      "Entrenando modelo GRU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:39<00:00,  8.90it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0.0m 41.65s\n",
      "\tTrain Loss: 1.397 | Train Acc: 48.24%\n",
      "\tValid Loss: 1.290 | Valid Acc: 45.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:39<00:00,  8.92it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0.0m 41.52s\n",
      "\tTrain Loss: 1.241 | Train Acc: 49.18%\n",
      "\tValid Loss: 1.295 | Valid Acc: 45.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:39<00:00,  8.90it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0.0m 41.55s\n",
      "\tTrain Loss: 1.225 | Train Acc: 49.52%\n",
      "\tValid Loss: 1.261 | Valid Acc: 46.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:39<00:00,  8.89it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0.0m 41.54s\n",
      "\tTrain Loss: 1.209 | Train Acc: 50.11%\n",
      "\tValid Loss: 1.268 | Valid Acc: 46.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:39<00:00,  8.89it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0.0m 41.55s\n",
      "\tTrain Loss: 1.198 | Train Acc: 50.37%\n",
      "\tValid Loss: 1.274 | Valid Acc: 46.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:39<00:00,  8.90it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 0.0m 41.57s\n",
      "\tTrain Loss: 1.180 | Train Acc: 51.22%\n",
      "\tValid Loss: 1.260 | Valid Acc: 47.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:39<00:00,  8.91it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 0.0m 41.60s\n",
      "\tTrain Loss: 1.158 | Train Acc: 52.05%\n",
      "\tValid Loss: 1.287 | Valid Acc: 47.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:39<00:00,  8.88it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 0.0m 41.70s\n",
      "\tTrain Loss: 1.132 | Train Acc: 53.07%\n",
      "\tValid Loss: 1.261 | Valid Acc: 47.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:38<00:00,  8.95it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 0.0m 41.44s\n",
      "\tTrain Loss: 1.107 | Train Acc: 54.27%\n",
      "\tValid Loss: 1.263 | Valid Acc: 47.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:39<00:00,  8.89it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 0.0m 41.76s\n",
      "\tTrain Loss: 1.077 | Train Acc: 55.59%\n",
      "\tValid Loss: 1.298 | Valid Acc: 47.91%\n",
      "\n",
      "Evaluando modelo RNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.193 | Test Acc: 50.72%\n",
      "Métricas adicionales:\n",
      "BLEU: 0.3298\n",
      "ROUGE-1: 0.8106\n",
      "ROUGE-2: 0.4866\n",
      "ROUGE-L: 0.7924\n",
      "Precision: 0.5397\n",
      "Recall: 0.4815\n",
      "F1: 0.4674\n",
      "Accuracy: 0.5072\n",
      "\n",
      "Evaluando modelo LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.192 | Test Acc: 50.19%\n",
      "Métricas adicionales:\n",
      "BLEU: 0.3521\n",
      "ROUGE-1: 0.7815\n",
      "ROUGE-2: 0.4873\n",
      "ROUGE-L: 0.7666\n",
      "Precision: 0.5397\n",
      "Recall: 0.4606\n",
      "F1: 0.4525\n",
      "Accuracy: 0.5019\n",
      "\n",
      "Evaluando modelo GRU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.192 | Test Acc: 51.01%\n",
      "Métricas adicionales:\n",
      "BLEU: 0.2983\n",
      "ROUGE-1: 0.7368\n",
      "ROUGE-2: 0.4584\n",
      "ROUGE-L: 0.7105\n",
      "Precision: 0.5300\n",
      "Recall: 0.4499\n",
      "F1: 0.4352\n",
      "Accuracy: 0.5101\n",
      "\n",
      "Analizando ejemplos específicos con el modelo LSTM...\n",
      "\n",
      "Análisis de ejemplos específicos:\n",
      "\n",
      "Ejemplo 1:\n",
      "Entrada: [ 'hey man , you wan na buy some <UNK> ? some what ? <UNK> ! you know ? pot , <UNK> , mary jane some <UNK> ! oh , umm , no thanks . i also have blow if you prefer to do a few lines . no , i am ok , really . come on man ! i even got <UNK> and <UNK> ! try some ! do you really have all of these drugs ? where do you get them from ? i got my connections ! just tell me what you want and\n",
      "Objetivo: [ 3 2 3 4 3 4 3 2 3 4 2 3 ]\n",
      "Predicción: [ 2 1 2 1 1 1 1 1 1 1 ] 1 ]\n",
      "\n",
      "Ejemplo 2:\n",
      "Entrada: [ 'the taxi drivers are on strike again . what for ? they want the government to reduce the price of the gasoline . it is really a hot potato . ' ]\n",
      "Objetivo: [ 1 2 1 1 ]\n",
      "Predicción: [ 1 2 1 2 1 1 1 1 1 1\n",
      "\n",
      "Ejemplo 3:\n",
      "Entrada: [ `` we 've managed to reduce our energy consumption in our factory by about 15 per cent in the last two years . that 's excellent . how have you managed that ? mainly because we 've invested in a heat recovery system . what does that mean exactly ? well , we use the exhaust <UNK> from our printing <UNK> to provide energy to heat our dryers . what other sources of energy do you use ? we do n't use any <UNK> <UNK> . most of our power comes from <UNK> plants . we 're\n",
      "Objetivo: [ 1 2 1 2 1 2 1 ]\n",
      "Predicción: [ 2 2 1 1 1 1 1 1 ]\n",
      "\n",
      "Ejemplo 4:\n",
      "Entrada: [ `` believe it or not , tea is the most popular beverage in the world after water . well , people from asia to europe all enjoy tea . right . and china is the <UNK> of tea . yes , chinese people love drinking tea so much . some even claim they ca n't live without tea . do you know there are several <UNK> of chinese tea ? yes , i believe there are green <UNK> , black <UNK> and <UNK> <UNK> . any others ? well , have you ever heard of <UNK> tea\n",
      "Objetivo: [ 1 1 1 1 2 2 2 2 1 1 1 3 4 3 ]\n",
      "Predicción: [ 2 1 2 1 1 1 1 1 ]\n",
      "\n",
      "Ejemplo 5:\n",
      "Entrada: [ 'what are your personal weaknesses ? i ’ m afraid i ’ m a poor <UNK> . i ’ m not comfortable talking with the people whom i have just met for the first time . that is not very good for business , so i have been studying public speaking . are you more of a leader or a follower ? i don ’ t try to lead people . i ’ d rather cooperate with everybody , and get the job done by working together . do you think you can make yourself easily understood\n",
      "Objetivo: [ 2 1 2 1 2 1 2 1 ]\n",
      "Predicción: [ 2 1 2 1 2 1 2 1 1 1 ]\n",
      "\n",
      "Analizando impacto de hiperparámetros en el modelo LSTM...\n",
      "\n",
      "Entrenando modelo con n_layers=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:34<00:00,  9.96it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0.0m 37.40s\n",
      "\tTrain Loss: 1.437 | Train Acc: 48.75%\n",
      "\tValid Loss: 1.342 | Valid Acc: 45.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:35<00:00,  9.92it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0.0m 37.44s\n",
      "\tTrain Loss: 1.230 | Train Acc: 49.48%\n",
      "\tValid Loss: 1.270 | Valid Acc: 46.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:34<00:00, 10.18it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0.0m 36.56s\n",
      "\tTrain Loss: 1.212 | Train Acc: 49.90%\n",
      "\tValid Loss: 1.275 | Valid Acc: 46.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:34<00:00, 10.16it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0.0m 36.62s\n",
      "\tTrain Loss: 1.200 | Train Acc: 50.22%\n",
      "\tValid Loss: 1.263 | Valid Acc: 46.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:34<00:00, 10.05it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0.0m 36.98s\n",
      "\tTrain Loss: 1.191 | Train Acc: 50.65%\n",
      "\tValid Loss: 1.257 | Valid Acc: 46.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.202 | Test Acc: 49.94%\n",
      "Métricas adicionales:\n",
      "BLEU: 0.2834\n",
      "ROUGE-1: 0.7260\n",
      "ROUGE-2: 0.4556\n",
      "ROUGE-L: 0.6996\n",
      "Precision: 0.5333\n",
      "Recall: 0.4152\n",
      "F1: 0.3944\n",
      "Accuracy: 0.4994\n",
      "\n",
      "Entrenando modelo con n_layers=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.38it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0.0m 44.20s\n",
      "\tTrain Loss: 1.449 | Train Acc: 47.73%\n",
      "\tValid Loss: 1.321 | Valid Acc: 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.43it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0.0m 43.92s\n",
      "\tTrain Loss: 1.260 | Train Acc: 48.92%\n",
      "\tValid Loss: 1.316 | Valid Acc: 44.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.32it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0.0m 44.45s\n",
      "\tTrain Loss: 1.248 | Train Acc: 49.17%\n",
      "\tValid Loss: 1.290 | Valid Acc: 45.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.25it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0.0m 44.80s\n",
      "\tTrain Loss: 1.233 | Train Acc: 49.57%\n",
      "\tValid Loss: 1.294 | Valid Acc: 45.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.28it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0.0m 44.63s\n",
      "\tTrain Loss: 1.220 | Train Acc: 49.87%\n",
      "\tValid Loss: 1.281 | Valid Acc: 46.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.211 | Test Acc: 50.49%\n",
      "Métricas adicionales:\n",
      "BLEU: 0.2810\n",
      "ROUGE-1: 0.7491\n",
      "ROUGE-2: 0.4489\n",
      "ROUGE-L: 0.7305\n",
      "Precision: 0.5442\n",
      "Recall: 0.4431\n",
      "F1: 0.4293\n",
      "Accuracy: 0.5049\n",
      "\n",
      "Entrenando modelo con n_layers=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:48<00:00,  7.21it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0.0m 51.06s\n",
      "\tTrain Loss: 1.473 | Train Acc: 47.16%\n",
      "\tValid Loss: 1.317 | Valid Acc: 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:48<00:00,  7.20it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0.0m 51.08s\n",
      "\tTrain Loss: 1.265 | Train Acc: 48.97%\n",
      "\tValid Loss: 1.398 | Valid Acc: 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:48<00:00,  7.22it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0.0m 50.95s\n",
      "\tTrain Loss: 1.265 | Train Acc: 48.93%\n",
      "\tValid Loss: 1.332 | Valid Acc: 45.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:48<00:00,  7.21it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0.0m 51.12s\n",
      "\tTrain Loss: 1.259 | Train Acc: 49.02%\n",
      "\tValid Loss: 1.331 | Valid Acc: 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:48<00:00,  7.21it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0.0m 51.19s\n",
      "\tTrain Loss: 1.261 | Train Acc: 48.94%\n",
      "\tValid Loss: 1.311 | Valid Acc: 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.250 | Test Acc: 49.01%\n",
      "Métricas adicionales:\n",
      "BLEU: 0.2027\n",
      "ROUGE-1: 0.7100\n",
      "ROUGE-2: 0.4098\n",
      "ROUGE-L: 0.6843\n",
      "Precision: 0.3587\n",
      "Recall: 0.3933\n",
      "F1: 0.3550\n",
      "Accuracy: 0.4901\n",
      "\n",
      "Entrenando modelo con n_layers=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:55<00:00,  6.32it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0.0m 58.13s\n",
      "\tTrain Loss: 1.470 | Train Acc: 47.47%\n",
      "\tValid Loss: 1.310 | Valid Acc: 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:55<00:00,  6.29it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0.0m 58.26s\n",
      "\tTrain Loss: 1.263 | Train Acc: 48.98%\n",
      "\tValid Loss: 1.330 | Valid Acc: 45.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:55<00:00,  6.28it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0.0m 58.29s\n",
      "\tTrain Loss: 1.261 | Train Acc: 48.93%\n",
      "\tValid Loss: 1.326 | Valid Acc: 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:55<00:00,  6.30it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:03<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0.0m 58.38s\n",
      "\tTrain Loss: 1.261 | Train Acc: 48.90%\n",
      "\tValid Loss: 1.307 | Valid Acc: 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:55<00:00,  6.33it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0.0m 57.93s\n",
      "\tTrain Loss: 1.260 | Train Acc: 48.84%\n",
      "\tValid Loss: 1.333 | Valid Acc: 43.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.247 | Test Acc: 49.01%\n",
      "Métricas adicionales:\n",
      "BLEU: 0.2027\n",
      "ROUGE-1: 0.7100\n",
      "ROUGE-2: 0.4098\n",
      "ROUGE-L: 0.6843\n",
      "Precision: 0.3587\n",
      "Recall: 0.3933\n",
      "F1: 0.3550\n",
      "Accuracy: 0.4901\n",
      "\n",
      "Entrenando modelo con learning_rate=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.26it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0.0m 44.74s\n",
      "\tTrain Loss: 2.339 | Train Acc: 44.46%\n",
      "\tValid Loss: 1.361 | Valid Acc: 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.25it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0.0m 44.84s\n",
      "\tTrain Loss: 1.276 | Train Acc: 49.04%\n",
      "\tValid Loss: 1.313 | Valid Acc: 44.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.24it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0.0m 44.87s\n",
      "\tTrain Loss: 1.259 | Train Acc: 49.12%\n",
      "\tValid Loss: 1.304 | Valid Acc: 45.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.24it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0.0m 44.87s\n",
      "\tTrain Loss: 1.252 | Train Acc: 49.21%\n",
      "\tValid Loss: 1.309 | Valid Acc: 45.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.23it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0.0m 44.90s\n",
      "\tTrain Loss: 1.248 | Train Acc: 49.30%\n",
      "\tValid Loss: 1.296 | Valid Acc: 45.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.238 | Test Acc: 49.60%\n",
      "Métricas adicionales:\n",
      "BLEU: 0.2130\n",
      "ROUGE-1: 0.7100\n",
      "ROUGE-2: 0.4098\n",
      "ROUGE-L: 0.6843\n",
      "Precision: 0.4058\n",
      "Recall: 0.4103\n",
      "F1: 0.3837\n",
      "Accuracy: 0.4960\n",
      "\n",
      "Entrenando modelo con learning_rate=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.25it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0.0m 44.87s\n",
      "\tTrain Loss: 1.451 | Train Acc: 47.76%\n",
      "\tValid Loss: 1.322 | Valid Acc: 44.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.25it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0.0m 44.77s\n",
      "\tTrain Loss: 1.259 | Train Acc: 48.99%\n",
      "\tValid Loss: 1.319 | Valid Acc: 44.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.26it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0.0m 44.81s\n",
      "\tTrain Loss: 1.246 | Train Acc: 49.13%\n",
      "\tValid Loss: 1.284 | Valid Acc: 45.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.24it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0.0m 44.85s\n",
      "\tTrain Loss: 1.231 | Train Acc: 49.54%\n",
      "\tValid Loss: 1.268 | Valid Acc: 46.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.27it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0.0m 44.52s\n",
      "\tTrain Loss: 1.218 | Train Acc: 49.96%\n",
      "\tValid Loss: 1.269 | Valid Acc: 45.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.214 | Test Acc: 50.27%\n",
      "Métricas adicionales:\n",
      "BLEU: 0.3016\n",
      "ROUGE-1: 0.7439\n",
      "ROUGE-2: 0.4588\n",
      "ROUGE-L: 0.7272\n",
      "Precision: 0.4368\n",
      "Recall: 0.4408\n",
      "F1: 0.4263\n",
      "Accuracy: 0.5027\n",
      "\n",
      "Entrenando modelo con learning_rate=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.27it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0.0m 44.70s\n",
      "\tTrain Loss: 1.343 | Train Acc: 47.16%\n",
      "\tValid Loss: 1.411 | Valid Acc: 42.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.21it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0.0m 45.00s\n",
      "\tTrain Loss: 1.288 | Train Acc: 47.85%\n",
      "\tValid Loss: 1.325 | Valid Acc: 40.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.38it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0.0m 44.18s\n",
      "\tTrain Loss: 1.281 | Train Acc: 48.02%\n",
      "\tValid Loss: 1.334 | Valid Acc: 44.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.43it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0.0m 43.82s\n",
      "\tTrain Loss: 1.287 | Train Acc: 47.60%\n",
      "\tValid Loss: 1.365 | Valid Acc: 43.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.45it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0.0m 43.65s\n",
      "\tTrain Loss: 1.276 | Train Acc: 47.79%\n",
      "\tValid Loss: 1.347 | Valid Acc: 42.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.272 | Test Acc: 44.50%\n",
      "Métricas adicionales:\n",
      "BLEU: 0.2279\n",
      "ROUGE-1: 0.8244\n",
      "ROUGE-2: 0.4528\n",
      "ROUGE-L: 0.8088\n",
      "Precision: 0.3845\n",
      "Recall: 0.4332\n",
      "F1: 0.3852\n",
      "Accuracy: 0.4450\n",
      "\n",
      "Entrenando modelo con learning_rate=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.43it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0.0m 43.87s\n",
      "\tTrain Loss: 1.772 | Train Acc: 41.99%\n",
      "\tValid Loss: 1.335 | Valid Acc: 44.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.42it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 0.0m 43.80s\n",
      "\tTrain Loss: 1.516 | Train Acc: 42.32%\n",
      "\tValid Loss: 1.575 | Valid Acc: 43.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:41<00:00,  8.31it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 0.0m 44.55s\n",
      "\tTrain Loss: 1.387 | Train Acc: 44.06%\n",
      "\tValid Loss: 1.526 | Valid Acc: 42.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.26it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 0.0m 44.68s\n",
      "\tTrain Loss: 1.348 | Train Acc: 44.70%\n",
      "\tValid Loss: 1.447 | Valid Acc: 44.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 348/348 [00:42<00:00,  8.26it/s]\n",
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 0.0m 44.65s\n",
      "\tTrain Loss: 1.368 | Train Acc: 44.21%\n",
      "\tValid Loss: 1.440 | Valid Acc: 44.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.269 | Test Acc: 49.00%\n",
      "Métricas adicionales:\n",
      "BLEU: 0.2040\n",
      "ROUGE-1: 0.7100\n",
      "ROUGE-2: 0.4106\n",
      "ROUGE-L: 0.6843\n",
      "Precision: 0.3585\n",
      "Recall: 0.3934\n",
      "F1: 0.3552\n",
      "Accuracy: 0.4900\n",
      "\n",
      "===== PARTE 2: MODELO TRANSFORMER =======\n",
      "\n",
      "Creando modelo Transformer...\n",
      "\n",
      "Entrenando modelo Transformer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/348 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Module [TransformerModel] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 996\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# Entrenar modelo\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEntrenando modelo Transformer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 996\u001b[0m transformer_model, transformer_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;66;03m# Visualizar historial de entrenamiento\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m plot_training_history(transformer_history, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 554\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, n_epochs, device, model_name)\u001b[0m\n\u001b[0;32m    551\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# Entrenar una época\u001b[39;00m\n\u001b[1;32m--> 554\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# Evaluar en conjunto de validación\u001b[39;00m\n\u001b[0;32m    557\u001b[0m valid_loss, valid_acc, _, _ \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[5], line 388\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m    385\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    387\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 388\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Reshape para calcular pérdida\u001b[39;00m\n\u001b[0;32m    391\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\patri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:246\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [TransformerModel] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "# Implementación de Modelos RNN/LSTM y Transformer para NLP\n",
    "# Basado en la rúbrica de evaluación proporcionada\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import math\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Ignorar advertencias\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar disponibilidad de GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Utilizando dispositivo: {device}\")\n",
    "\n",
    "# Descargar recursos de NLTK si es necesario\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Configuración de semilla para reproducibilidad\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Cargar la rúbrica de evaluación\n",
    "try:\n",
    "    with open('rubrica_evaluacion.json', 'r', encoding='utf-8') as f:\n",
    "        rubrica = json.load(f)\n",
    "    print(\"Rúbrica de evaluación cargada correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar la rúbrica: {e}\")\n",
    "    rubrica = {\"rubrica\": {\"metricas_evaluacion\": {\"rnn_lstm\": [\"accuracy\", \"precision\", \"recall\", \"F1-score\"], \n",
    "                                                  \"transformer\": [\"BLEU Score\", \"ROUGE\"]}}}\n",
    "\n",
    "# Cargar los datos desde archivos parquet\n",
    "print(\"Cargando datos...\")\n",
    "try:\n",
    "    train_data = pd.read_parquet('train.parquet')\n",
    "    val_data = pd.read_parquet('validation.parquet')\n",
    "    test_data = pd.read_parquet('test.parquet')\n",
    "    print(f\"Datos cargados: {len(train_data)} ejemplos de entrenamiento, {len(val_data)} de validación, {len(test_data)} de prueba\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar los datos: {e}\")\n",
    "    print(\"Generando datos sintéticos para demostración...\")\n",
    "    # Generar datos sintéticos para demostración\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "    \n",
    "    # Crear DataFrame con textos y etiquetas\n",
    "    data = pd.DataFrame({\n",
    "        'text': newsgroups.data[:1000],\n",
    "        'target': newsgroups.target[:1000]\n",
    "    })\n",
    "    \n",
    "    # Dividir en train, val, test\n",
    "    train_data, temp_data = train_test_split(data, test_size=0.3, random_state=SEED)\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=SEED)\n",
    "    \n",
    "    print(f\"Datos sintéticos generados: {len(train_data)} ejemplos de entrenamiento, {len(val_data)} de validación, {len(test_data)} de prueba\")\n",
    "\n",
    "# Mostrar información sobre los datos\n",
    "print(\"\\nEstructura de los datos de entrenamiento:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nColumnas disponibles:\")\n",
    "print(train_data.columns.tolist())\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "class TextProcessor:\n",
    "    def __init__(self, max_vocab_size=10000, max_seq_length=100):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}\n",
    "        self.word_freq = {}\n",
    "        self.vocab_size = 4  # Inicialmente tenemos 4 tokens especiales\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"Construye el vocabulario a partir de los textos de entrenamiento\"\"\"\n",
    "        # Contar frecuencia de palabras\n",
    "        for text in texts:\n",
    "            if isinstance(text, str):  # Asegurarse de que el texto es una cadena\n",
    "                for word in nltk.word_tokenize(text.lower()):\n",
    "                    if word not in self.word_freq:\n",
    "                        self.word_freq[word] = 1\n",
    "                    else:\n",
    "                        self.word_freq[word] += 1\n",
    "        \n",
    "        # Ordenar palabras por frecuencia (descendente)\n",
    "        sorted_words = sorted(self.word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Añadir palabras al vocabulario (limitado por max_vocab_size)\n",
    "        for word, freq in sorted_words[:self.max_vocab_size - 4]:  # -4 por los tokens especiales\n",
    "            self.word2idx[word] = self.vocab_size\n",
    "            self.idx2word[self.vocab_size] = word\n",
    "            self.vocab_size += 1\n",
    "            \n",
    "        print(f\"Vocabulario construido con {self.vocab_size} palabras\")\n",
    "        \n",
    "    def text_to_indices(self, text, add_special_tokens=False):\n",
    "        \"\"\"Convierte un texto en una secuencia de índices\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            \n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        indices = []\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            indices.append(self.word2idx['<SOS>'])\n",
    "            \n",
    "        for token in tokens[:self.max_seq_length - 2 if add_special_tokens else self.max_seq_length]:\n",
    "            if token in self.word2idx:\n",
    "                indices.append(self.word2idx[token])\n",
    "            else:\n",
    "                indices.append(self.word2idx['<UNK>'])\n",
    "                \n",
    "        if add_special_tokens:\n",
    "            indices.append(self.word2idx['<EOS>'])\n",
    "            \n",
    "        # Padding\n",
    "        if len(indices) < self.max_seq_length:\n",
    "            indices += [self.word2idx['<PAD>']] * (self.max_seq_length - len(indices))\n",
    "        else:\n",
    "            indices = indices[:self.max_seq_length]\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def indices_to_text(self, indices):\n",
    "        \"\"\"Convierte una secuencia de índices en texto\"\"\"\n",
    "        tokens = []\n",
    "        for idx in indices:\n",
    "            if idx == self.word2idx['<PAD>'] or idx == self.word2idx['<EOS>']:\n",
    "                break\n",
    "            if idx != self.word2idx['<SOS>']:\n",
    "                tokens.append(self.idx2word.get(idx, '<UNK>'))\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Preparar los datos\n",
    "print(\"Preparando los datos...\")\n",
    "\n",
    "# Determinar las columnas de entrada y salida según la estructura de los datos\n",
    "# Esto puede necesitar ajustes según tus datos específicos\n",
    "if 'text' in train_data.columns and 'target' in train_data.columns:\n",
    "    input_col = 'text'\n",
    "    output_col = 'target'\n",
    "elif len(train_data.columns) >= 2:\n",
    "    input_col = train_data.columns[0]\n",
    "    output_col = train_data.columns[1]\n",
    "else:\n",
    "    input_col = train_data.columns[0]\n",
    "    output_col = train_data.columns[0]  # Usar la misma columna como entrada y salida\n",
    "\n",
    "print(f\"Usando {input_col} como entrada y {output_col} como salida\")\n",
    "\n",
    "# Inicializar el procesador de texto\n",
    "text_processor = TextProcessor(max_vocab_size=10000, max_seq_length=100)\n",
    "\n",
    "# Construir vocabulario con los datos de entrenamiento\n",
    "all_texts = []\n",
    "for text in train_data[input_col]:\n",
    "    if isinstance(text, str):\n",
    "        all_texts.append(text)\n",
    "    else:\n",
    "        all_texts.append(str(text))\n",
    "\n",
    "if input_col != output_col:\n",
    "    for text in train_data[output_col]:\n",
    "        if isinstance(text, str):\n",
    "            all_texts.append(text)\n",
    "        else:\n",
    "            all_texts.append(str(text))\n",
    "\n",
    "text_processor.build_vocab(all_texts)\n",
    "\n",
    "# Clase de Dataset personalizada para secuencias\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, input_texts, output_texts, text_processor, is_transformer=False):\n",
    "        self.input_texts = input_texts\n",
    "        self.output_texts = output_texts\n",
    "        self.text_processor = text_processor\n",
    "        self.is_transformer = is_transformer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.input_texts[idx]\n",
    "        output_text = self.output_texts[idx]\n",
    "        \n",
    "        # Convertir textos a secuencias de índices\n",
    "        input_indices = self.text_processor.text_to_indices(input_text, add_special_tokens=True)\n",
    "        output_indices = self.text_processor.text_to_indices(output_text, add_special_tokens=True)\n",
    "        \n",
    "        # Convertir a tensores\n",
    "        input_tensor = torch.tensor(input_indices, dtype=torch.long)\n",
    "        output_tensor = torch.tensor(output_indices, dtype=torch.long)\n",
    "        \n",
    "        if self.is_transformer:\n",
    "            # Para transformer, necesitamos máscaras de atención\n",
    "            input_mask = (input_tensor != self.text_processor.word2idx['<PAD>']).float()\n",
    "            output_mask = (output_tensor != self.text_processor.word2idx['<PAD>']).float()\n",
    "            return input_tensor, output_tensor, input_mask, output_mask\n",
    "        else:\n",
    "            return input_tensor, output_tensor\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = SequenceDataset(\n",
    "    train_data[input_col].tolist(),\n",
    "    train_data[output_col].tolist(),\n",
    "    text_processor\n",
    ")\n",
    "\n",
    "val_dataset = SequenceDataset(\n",
    "    val_data[input_col].tolist(),\n",
    "    val_data[output_col].tolist(),\n",
    "    text_processor\n",
    ")\n",
    "\n",
    "test_dataset = SequenceDataset(\n",
    "    test_data[input_col].tolist(),\n",
    "    test_data[output_col].tolist(),\n",
    "    text_processor\n",
    ")\n",
    "\n",
    "# Crear dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Dataloaders creados con batch_size={batch_size}\")\n",
    "\n",
    "# Definición de modelos\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src = [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs = [batch_size, src_len, hidden_dim]\n",
    "        # hidden = [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        predictions = self.fc_out(outputs)\n",
    "        # predictions = [batch_size, src_len, output_dim]\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, dropout, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src = [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # outputs = [batch_size, src_len, hidden_dim * n_directions]\n",
    "        # hidden = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        # cell = [n_layers * n_directions, batch_size, hidden_dim]\n",
    "        \n",
    "        predictions = self.fc_out(outputs)\n",
    "        # predictions = [batch_size, src_len, output_dim]\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src = [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        # outputs = [batch_size, src_len, hidden_dim]\n",
    "        # hidden = [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        predictions = self.fc_out(outputs)\n",
    "        # predictions = [batch_size, src_len, output_dim]\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, n_layers, n_heads, dropout, max_length=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, dropout)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, \n",
    "                                                   dim_feedforward=hidden_dim, dropout=dropout,\n",
    "                                                   batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "def forward(self, src, src_mask=None):\n",
    "    # src = [batch_size, src_len]\n",
    "    embedded = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)\n",
    "    # embedded = [batch_size, src_len, emb_dim]\n",
    "    \n",
    "    embedded = self.pos_encoder(embedded)\n",
    "    \n",
    "    # Corregir la máscara de padding\n",
    "    if src_mask is None:\n",
    "        # Crear máscara de padding (1 para tokens reales, 0 para padding)\n",
    "        src_key_padding_mask = (src == 0)  # [batch_size, src_len]\n",
    "    else:\n",
    "        src_key_padding_mask = src_mask\n",
    "    \n",
    "    outputs = self.transformer_encoder(embedded, src_key_padding_mask=src_key_padding_mask)\n",
    "    # outputs = [batch_size, src_len, emb_dim]\n",
    "    \n",
    "    predictions = self.fc_out(outputs)\n",
    "    # predictions = [batch_size, src_len, output_dim]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Funciones de entrenamiento y evaluación\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, (src, trg) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src)\n",
    "        \n",
    "        # Reshape para calcular pérdida\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.view(-1, output_dim)\n",
    "        trg = trg.view(-1)\n",
    "        \n",
    "        # Calcular pérdida\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Actualizar pesos\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calcular precisión\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct = (predicted == trg).float()\n",
    "        mask = (trg != 0).float()  # Ignorar padding\n",
    "        correct = (correct * mask).sum().item()\n",
    "        total = mask.sum().item()\n",
    "        \n",
    "        # Actualizar métricas\n",
    "        epoch_loss += loss.item() * src.size(0)\n",
    "        epoch_acc += correct\n",
    "        total_samples += total\n",
    "    \n",
    "    return epoch_loss / len(dataloader.dataset), epoch_acc / total_samples\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_trgs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (src, trg) in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src)\n",
    "            \n",
    "            # Reshape para calcular pérdida\n",
    "            output_dim = output.shape[-1]\n",
    "            output_flat = output.view(-1, output_dim)\n",
    "            trg_flat = trg.view(-1)\n",
    "            \n",
    "            # Calcular pérdida\n",
    "            loss = criterion(output_flat, trg_flat)\n",
    "            \n",
    "            # Calcular precisión\n",
    "            _, predicted = torch.max(output_flat, 1)\n",
    "            correct = (predicted == trg_flat).float()\n",
    "            mask = (trg_flat != 0).float()  # Ignorar padding\n",
    "            correct = (correct * mask).sum().item()\n",
    "            total = mask.sum().item()\n",
    "            \n",
    "            # Actualizar métricas\n",
    "            epoch_loss += loss.item() * src.size(0)\n",
    "            epoch_acc += correct\n",
    "            total_samples += total\n",
    "            \n",
    "            # Guardar predicciones y targets para calcular métricas adicionales\n",
    "            for i in range(src.size(0)):\n",
    "                pred_seq = torch.argmax(output[i], dim=1).cpu().numpy()\n",
    "                trg_seq = trg[i].cpu().numpy()\n",
    "                \n",
    "                # Filtrar padding\n",
    "                pred_seq = pred_seq[trg_seq != 0]\n",
    "                trg_seq = trg_seq[trg_seq != 0]\n",
    "                \n",
    "                all_preds.append(pred_seq)\n",
    "                all_trgs.append(trg_seq)\n",
    "    \n",
    "    return epoch_loss / len(dataloader.dataset), epoch_acc / total_samples, all_preds, all_trgs\n",
    "\n",
    "def calculate_metrics(predictions, targets, idx2word):\n",
    "    \"\"\"\n",
    "    Calcula métricas adicionales como F1, precisión, recall y BLEU/ROUGE\n",
    "    \"\"\"\n",
    "    # Convertir índices a palabras\n",
    "    pred_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_text = [idx2word.get(idx, '<UNK>') for idx in pred if idx > 3]  # Ignorar tokens especiales\n",
    "        target_text = [idx2word.get(idx, '<UNK>') for idx in target if idx > 3]  # Ignorar tokens especiales\n",
    "        \n",
    "        pred_texts.append(pred_text)\n",
    "        target_texts.append([target_text])  # BLEU espera una lista de referencias\n",
    "    \n",
    "    # Calcular BLEU\n",
    "    try:\n",
    "        smoothie = SmoothingFunction().method1\n",
    "        bleu_score = corpus_bleu(target_texts, pred_texts, smoothing_function=smoothie)\n",
    "    except:\n",
    "        bleu_score = 0\n",
    "    \n",
    "    # Calcular ROUGE\n",
    "    try:\n",
    "        rouge = Rouge()\n",
    "        \n",
    "        # Convertir listas de tokens a strings\n",
    "        pred_strings = [' '.join(pred) for pred in pred_texts]\n",
    "        target_strings = [' '.join(target[0]) for target in target_texts]\n",
    "        \n",
    "        # Asegurarse de que no hay strings vacíos\n",
    "        valid_pairs = [(p, t) for p, t in zip(pred_strings, target_strings) if p and t]\n",
    "        \n",
    "        if valid_pairs:\n",
    "            pred_valid, target_valid = zip(*valid_pairs)\n",
    "            rouge_scores = rouge.get_scores(pred_valid, target_valid, avg=True)\n",
    "            rouge_1 = rouge_scores['rouge-1']['f']\n",
    "            rouge_2 = rouge_scores['rouge-2']['f']\n",
    "            rouge_l = rouge_scores['rouge-l']['f']\n",
    "        else:\n",
    "            rouge_1 = rouge_2 = rouge_l = 0\n",
    "    except:\n",
    "        rouge_1 = rouge_2 = rouge_l = 0\n",
    "    \n",
    "    # Calcular precisión, recall y F1 (para tareas de clasificación)\n",
    "    # Aplanar todas las predicciones y targets\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        all_preds.extend(pred)\n",
    "        all_targets.extend(target)\n",
    "    \n",
    "    try:\n",
    "        precision = precision_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "    except:\n",
    "        precision = recall = f1 = accuracy = 0\n",
    "    \n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'rouge-1': rouge_1,\n",
    "        'rouge-2': rouge_2,\n",
    "        'rouge-l': rouge_l,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, n_epochs, device, model_name):\n",
    "    \"\"\"\n",
    "    Entrena un modelo y guarda el mejor modelo basado en la pérdida de validación\n",
    "    \"\"\"\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Entrenar una época\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Evaluar en conjunto de validación\n",
    "        valid_loss, valid_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Guardar métricas\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "        \n",
    "        # Guardar el mejor modelo\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f'{model_name}_best.pt')\n",
    "        \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    # Cargar el mejor modelo\n",
    "    model.load_state_dict(torch.load(f'{model_name}_best.pt'))\n",
    "    \n",
    "    # Devolver historiales para visualización\n",
    "    history = {\n",
    "        'train_loss': train_losses,\n",
    "        'train_acc': train_accs,\n",
    "        'val_loss': valid_losses,\n",
    "        'val_acc': valid_accs\n",
    "    }\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, idx2word):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo en el conjunto de prueba y calcula métricas adicionales\n",
    "    \"\"\"\n",
    "    test_loss, test_acc, all_preds, all_trgs = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "    \n",
    "    # Calcular métricas adicionales\n",
    "    metrics = calculate_metrics(all_preds, all_trgs, idx2word)\n",
    "    \n",
    "    print(f\"Métricas adicionales:\")\n",
    "    print(f\"BLEU: {metrics['bleu']:.4f}\")\n",
    "    print(f\"ROUGE-1: {metrics['rouge-1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {metrics['rouge-2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {metrics['rouge-l']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1: {metrics['f1']:.4f}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Visualiza el historial de entrenamiento\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Gráfico de pérdida\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Gráfico de precisión\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_history.png')\n",
    "    plt.close()\n",
    "\n",
    "def compare_models(metrics_dict, model_names, metric_names):\n",
    "    \"\"\"\n",
    "    Compara diferentes modelos según varias métricas\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, metric in enumerate(metric_names):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        values = [metrics_dict[model][metric] for model in model_names]\n",
    "        \n",
    "        # Crear gráfico de barras\n",
    "        bars = plt.bar(model_names, values)\n",
    "        \n",
    "        # Añadir valores sobre las barras\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.4f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.title(metric.capitalize())\n",
    "        plt.ylabel('Value')\n",
    "        plt.ylim(0, max(values) * 1.2)  # Ajustar límite vertical\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_hyperparameters(model_class, train_loader, val_loader, test_loader, text_processor, \n",
    "                           param_name, param_values, fixed_params, n_epochs, device):\n",
    "    \"\"\"\n",
    "    Analiza el impacto de un hiperparámetro específico\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"\\nEntrenando modelo con {param_name}={value}\")\n",
    "        \n",
    "        # Crear modelo con el valor actual del hiperparámetro\n",
    "        params = fixed_params.copy()\n",
    "        params[param_name] = value\n",
    "        \n",
    "        if model_class.__name__ == 'TransformerModel':\n",
    "            model = model_class(\n",
    "                input_dim=text_processor.vocab_size,\n",
    "                emb_dim=params['emb_dim'],\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                output_dim=params['output_dim'],\n",
    "                n_layers=params['n_layers'],\n",
    "                n_heads=params['n_heads'],\n",
    "                dropout=params['dropout']\n",
    "            ).to(device)\n",
    "        else:\n",
    "            model = model_class(\n",
    "                input_dim=text_processor.vocab_size,\n",
    "                emb_dim=params['emb_dim'],\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                output_dim=params['output_dim'],\n",
    "                n_layers=params['n_layers'],\n",
    "                dropout=params['dropout']\n",
    "            ).to(device)\n",
    "        \n",
    "        # Crear optimizador\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "        # Criterio de pérdida\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        model, history = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            n_epochs=n_epochs,\n",
    "            device=device,\n",
    "            model_name=f\"{model_class.__name__}_{param_name}_{value}\"\n",
    "        )\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        metrics = evaluate_model(\n",
    "            model=model,\n",
    "            test_loader=test_loader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            idx2word=text_processor.idx2word\n",
    "        )\n",
    "        \n",
    "        # Guardar resultados\n",
    "        results[value] = {\n",
    "            'metrics': metrics,\n",
    "            'history': history\n",
    "        }\n",
    "    \n",
    "    # Visualizar resultados\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Métricas a visualizar\n",
    "    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        \n",
    "        values = [results[param_value]['metrics'][metric] for param_value in param_values]\n",
    "        \n",
    "        plt.plot(param_values, values, 'o-', linewidth=2)\n",
    "        plt.title(f'Impact of {param_name} on {metric.capitalize()}')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Añadir valores sobre los puntos\n",
    "        for j, val in enumerate(values):\n",
    "            plt.text(param_values[j], val + 0.01, f'{val:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'impact_{param_name}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_examples(model, dataloader, text_processor, device, num_examples=5):\n",
    "    \"\"\"\n",
    "    Analiza ejemplos específicos para entender el comportamiento del modelo\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            if len(examples) >= num_examples:\n",
    "                break\n",
    "                \n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src)\n",
    "            \n",
    "            # Obtener predicciones\n",
    "            predictions = torch.argmax(output, dim=2)\n",
    "            \n",
    "            # Analizar cada ejemplo en el batch\n",
    "            for i in range(src.size(0)):\n",
    "                if len(examples) >= num_examples:\n",
    "                    break\n",
    "                    \n",
    "                input_text = text_processor.indices_to_text(src[i].cpu().numpy())\n",
    "                target_text = text_processor.indices_to_text(trg[i].cpu().numpy())\n",
    "                pred_text = text_processor.indices_to_text(predictions[i].cpu().numpy())\n",
    "                \n",
    "                examples.append({\n",
    "                    'input': input_text,\n",
    "                    'target': target_text,\n",
    "                    'prediction': pred_text\n",
    "                })\n",
    "    \n",
    "    # Mostrar ejemplos\n",
    "    print(\"\\nAnálisis de ejemplos específicos:\")\n",
    "    for i, example in enumerate(examples):\n",
    "        print(f\"\\nEjemplo {i+1}:\")\n",
    "        print(f\"Entrada: {example['input']}\")\n",
    "        print(f\"Objetivo: {example['target']}\")\n",
    "        print(f\"Predicción: {example['prediction']}\")\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Configuración principal\n",
    "INPUT_DIM = text_processor.vocab_size\n",
    "OUTPUT_DIM = text_processor.vocab_size  # Para generación de secuencia a secuencia\n",
    "EMB_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 8  # Para Transformer\n",
    "DROPOUT = 0.3\n",
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Mover al dispositivo adecuado\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Criterio de pérdida (ignorar padding)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# ======= PARTE 1: MODELOS RNN/LSTM =======\n",
    "print(\"\\n===== PARTE 1: MODELOS RNN/LSTM =====\")\n",
    "\n",
    "# Crear modelos\n",
    "print(\"\\nCreando modelos RNN/LSTM...\")\n",
    "\n",
    "# Modelo RNN simple\n",
    "rnn_model = SimpleRNN(\n",
    "    input_dim=INPUT_DIM,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Modelo LSTM\n",
    "lstm_model = LSTM(\n",
    "    input_dim=INPUT_DIM,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Modelo GRU\n",
    "gru_model = GRU(\n",
    "    input_dim=INPUT_DIM,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Optimizadores\n",
    "optimizer_rnn = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_gru = optim.Adam(gru_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Entrenar modelos\n",
    "print(\"\\nEntrenando modelo RNN...\")\n",
    "rnn_model, rnn_history = train_model(\n",
    "    model=rnn_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer_rnn,\n",
    "    criterion=criterion,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    device=device,\n",
    "    model_name=\"RNN\"\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando modelo LSTM...\")\n",
    "lstm_model, lstm_history = train_model(\n",
    "    model=lstm_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer_lstm,\n",
    "    criterion=criterion,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    device=device,\n",
    "    model_name=\"LSTM\"\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando modelo GRU...\")\n",
    "gru_model, gru_history = train_model(\n",
    "    model=gru_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer_gru,\n",
    "    criterion=criterion,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    device=device,\n",
    "    model_name=\"GRU\"\n",
    ")\n",
    "\n",
    "# Visualizar historiales de entrenamiento\n",
    "plot_training_history(rnn_history, \"RNN\")\n",
    "plot_training_history(lstm_history, \"LSTM\")\n",
    "plot_training_history(gru_history, \"GRU\")\n",
    "\n",
    "# Evaluar modelos\n",
    "print(\"\\nEvaluando modelo RNN...\")\n",
    "rnn_metrics = evaluate_model(rnn_model, test_loader, criterion, device, text_processor.idx2word)\n",
    "\n",
    "print(\"\\nEvaluando modelo LSTM...\")\n",
    "lstm_metrics = evaluate_model(lstm_model, test_loader, criterion, device, text_processor.idx2word)\n",
    "\n",
    "print(\"\\nEvaluando modelo GRU...\")\n",
    "gru_metrics = evaluate_model(gru_model, test_loader, criterion, device, text_processor.idx2word)\n",
    "\n",
    "# Comparar modelos RNN/LSTM\n",
    "rnn_lstm_metrics = {\n",
    "    'RNN': rnn_metrics,\n",
    "    'LSTM': lstm_metrics,\n",
    "    'GRU': gru_metrics\n",
    "}\n",
    "\n",
    "compare_models(\n",
    "    metrics_dict=rnn_lstm_metrics,\n",
    "    model_names=['RNN', 'LSTM', 'GRU'],\n",
    "    metric_names=['accuracy', 'precision', 'recall', 'f1']\n",
    ")\n",
    "\n",
    "# Analizar ejemplos específicos\n",
    "print(\"\\nAnalizando ejemplos específicos con el modelo LSTM...\")\n",
    "lstm_examples = analyze_examples(lstm_model, test_loader, text_processor, device)\n",
    "\n",
    "# Analizar impacto de hiperparámetros\n",
    "print(\"\\nAnalizando impacto de hiperparámetros en el modelo LSTM...\")\n",
    "\n",
    "# Parámetros fijos\n",
    "fixed_params = {\n",
    "    'emb_dim': EMB_DIM,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'output_dim': OUTPUT_DIM,\n",
    "    'n_layers': N_LAYERS,\n",
    "    'dropout': DROPOUT,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'n_heads': N_HEADS  # Solo para Transformer\n",
    "}\n",
    "\n",
    "# Analizar impacto del número de capas\n",
    "n_layers_values = [1, 2, 3, 4]\n",
    "n_layers_results = analyze_hyperparameters(\n",
    "    model_class=LSTM,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    text_processor=text_processor,\n",
    "    param_name='n_layers',\n",
    "    param_values=n_layers_values,\n",
    "    fixed_params=fixed_params,\n",
    "    n_epochs=5,  # Reducir épocas para agilizar\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Analizar impacto de la tasa de aprendizaje\n",
    "lr_values = [0.0001, 0.001, 0.01, 0.1]\n",
    "lr_results = analyze_hyperparameters(\n",
    "    model_class=LSTM,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    text_processor=text_processor,\n",
    "    param_name='learning_rate',\n",
    "    param_values=lr_values,\n",
    "    fixed_params=fixed_params,\n",
    "    n_epochs=5,  # Reducir épocas para agilizar\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ======= PARTE 2: MODELO TRANSFORMER =======\n",
    "print(\"\\n===== PARTE 2: MODELO TRANSFORMER =======\")\n",
    "\n",
    "# Crear modelo Transformer\n",
    "print(\"\\nCreando modelo Transformer...\")\n",
    "transformer_model = TransformerModel(\n",
    "    input_dim=INPUT_DIM,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Optimizador\n",
    "optimizer_transformer = optim.Adam(transformer_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Entrenar modelo\n",
    "print(\"\\nEntrenando modelo Transformer...\")\n",
    "transformer_model, transformer_history = train_model(\n",
    "    model=transformer_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer_transformer,\n",
    "    criterion=criterion,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    device=device,\n",
    "    model_name=\"Transformer\"\n",
    ")\n",
    "\n",
    "# Visualizar historial de entrenamiento\n",
    "plot_training_history(transformer_history, \"Transformer\")\n",
    "\n",
    "# Evaluar modelo\n",
    "print(\"\\nEvaluando modelo Transformer...\")\n",
    "transformer_metrics = evaluate_model(transformer_model, test_loader, criterion, device, text_processor.idx2word)\n",
    "\n",
    "# Comparar todos los modelos\n",
    "all_metrics = {\n",
    "    'RNN': rnn_metrics,\n",
    "    'LSTM': lstm_metrics,\n",
    "    'GRU': gru_metrics,\n",
    "    'Transformer': transformer_metrics\n",
    "}\n",
    "\n",
    "compare_models(\n",
    "    metrics_dict=all_metrics,\n",
    "    model_names=['RNN', 'LSTM', 'GRU', 'Transformer'],\n",
    "    metric_names=['accuracy', 'precision', 'recall', 'f1']\n",
    ")\n",
    "\n",
    "# Comparar BLEU y ROUGE para modelos de generación\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Métricas a visualizar\n",
    "nlp_metrics = ['bleu', 'rouge-1', 'rouge-2', 'rouge-l']\n",
    "model_names = ['RNN', 'LSTM', 'GRU', 'Transformer']\n",
    "\n",
    "for i, metric in enumerate(nlp_metrics):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    values = [all_metrics[model][metric] for model in model_names]\n",
    "    \n",
    "    # Crear gráfico de barras\n",
    "    bars = plt.bar(model_names, values)\n",
    "    \n",
    "    # Añadir valores sobre las barras\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title(metric.upper())\n",
    "    plt.ylabel('Value')\n",
    "    plt.ylim(0, max(values) * 1.2)  # Ajustar límite vertical\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nlp_metrics_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Analizar ejemplos específicos con Transformer\n",
    "print(\"\\nAnalizando ejemplos específicos con el modelo Transformer...\")\n",
    "transformer_examples = analyze_examples(transformer_model, test_loader, text_processor, device)\n",
    "\n",
    "# Analizar impacto de hiperparámetros en Transformer\n",
    "print(\"\\nAnalizando impacto de hiperparámetros en el modelo Transformer...\")\n",
    "\n",
    "# Analizar impacto del número de capas\n",
    "n_layers_transformer_results = analyze_hyperparameters(\n",
    "    model_class=TransformerModel,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    text_processor=text_processor,\n",
    "    param_name='n_layers',\n",
    "    param_values=n_layers_values,\n",
    "    fixed_params=fixed_params,\n",
    "    n_epochs=5,  # Reducir épocas para agilizar\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Analizar impacto del número de cabezas de atención\n",
    "n_heads_values = [2, 4, 8, 16]\n",
    "n_heads_results = analyze_hyperparameters(\n",
    "    model_class=TransformerModel,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    text_processor=text_processor,\n",
    "    param_name='n_heads',\n",
    "    param_values=n_heads_values,\n",
    "    fixed_params=fixed_params,\n",
    "    n_epochs=5,  # Reducir épocas para agilizar\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ======= ANÁLISIS COMPARATIVO FINAL =======\n",
    "print(\"\\n===== ANÁLISIS COMPARATIVO FINAL =====\")\n",
    "\n",
    "# Comparar tiempos de inferencia\n",
    "def measure_inference_time(model, dataloader, device, num_batches=10):\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (src, _) in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            src = src.to(device)\n",
    "            batch_size = src.size(0)\n",
    "            \n",
    "            # Medir tiempo\n",
    "            start_time = time.time()\n",
    "            _ = model(src)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            total_time += (end_time - start_time)\n",
    "            total_samples += batch_size\n",
    "    \n",
    "    # Tiempo promedio por muestra\n",
    "    avg_time = total_time / total_samples\n",
    "    return avg_time\n",
    "\n",
    "print(\"\\nMidiendo tiempos de inferencia...\")\n",
    "rnn_time = measure_inference_time(rnn_model, test_loader, device)\n",
    "lstm_time = measure_inference_time(lstm_model, test_loader, device)\n",
    "gru_time = measure_inference_time(gru_model, test_loader, device)\n",
    "transformer_time = measure_inference_time(transformer_model, test_loader, device)\n",
    "\n",
    "# Normalizar tiempos (relativo al más rápido)\n",
    "min_time = min(rnn_time, lstm_time, gru_time, transformer_time)\n",
    "relative_times = {\n",
    "    'RNN': rnn_time / min_time,\n",
    "    'LSTM': lstm_time / min_time,\n",
    "    'GRU': gru_time / min_time,\n",
    "    'Transformer': transformer_time / min_time\n",
    "}\n",
    "\n",
    "print(f\"Tiempos de inferencia relativos (menor es mejor):\")\n",
    "for model_name, rel_time in relative_times.items():\n",
    "    print(f\"{model_name}: {rel_time:.2f}x\")\n",
    "\n",
    "# Visualizar tiempos de inferencia\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(relative_times.keys(), relative_times.values())\n",
    "plt.title('Tiempo de inferencia relativo (menor es mejor)')\n",
    "plt.ylabel('Tiempo relativo')\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "# Añadir valores sobre las barras\n",
    "for i, (model, time) in enumerate(relative_times.items()):\n",
    "    plt.text(i, time + 0.05, f'{time:.2f}x', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('inference_times.png')\n",
    "plt.close()\n",
    "\n",
    "# Resumen final de resultados\n",
    "print(\"\\nResumen final de resultados:\")\n",
    "print(\"\\nMétricas de evaluación:\")\n",
    "for model_name in ['RNN', 'LSTM', 'GRU', 'Transformer']:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric, value in all_metrics[model_name].items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Seleccionar el mejor modelo RNN/LSTM basado en F1-score\n",
    "best_rnn_lstm_model = max(['RNN', 'LSTM', 'GRU'], key=lambda x: all_metrics[x]['f1'])\n",
    "print(f\"\\nMejor modelo RNN/LSTM: {best_rnn_lstm_model} (F1: {all_metrics[best_rnn_lstm_model]['f1']:.4f})\")\n",
    "\n",
    "# Comparar el mejor modelo RNN/LSTM con Transformer\n",
    "print(\"\\nComparación del mejor modelo RNN/LSTM vs Transformer:\")\n",
    "print(f\"F1-score - {best_rnn_lstm_model}: {all_metrics[best_rnn_lstm_model]['f1']:.4f}, Transformer: {all_metrics['Transformer']['f1']:.4f}\")\n",
    "print(f\"BLEU - {best_rnn_lstm_model}: {all_metrics[best_rnn_lstm_model]['bleu']:.4f}, Transformer: {all_metrics['Transformer']['bleu']:.4f}\")\n",
    "print(f\"ROUGE-L - {best_rnn_lstm_model}: {all_metrics[best_rnn_lstm_model]['rouge-l']:.4f}, Transformer: {all_metrics['Transformer']['rouge-l']:.4f}\")\n",
    "print(f\"Tiempo relativo - {best_rnn_lstm_model}: {relative_times[best_rnn_lstm_model]:.2f}x, Transformer: {relative_times['Transformer']:.2f}x\")\n",
    "\n",
    "# Visualizar comparación final entre el mejor RNN/LSTM y Transformer\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Métricas a visualizar\n",
    "final_metrics = ['accuracy', 'f1', 'bleu', 'rouge-l']\n",
    "final_models = [best_rnn_lstm_model, 'Transformer']\n",
    "\n",
    "for i, metric in enumerate(final_metrics):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    values = [all_metrics[model][metric] for model in final_models]\n",
    "    \n",
    "    # Crear gráfico de barras\n",
    "    bars = plt.bar(final_models, values)\n",
    "    \n",
    "    # Añadir valores sobre las barras\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title(metric.capitalize())\n",
    "    plt.ylabel('Value')\n",
    "    plt.ylim(0, max(values) * 1.2)  # Ajustar límite vertical\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Análisis de componentes clave del Transformer\n",
    "print(\"\\nAnálisis de componentes clave del Transformer:\")\n",
    "print(\"1. Mecanismo de autoatención: Permite al modelo atender a diferentes partes de la secuencia de entrada simultáneamente.\")\n",
    "print(\"2. Codificación posicional: Proporciona información sobre la posición de cada token en la secuencia.\")\n",
    "print(\"3. Arquitectura encoder-decoder: Permite procesar la entrada y generar la salida de manera eficiente.\")\n",
    "print(\"4. Multi-head attention: Permite al modelo atender a diferentes representaciones del espacio simultáneamente.\")\n",
    "\n",
    "# Conclusiones\n",
    "print(\"\\nConclusiones:\")\n",
    "print(\"1. Comparación de arquitecturas:\")\n",
    "if all_metrics['Transformer']['f1'] > all_metrics[best_rnn_lstm_model]['f1']:\n",
    "    print(f\"   - El modelo Transformer superó al mejor modelo RNN/LSTM ({best_rnn_lstm_model}) en términos de F1-score.\")\n",
    "else:\n",
    "    print(f\"   - El mejor modelo RNN/LSTM ({best_rnn_lstm_model}) superó al Transformer en términos de F1-score.\")\n",
    "\n",
    "if all_metrics['Transformer']['bleu'] > all_metrics[best_rnn_lstm_model]['bleu']:\n",
    "    print(f\"   - El modelo Transformer superó al mejor modelo RNN/LSTM en términos de BLEU score.\")\n",
    "else:\n",
    "    print(f\"   - El mejor modelo RNN/LSTM superó al Transformer en términos de BLEU score.\")\n",
    "\n",
    "if relative_times['Transformer'] < relative_times[best_rnn_lstm_model]:\n",
    "    print(f\"   - El modelo Transformer fue más rápido en inferencia que el mejor modelo RNN/LSTM.\")\n",
    "else:\n",
    "    print(f\"   - El mejor modelo RNN/LSTM fue más rápido en inferencia que el Transformer.\")\n",
    "\n",
    "print(\"\\n2. Impacto de hiperparámetros:\")\n",
    "print(\"   - Número de capas: Un mayor número de capas puede mejorar el rendimiento hasta cierto punto, pero también aumenta el riesgo de sobreajuste.\")\n",
    "print(\"   - Tasa de aprendizaje: Una tasa de aprendizaje adecuada es crucial para la convergencia del modelo.\")\n",
    "print(\"   - Número de cabezas de atención (Transformer): Más cabezas permiten capturar diferentes tipos de relaciones en los datos.\")\n",
    "\n",
    "print(\"\\n3. Ventajas y desventajas:\")\n",
    "print(\"   - RNN/LSTM:\")\n",
    "print(\"     * Ventajas: Más simples, menos parámetros, eficientes para secuencias cortas.\")\n",
    "print(\"     * Desventajas: Dificultad para capturar dependencias a largo plazo, procesamiento secuencial.\")\n",
    "print(\"   - Transformer:\")\n",
    "print(\"     * Ventajas: Paralelización, mejor captura de dependencias a largo plazo, atención a diferentes partes de la secuencia.\")\n",
    "print(\"     * Desventajas: Mayor número de parámetros, requiere más datos para entrenar efectivamente.\")\n",
    "\n",
    "print(\"\\nAnálisis completado. Se han generado gráficos para visualizar los resultados.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
