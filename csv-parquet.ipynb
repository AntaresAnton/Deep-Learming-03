{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a1b3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Directorio creado: datos_normalizados\n",
      "‚úÖ Archivo train.csv cargado correctamente\n",
      "‚úÖ Archivo test.csv cargado correctamente\n",
      "‚úÖ Archivo validation.csv cargado correctamente\n",
      "\n",
      "================================================================================\n",
      "PROCESANDO CONJUNTO: train\n",
      "================================================================================\n",
      "1Ô∏è‚É£ Convirtiendo strings a listas...\n",
      "2Ô∏è‚É£ Extrayendo informaci√≥n de di√°logos...\n",
      "3Ô∏è‚É£ Normalizando actos y emociones...\n",
      "4Ô∏è‚É£ Calculando estad√≠sticas de texto...\n",
      "5Ô∏è‚É£ Generando visualizaciones...\n",
      "‚úÖ Visualizaciones guardadas para el conjunto train\n",
      "‚úÖ Datos guardados en: datos_normalizados\\train.parquet\n",
      "\n",
      "üìä ESTAD√çSTICAS B√ÅSICAS:\n",
      "- N√∫mero de di√°logos: 11118\n",
      "- Longitud promedio de di√°logo (caracteres): 485.26\n",
      "- N√∫mero promedio de palabras por di√°logo: 106.68\n",
      "- N√∫mero promedio de expresiones por di√°logo: 1.00\n",
      "\n",
      "üîç VERIFICACI√ìN DE INTEGRIDAD:\n",
      "- Filas con longitudes inconsistentes: 11118 (100.00%)\n",
      "\n",
      "================================================================================\n",
      "PROCESANDO CONJUNTO: test\n",
      "================================================================================\n",
      "1Ô∏è‚É£ Convirtiendo strings a listas...\n",
      "2Ô∏è‚É£ Extrayendo informaci√≥n de di√°logos...\n",
      "3Ô∏è‚É£ Normalizando actos y emociones...\n",
      "4Ô∏è‚É£ Calculando estad√≠sticas de texto...\n",
      "5Ô∏è‚É£ Generando visualizaciones...\n",
      "‚úÖ Visualizaciones guardadas para el conjunto test\n",
      "‚úÖ Datos guardados en: datos_normalizados\\test.parquet\n",
      "\n",
      "üìä ESTAD√çSTICAS B√ÅSICAS:\n",
      "- N√∫mero de di√°logos: 1000\n",
      "- Longitud promedio de di√°logo (caracteres): 486.93\n",
      "- N√∫mero promedio de palabras por di√°logo: 106.63\n",
      "- N√∫mero promedio de expresiones por di√°logo: 1.00\n",
      "\n",
      "üîç VERIFICACI√ìN DE INTEGRIDAD:\n",
      "- Filas con longitudes inconsistentes: 1000 (100.00%)\n",
      "\n",
      "================================================================================\n",
      "PROCESANDO CONJUNTO: validation\n",
      "================================================================================\n",
      "1Ô∏è‚É£ Convirtiendo strings a listas...\n",
      "2Ô∏è‚É£ Extrayendo informaci√≥n de di√°logos...\n",
      "3Ô∏è‚É£ Normalizando actos y emociones...\n",
      "4Ô∏è‚É£ Calculando estad√≠sticas de texto...\n",
      "5Ô∏è‚É£ Generando visualizaciones...\n",
      "‚úÖ Visualizaciones guardadas para el conjunto validation\n",
      "‚úÖ Datos guardados en: datos_normalizados\\validation.parquet\n",
      "\n",
      "üìä ESTAD√çSTICAS B√ÅSICAS:\n",
      "- N√∫mero de di√°logos: 1000\n",
      "- Longitud promedio de di√°logo (caracteres): 498.76\n",
      "- N√∫mero promedio de palabras por di√°logo: 108.93\n",
      "- N√∫mero promedio de expresiones por di√°logo: 1.00\n",
      "\n",
      "üîç VERIFICACI√ìN DE INTEGRIDAD:\n",
      "- Filas con longitudes inconsistentes: 1000 (100.00%)\n",
      "‚úÖ Mapeos de encoders guardados en el directorio datos_normalizados\n",
      "\n",
      "‚úÖ Procesamiento CRISP-DM completado. Los datos est√°n listos para modelado.\n",
      "\n",
      "üìã RESUMEN FINAL:\n",
      "- train: 11118 filas x 17 columnas\n",
      "- test: 1000 filas x 17 columnas\n",
      "- validation: 1000 filas x 17 columnas\n",
      "\n",
      "‚úÖ Todas las columnas son iguales en los conjuntos procesados: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configuraci√≥n para mostrar m√°s columnas y filas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "def cargar_datos(archivos):\n",
    "    \"\"\"\n",
    "    Carga los archivos CSV en DataFrames\n",
    "    \n",
    "    Args:\n",
    "        archivos: Lista de rutas a los archivos CSV\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con los DataFrames cargados\n",
    "    \"\"\"\n",
    "    datos = {}\n",
    "    for archivo in archivos:\n",
    "        nombre = os.path.basename(archivo).split('.')[0]\n",
    "        try:\n",
    "            datos[nombre] = pd.read_csv(archivo)\n",
    "            print(f\"‚úÖ Archivo {archivo} cargado correctamente\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error al cargar {archivo}: {e}\")\n",
    "    \n",
    "    return datos\n",
    "\n",
    "def convertir_listas(df):\n",
    "    \"\"\"\n",
    "    Convierte las cadenas de texto que representan listas a listas reales de Python\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con las columnas a convertir\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con las columnas convertidas\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in df_copy.columns:\n",
    "        try:\n",
    "            # Intentar convertir cada valor de la columna a una lista Python\n",
    "            df_copy[col] = df_copy[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "        except (ValueError, SyntaxError):\n",
    "            # Si falla, mantener la columna como est√°\n",
    "            pass\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def extraer_dialogos(df):\n",
    "    \"\"\"\n",
    "    Extrae los di√°logos de la columna 'dialog' y crea nuevas columnas\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con la columna 'dialog'\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con los di√°logos extra√≠dos\n",
    "    \"\"\"\n",
    "    df_expanded = df.copy()\n",
    "    \n",
    "    # Extraer los di√°logos individuales\n",
    "    df_expanded['num_utterances'] = df_expanded['dialog'].apply(len)\n",
    "    df_expanded['dialog_text'] = df_expanded['dialog'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Extraer el primer y √∫ltimo di√°logo (pueden ser √∫tiles para an√°lisis)\n",
    "    df_expanded['first_utterance'] = df_expanded['dialog'].apply(lambda x: x[0] if len(x) > 0 else '')\n",
    "    df_expanded['last_utterance'] = df_expanded['dialog'].apply(lambda x: x[-1] if len(x) > 0 else '')\n",
    "    \n",
    "    return df_expanded\n",
    "\n",
    "def normalizar_actos_emociones(df):\n",
    "    \"\"\"\n",
    "    Normaliza los actos y emociones, creando columnas num√©ricas\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con las columnas 'act' y 'emotion'\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con actos y emociones normalizados y encoders utilizados\n",
    "    \"\"\"\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # Crear columnas para estad√≠sticas de actos y emociones\n",
    "    df_norm['act_counts'] = df_norm['act'].apply(lambda x: len(x))\n",
    "    df_norm['emotion_counts'] = df_norm['emotion'].apply(lambda x: len(x))\n",
    "    \n",
    "    # Verificar que las longitudes coincidan\n",
    "    df_norm['lengths_match'] = df_norm.apply(\n",
    "        lambda row: len(row['dialog']) == len(row['act']) == len(row['emotion']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Extraer actos y emociones m√°s frecuentes\n",
    "    def most_common(lst):\n",
    "        if not lst:\n",
    "            return None\n",
    "        return max(set(lst), key=lst.count)\n",
    "    \n",
    "    df_norm['most_common_act'] = df_norm['act'].apply(most_common)\n",
    "    df_norm['most_common_emotion'] = df_norm['emotion'].apply(most_common)\n",
    "    \n",
    "    # Codificar actos y emociones m√°s frecuentes\n",
    "    act_encoder = LabelEncoder()\n",
    "    emotion_encoder = LabelEncoder()\n",
    "    \n",
    "    # Ajustar encoders con todos los valores posibles\n",
    "    all_acts = [act for acts in df_norm['most_common_act'].dropna() for act in [acts]]\n",
    "    all_emotions = [emotion for emotions in df_norm['most_common_emotion'].dropna() for emotion in [emotions]]\n",
    "    \n",
    "    act_encoder.fit(all_acts)\n",
    "    emotion_encoder.fit(all_emotions)\n",
    "    \n",
    "    # Transformar valores\n",
    "    df_norm['most_common_act_encoded'] = df_norm['most_common_act'].apply(\n",
    "        lambda x: act_encoder.transform([x])[0] if x is not None else np.nan\n",
    "    )\n",
    "    \n",
    "    df_norm['most_common_emotion_encoded'] = df_norm['most_common_emotion'].apply(\n",
    "        lambda x: emotion_encoder.transform([x])[0] if x is not None else np.nan\n",
    "    )\n",
    "    \n",
    "    # Crear mapeos para referencia\n",
    "    act_mapping = {i: label for i, label in enumerate(act_encoder.classes_)}\n",
    "    emotion_mapping = {i: label for i, label in enumerate(emotion_encoder.classes_)}\n",
    "    \n",
    "    return df_norm, {'act_encoder': act_encoder, 'emotion_encoder': emotion_encoder, \n",
    "                     'act_mapping': act_mapping, 'emotion_mapping': emotion_mapping}\n",
    "\n",
    "def calcular_estadisticas_texto(df):\n",
    "    \"\"\"\n",
    "    Calcula estad√≠sticas sobre los textos de di√°logo\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con la columna 'dialog_text'\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con estad√≠sticas de texto a√±adidas\n",
    "    \"\"\"\n",
    "    df_stats = df.copy()\n",
    "    \n",
    "    # Calcular longitud de texto\n",
    "    df_stats['dialog_length'] = df_stats['dialog_text'].apply(len)\n",
    "    \n",
    "    # Calcular n√∫mero de palabras\n",
    "    df_stats['word_count'] = df_stats['dialog_text'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # Calcular longitud promedio de palabras\n",
    "    df_stats['avg_word_length'] = df_stats['dialog_text'].apply(\n",
    "        lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    return df_stats\n",
    "\n",
    "def visualizar_distribucion(df, encoders, conjunto):\n",
    "    \"\"\"\n",
    "    Visualiza la distribuci√≥n de actos y emociones\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame normalizado\n",
    "        encoders: Diccionario con encoders y mapeos\n",
    "        conjunto: Nombre del conjunto de datos (train, test, validation)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    \n",
    "    # Distribuci√≥n de actos\n",
    "    plt.subplot(1, 2, 1)\n",
    "    act_counts = df['most_common_act'].value_counts().head(10)\n",
    "    sns.barplot(x=act_counts.index, y=act_counts.values)\n",
    "    plt.title(f'Top 10 Actos m√°s frecuentes - {conjunto}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Distribuci√≥n de emociones\n",
    "    plt.subplot(1, 2, 2)\n",
    "    emotion_counts = df['most_common_emotion'].value_counts().head(10)\n",
    "    sns.barplot(x=emotion_counts.index, y=emotion_counts.values)\n",
    "    plt.title(f'Top 10 Emociones m√°s frecuentes - {conjunto}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'distribucion_{conjunto}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Estad√≠sticas de longitud de di√°logo\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(df['num_utterances'], bins=20, kde=True)\n",
    "    plt.title(f'Distribuci√≥n de n√∫mero de expresiones por di√°logo - {conjunto}')\n",
    "    plt.xlabel('N√∫mero de expresiones')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.savefig(f'longitud_dialogos_{conjunto}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"‚úÖ Visualizaciones guardadas para el conjunto {conjunto}\")\n",
    "\n",
    "def procesar_y_guardar(archivos_csv, directorio_salida='datos_normalizados'):\n",
    "    \"\"\"\n",
    "    Procesa los archivos CSV y guarda los resultados en formato Parquet\n",
    "    \n",
    "    Args:\n",
    "        archivos_csv: Lista de rutas a los archivos CSV\n",
    "        directorio_salida: Directorio donde guardar los archivos Parquet\n",
    "    \"\"\"\n",
    "    # Crear directorio de salida si no existe\n",
    "    if not os.path.exists(directorio_salida):\n",
    "        os.makedirs(directorio_salida)\n",
    "        print(f\"‚úÖ Directorio creado: {directorio_salida}\")\n",
    "    \n",
    "    # Cargar datos\n",
    "    datos = cargar_datos(archivos_csv)\n",
    "    \n",
    "    # Procesar cada conjunto de datos\n",
    "    dataframes_procesados = {}\n",
    "    encoders = None\n",
    "    \n",
    "    for nombre, df in datos.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"PROCESANDO CONJUNTO: {nombre}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Paso 1: Convertir strings a listas\n",
    "        print(\"1Ô∏è‚É£ Convirtiendo strings a listas...\")\n",
    "        df_listas = convertir_listas(df)\n",
    "        \n",
    "        # Paso 2: Extraer y procesar di√°logos\n",
    "        print(\"2Ô∏è‚É£ Extrayendo informaci√≥n de di√°logos...\")\n",
    "        df_dialogos = extraer_dialogos(df_listas)\n",
    "        \n",
    "        # Paso 3: Normalizar actos y emociones\n",
    "        print(\"3Ô∏è‚É£ Normalizando actos y emociones...\")\n",
    "        if nombre == 'train':\n",
    "            # Solo entrenar encoders en el conjunto de entrenamiento\n",
    "            df_norm, encoders = normalizar_actos_emociones(df_dialogos)\n",
    "        else:\n",
    "            # Usar los encoders del conjunto de entrenamiento\n",
    "            df_norm, _ = normalizar_actos_emociones(df_dialogos)\n",
    "        \n",
    "        # Paso 4: Calcular estad√≠sticas de texto\n",
    "        print(\"4Ô∏è‚É£ Calculando estad√≠sticas de texto...\")\n",
    "        df_final = calcular_estadisticas_texto(df_norm)\n",
    "        \n",
    "        # Guardar DataFrame procesado\n",
    "        dataframes_procesados[nombre] = df_final\n",
    "        \n",
    "        # Visualizar distribuciones\n",
    "        print(\"5Ô∏è‚É£ Generando visualizaciones...\")\n",
    "        visualizar_distribucion(df_final, encoders, nombre)\n",
    "        \n",
    "        # Guardar en formato Parquet\n",
    "        ruta_parquet = os.path.join(directorio_salida, f\"{nombre}.parquet\")\n",
    "        df_final.to_parquet(ruta_parquet, index=False)\n",
    "        print(f\"‚úÖ Datos guardados en: {ruta_parquet}\")\n",
    "        \n",
    "        # Mostrar estad√≠sticas b√°sicas\n",
    "        print(\"\\nüìä ESTAD√çSTICAS B√ÅSICAS:\")\n",
    "        print(f\"- N√∫mero de di√°logos: {len(df_final)}\")\n",
    "        print(f\"- Longitud promedio de di√°logo (caracteres): {df_final['dialog_length'].mean():.2f}\")\n",
    "        print(f\"- N√∫mero promedio de palabras por di√°logo: {df_final['word_count'].mean():.2f}\")\n",
    "        print(f\"- N√∫mero promedio de expresiones por di√°logo: {df_final['num_utterances'].mean():.2f}\")\n",
    "        \n",
    "        # Verificar integridad\n",
    "        print(f\"\\nüîç VERIFICACI√ìN DE INTEGRIDAD:\")\n",
    "        print(f\"- Filas con longitudes inconsistentes: {(~df_final['lengths_match']).sum()} ({(~df_final['lengths_match']).sum() / len(df_final) * 100:.2f}%)\")\n",
    "    \n",
    "    # Guardar mapeos de encoders\n",
    "    if encoders:\n",
    "        import json\n",
    "        \n",
    "        # Convertir mapeos a formato serializable\n",
    "        act_mapping_serializable = {str(k): str(v) for k, v in encoders['act_mapping'].items()}\n",
    "        emotion_mapping_serializable = {str(k): str(v) for k, v in encoders['emotion_mapping'].items()}\n",
    "        \n",
    "        with open(os.path.join(directorio_salida, 'act_mapping.json'), 'w') as f:\n",
    "            json.dump(act_mapping_serializable, f)\n",
    "        \n",
    "        with open(os.path.join(directorio_salida, 'emotion_mapping.json'), 'w') as f:\n",
    "            json.dump(emotion_mapping_serializable, f)\n",
    "        \n",
    "        print(f\"‚úÖ Mapeos de encoders guardados en el directorio {directorio_salida}\")\n",
    "    \n",
    "    return dataframes_procesados\n",
    "\n",
    "# Ejecutar el procesamiento\n",
    "archivos = ['train.csv', 'test.csv', 'validation.csv']\n",
    "dataframes = procesar_y_guardar(archivos)\n",
    "\n",
    "print(\"\\n‚úÖ Procesamiento CRISP-DM completado. Los datos est√°n listos para modelado.\")\n",
    "\n",
    "# Mostrar un resumen final\n",
    "print(\"\\nüìã RESUMEN FINAL:\")\n",
    "for nombre, df in dataframes.items():\n",
    "    print(f\"- {nombre}: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
    "\n",
    "# Verificar columnas en los conjuntos procesados\n",
    "train_cols = set(dataframes['train'].columns)\n",
    "test_cols = set(dataframes['test'].columns)\n",
    "val_cols = set(dataframes['validation'].columns)\n",
    "\n",
    "print(f\"\\n‚úÖ Todas las columnas son iguales en los conjuntos procesados: {train_cols == test_cols == val_cols}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
