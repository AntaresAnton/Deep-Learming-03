#!/usr/bin/env python3
"""
üß™ VALIDADOR COMPLETO PARA proyecto_deep_learning_dialogos.py
Prueba todas las funciones del archivo principal sin ejecutar el entrenamiento completo
INCLUYE ESTIMACI√ìN DE TIEMPO DE EJECUCI√ìN
"""

import importlib.util
import os
import sys
import time
import traceback
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import torch


def load_main_module():
    """Carga el m√≥dulo principal"""
    try:
        spec = importlib.util.spec_from_file_location(
            "main_project", "proyecto_deep_learning_dialogos.py"
        )
        main_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(main_module)
        return main_module, True
    except Exception as e:
        print(f"‚ùå Error cargando m√≥dulo principal: {e}")
        return None, False


def estimate_execution_time():
    """Estima el tiempo de ejecuci√≥n del proyecto completo"""
    print("‚è±Ô∏è ESTIMANDO TIEMPO DE EJECUCI√ìN DEL PROYECTO COMPLETO")
    print("-" * 60)

    try:
        # Verificar archivos de datos
        if not all(
            os.path.exists(f)
            for f in ["train.parquet", "validation.parquet", "test.parquet"]
        ):
            print("‚ùå No se pueden estimar tiempos sin archivos de datos")
            return None

        # Cargar datos para an√°lisis
        print("üìä Analizando tama√±os de datos...")
        train_df = pd.read_parquet("train.parquet")
        val_df = pd.read_parquet("validation.parquet")
        test_df = pd.read_parquet("test.parquet")

        # Calcular estad√≠sticas
        total_train_dialogs = sum(
            len(row) if isinstance(row, list) else 1 for row in train_df["dialog"]
        )
        total_val_dialogs = sum(
            len(row) if isinstance(row, list) else 1 for row in val_df["dialog"]
        )
        total_test_dialogs = sum(
            len(row) if isinstance(row, list) else 1 for row in test_df["dialog"]
        )

        total_expanded = total_train_dialogs + total_val_dialogs + total_test_dialogs

        print(f"   üìã Datos originales:")
        print(f"      Train: {len(train_df):,} filas")
        print(f"      Validation: {len(val_df):,} filas")
        print(f"      Test: {len(test_df):,} filas")
        print(f"   üìà Datos expandidos estimados:")
        print(f"      Train: {total_train_dialogs:,} di√°logos")
        print(f"      Validation: {total_val_dialogs:,} di√°logos")
        print(f"      Test: {total_test_dialogs:,} di√°logos")
        print(f"      Total: {total_expanded:,} di√°logos")

        # Configuraci√≥n del proyecto
        n_epochs = 15
        n_models = 4  # SimpleRNN, LSTM, GRU, Transformer
        n_tasks = 2  # Actos de habla y emociones

        # Detectar dispositivo
        device = "GPU" if torch.cuda.is_available() else "CPU"
        if device == "GPU":
            gpu_name = torch.cuda.get_device_name(0)
            print(f"üéÆ Dispositivo: {device} ({gpu_name})")
        else:
            print(f"üéÆ Dispositivo: {device}")

        # Estimaciones de tiempo basadas en el dispositivo y tama√±o de datos
        if device == "GPU":
            # Tiempos con GPU (m√°s r√°pidos)
            base_time_per_epoch = 30  # segundos base por √©poca
            data_loading_time = 60  # 1 minuto
            preprocessing_time = 120  # 2 minutos
            evaluation_time = 30  # 30 segundos por modelo por tarea
            visualization_time = 60  # 1 minuto

            # Ajustar por tama√±o de datos
            if total_expanded > 50000:
                base_time_per_epoch *= 2
                preprocessing_time *= 1.5
            elif total_expanded > 100000:
                base_time_per_epoch *= 3
                preprocessing_time *= 2

        else:
            # Tiempos con CPU (m√°s lentos)
            base_time_per_epoch = 120  # 2 minutos base por √©poca
            data_loading_time = 120  # 2 minutos
            preprocessing_time = 300  # 5 minutos
            evaluation_time = 90  # 1.5 minutos por modelo por tarea
            visualization_time = 120  # 2 minutos

            # Ajustar por tama√±o de datos
            if total_expanded > 50000:
                base_time_per_epoch *= 2.5
                preprocessing_time *= 2
            elif total_expanded > 100000:
                base_time_per_epoch *= 4
                preprocessing_time *= 3

        # C√°lculos detallados
        training_time_per_task = n_epochs * base_time_per_epoch * n_models
        total_training_time = training_time_per_task * n_tasks
        total_evaluation_time = evaluation_time * n_models * n_tasks

        # Tiempo total
        total_time = (
            data_loading_time
            + preprocessing_time
            + total_training_time
            + total_evaluation_time
            + visualization_time
        )

        print(f"\n‚è±Ô∏è ESTIMACI√ìN DETALLADA DE TIEMPOS:")
        print(f"   üìÇ Carga de datos: {timedelta(seconds=data_loading_time)}")
        print(f"   üîÑ Preprocesamiento: {timedelta(seconds=preprocessing_time)}")
        print(f"   üèãÔ∏è Entrenamiento:")
        print(f"      Por √©poca por modelo: {base_time_per_epoch}s")
        print(f"      Por tarea: {timedelta(seconds=training_time_per_task)}")
        print(f"      Total: {timedelta(seconds=total_training_time)}")
        print(f"   üìä Evaluaci√≥n: {timedelta(seconds=total_evaluation_time)}")
        print(f"   üìà Visualizaciones: {timedelta(seconds=visualization_time)}")

        print(f"\nüéØ TIEMPO TOTAL ESTIMADO: {timedelta(seconds=total_time)}")
        print(f"   ({total_time/3600:.1f} horas)")

        # Desglose por componente
        print(f"\nüìã DESGLOSE POR COMPONENTE:")
        components = [
            ("Carga de datos", data_loading_time),
            ("Preprocesamiento", preprocessing_time),
            ("Entrenamiento", total_training_time),
            ("Evaluaci√≥n", total_evaluation_time),
            ("Visualizaciones", visualization_time),
        ]

        for name, time_sec in components:
            percentage = (time_sec / total_time) * 100
            print(f"   {name:<15}: {timedelta(seconds=time_sec)} ({percentage:.1f}%)")

        # Recomendaciones basadas en el tiempo estimado
        print(f"\nüí° RECOMENDACIONES:")

        if total_time > 7200:  # M√°s de 2 horas
            print(f"   ‚ö†Ô∏è TIEMPO LARGO (>2 horas)")
            print(f"   üîß Considera estas opciones:")
            print(f"      ‚Ä¢ Reducir √©pocas de 15 a 10")
            print(f"      ‚Ä¢ Usar solo 2-3 modelos principales")
            print(f"      ‚Ä¢ Ejecutar en horario nocturno")
            print(f"      ‚Ä¢ Usar GPU si est√° disponible")
        elif total_time > 3600:  # M√°s de 1 hora
            print(f"   ‚è∞ TIEMPO MODERADO (1-2 horas)")
            print(f"   üëç Tiempo razonable para ejecuci√≥n completa")
            print(f"   üí° Puedes ejecutar durante una pausa")
        else:
            print(f"   ‚úÖ TIEMPO CORTO (<1 hora)")
            print(f"   üöÄ Perfecto para ejecuci√≥n inmediata")

        # Opciones de optimizaci√≥n
        print(f"\n‚ö° OPCIONES DE OPTIMIZACI√ìN:")

        # Versi√≥n r√°pida
        quick_time = total_time * 0.3  # 30% del tiempo original
        print(
            f"   üèÉ Versi√≥n r√°pida (5 √©pocas, 2 modelos): ~{timedelta(seconds=quick_time)}"
        )

        # Solo validaci√≥n
        validation_time = (
            preprocessing_time
            + (base_time_per_epoch * 2 * n_tasks)
            + (evaluation_time * 2 * n_tasks)
        )
        print(
            f"   üß™ Solo validaci√≥n (2 √©pocas): ~{timedelta(seconds=validation_time)}"
        )

        # Por partes
        per_task_time = total_time / n_tasks
        print(f"   üìä Por tarea individual: ~{timedelta(seconds=per_task_time)}")

        return {
            "total_time": total_time,
            "components": dict(components),
            "device": device,
            "data_size": total_expanded,
            "recommendations": (
                "fast"
                if total_time < 3600
                else "moderate" if total_time < 7200 else "slow"
            ),
        }

    except Exception as e:
        print(f"‚ùå Error estimando tiempo: {e}")
        return None


def benchmark_training_speed(main_module):
    """Hace un benchmark r√°pido para calibrar estimaciones"""
    print("üèÉ BENCHMARK DE VELOCIDAD DE ENTRENAMIENTO")
    print("-" * 50)

    try:
        # Crear datos dummy peque√±os para benchmark
        texts = ["hello world"] * 100
        labels = [0, 1] * 50
        word_to_idx = {"<PAD>": 0, "<UNK>": 1, "hello": 2, "world": 3}

        dataset = main_module.DialogDataset(texts, labels, word_to_idx, max_length=10)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=16)

        # Crear modelo simple
        model = main_module.SimpleRNN(10, 32, 64, 2)
        optimizer = torch.optim.Adam(model.parameters())
        criterion = torch.nn.CrossEntropyLoss()

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)

        # Medir tiempo de una √©poca
        start_time = time.time()

        model.train()
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        epoch_time = time.time() - start_time

        print(f"   ‚è±Ô∏è Tiempo por √©poca (100 muestras): {epoch_time:.2f}s")
        print(f"   üìä Velocidad estimada: {100/epoch_time:.1f} muestras/segundo")

        # Extrapolar para dataset real
        if os.path.exists("train.parquet"):
            train_df = pd.read_parquet("train.parquet")
            estimated_samples = sum(
                len(row) if isinstance(row, list) else 1 for row in train_df["dialog"]
            )
            estimated_time_per_epoch = (estimated_samples / 100) * epoch_time

            print(
                f"   üéØ Tiempo estimado por √©poca (dataset real): {estimated_time_per_epoch:.1f}s"
            )
            print(
                f"   üèãÔ∏è Tiempo estimado para 15 √©pocas: {timedelta(seconds=estimated_time_per_epoch * 15)}"
            )

        return epoch_time

    except Exception as e:
        print(f"   ‚ùå Error en benchmark: {e}")
        return None


def test_tokenizer_class(main_module):
    """Prueba la clase SimpleTokenizer"""
    print("üî§ Probando SimpleTokenizer...")
    try:
        texts = ["hello world", "this is a test", "tokenizer works great"]
        tokenizer = main_module.SimpleTokenizer(texts, vocab_size=100)

        assert hasattr(tokenizer, "word_to_idx")
        assert hasattr(tokenizer, "idx_to_word")
        assert "<PAD>" in tokenizer.word_to_idx
        assert "<UNK>" in tokenizer.word_to_idx

        print("   ‚úÖ SimpleTokenizer funciona correctamente")
        return True
    except Exception as e:
        print(f"   ‚ùå Error en SimpleTokenizer: {e}")
        return False


def test_dataset_class(main_module):
    """Prueba la clase DialogDataset"""
    print("üìä Probando DialogDataset...")
    try:
        texts = ["hello world", "this is a test"]
        labels = [0, 1]
        word_to_idx = {
            "<PAD>": 0,
            "<UNK>": 1,
            "hello": 2,
            "world": 3,
            "this": 4,
            "is": 5,
            "a": 6,
            "test": 7,
        }

        dataset = main_module.DialogDataset(texts, labels, word_to_idx, max_length=10)

        assert len(dataset) == 2
        sample = dataset[0]
        assert "input_ids" in sample
        assert "attention_mask" in sample
        assert "labels" in sample

        print("   ‚úÖ DialogDataset funciona correctamente")
        return True
    except Exception as e:
        print(f"   ‚ùå Error en DialogDataset: {e}")
        return False


def test_model_classes(main_module):
    """Prueba todas las clases de modelos"""
    print("üß† Probando clases de modelos...")

    vocab_size = 1000
    embedding_dim = 64
    hidden_dim = 128
    output_dim = 5

    models_to_test = [
        ("SimpleRNN", main_module.SimpleRNN),
        ("LSTMClassifier", main_module.LSTMClassifier),
        ("GRUClassifier", main_module.GRUClassifier),
        ("TransformerClassifier", main_module.TransformerClassifier),
    ]

    all_passed = True

    for model_name, model_class in models_to_test:
        try:
            print(f"   üîç Probando {model_name}...")

            if model_name == "TransformerClassifier":
                model = model_class(vocab_size, embedding_dim, output_dim)
            else:
                model = model_class(vocab_size, embedding_dim, hidden_dim, output_dim)

            # Probar forward pass
            batch_size = 4
            seq_length = 20
            input_ids = torch.randint(0, vocab_size, (batch_size, seq_length))
            attention_mask = torch.ones(batch_size, seq_length)

            with torch.no_grad():
                output = model(input_ids, attention_mask)

            assert output.shape == (batch_size, output_dim)
            print(f"      ‚úÖ {model_name} funciona correctamente")

        except Exception as e:
            print(f"      ‚ùå Error en {model_name}: {e}")
            all_passed = False

    return all_passed


def test_utility_functions(main_module):
    """Prueba funciones utilitarias"""
    print("üõ†Ô∏è Probando funciones utilitarias...")

    functions_to_test = [
        "expand_dialogues",
        "plot_training_history",
        "plot_confusion_matrix",
        "plot_model_comparison",
        "visualize_model_architecture",
        "save_results",
        "analyze_model_performance",
        "hyperparameter_analysis",
    ]

    all_passed = True

    for func_name in functions_to_test:
        try:
            if hasattr(main_module, func_name):
                print(f"   ‚úÖ {func_name} encontrada")
            else:
                print(f"   ‚ùå {func_name} no encontrada")
                all_passed = False
        except Exception as e:
            print(f"   ‚ùå Error verificando {func_name}: {e}")
            all_passed = False

    return all_passed


def test_expand_dialogues_function(main_module):
    """Prueba espec√≠fica de la funci√≥n expand_dialogues"""
    print("üîÑ Probando funci√≥n expand_dialogues...")
    try:
        # Crear datos de prueba
        test_data = pd.DataFrame(
            {
                "dialog": [
                    ["Hello", "How are you?", "I am fine"],
                    ["Good morning", "Nice weather today"],
                ],
                "act": [["greeting", "question", "inform"], ["greeting", "inform"]],
                "emotion": [["neutral", "curious", "happy"], ["happy", "happy"]],
            }
        )

        expanded = main_module.expand_dialogues(test_data)

        assert len(expanded) == 5  # 3 + 2 di√°logos expandidos
        assert "dialog" in expanded.columns
        assert "act" in expanded.columns
        assert "emotion" in expanded.columns

        print("   ‚úÖ expand_dialogues funciona correctamente")
        return True
    except Exception as e:
        print(f"   ‚ùå Error en expand_dialogues: {e}")
        return False


def test_training_components(main_module):
    """Prueba componentes de entrenamiento sin entrenar realmente"""
    print("üèãÔ∏è Probando componentes de entrenamiento...")
    try:
        # Crear modelo y datos dummy
        model = main_module.SimpleRNN(100, 32, 64, 3)

        # Crear datos dummy
        texts = ["hello world", "test sentence", "another example"]
        labels = [0, 1, 2]
        word_to_idx = {
            "<PAD>": 0,
            "<UNK>": 1,
            "hello": 2,
            "world": 3,
            "test": 4,
            "sentence": 5,
            "another": 6,
            "example": 7,
        }

        dataset = main_module.DialogDataset(texts, labels, word_to_idx, max_length=10)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=2)

        # Probar un paso de entrenamiento
        optimizer = torch.optim.Adam(model.parameters())
        criterion = torch.nn.CrossEntropyLoss()

        model.train()
        for batch in dataloader:
            input_ids = batch["input_ids"]
            attention_mask = batch["attention_mask"]
            labels = batch["labels"]

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            break  # Solo un batch

        print("   ‚úÖ Componentes de entrenamiento funcionan correctamente")
        return True
    except Exception as e:
        print(f"   ‚ùå Error en componentes de entrenamiento: {e}")
        return False


def test_evaluation_components(main_module):
    """Prueba componentes de evaluaci√≥n"""
    print("üìä Probando componentes de evaluaci√≥n...")
    try:
        # Simular resultados de evaluaci√≥n
        dummy_results = {
            "accuracy": 0.85,
            "precision": 0.83,
            "recall": 0.82,
            "f1": 0.84,
            "classification_report": {
                "0": {"precision": 0.8, "recall": 0.9, "f1-score": 0.85}
            },
            "confusion_matrix": np.array([[10, 2], [1, 8]]),
            "predictions": [0, 1, 0, 1],
            "true_labels": [0, 1, 1, 1],
        }

        # Probar an√°lisis de rendimiento
        results_dict = {"LSTM": dummy_results, "GRU": dummy_results}

        # Esto deber√≠a funcionar sin errores
        analysis_df = main_module.analyze_model_performance(results_dict, "Test Task")

        assert len(analysis_df) == 2
        assert "Modelo" in analysis_df.columns

        print("   ‚úÖ Componentes de evaluaci√≥n funcionan correctamente")
        return True
    except Exception as e:
        print(f"   ‚ùå Error en componentes de evaluaci√≥n: {e}")
        return False


def test_data_file_requirements():
    """Verifica si los archivos de datos est√°n presentes"""
    print("üìÇ Verificando archivos de datos requeridos...")

    required_files = ["train.parquet", "validation.parquet", "test.parquet"]
    found_files = []
    missing_files = []

    for file in required_files:
        if os.path.exists(file):
            found_files.append(file)
            size_mb = os.path.getsize(file) / (1024 * 1024)
            print(f"   ‚úÖ {file} encontrado ({size_mb:.1f} MB)")
        else:
            missing_files.append(file)
            print(f"   ‚ùå {file} no encontrado")

    if missing_files:
        print(f"   ‚ö†Ô∏è Archivos faltantes: {missing_files}")
        print(f"   ‚ÑπÔ∏è El proyecto usar√° datos dummy para testing")
        return False
    else:
        print(f"   ‚úÖ Todos los archivos de datos est√°n presentes")
        return True


def run_integration_test(main_module):
    """Ejecuta un test de integraci√≥n completo pero r√°pido"""
    print("üîÑ Ejecutando test de integraci√≥n...")
    try:
        # Crear datos dummy que simulen la estructura real
        dummy_train = pd.DataFrame(
            {
                "dialog": [
                    ["Hello", "How are you?"],
                    ["Good morning", "Nice day"],
                    ["Thank you", "You are welcome"],
                ],
                "act": [
                    ["greeting", "question"],
                    ["greeting", "inform"],
                    ["thanking", "acknowledge"],
                ],
                "emotion": [
                    ["neutral", "curious"],
                    ["happy", "happy"],
                    ["grateful", "kind"],
                ],
            }
        )

        # Expandir datos
        expanded_train = main_module.expand_dialogues(dummy_train)

        # Crear tokenizer
        all_texts = list(expanded_train["dialog"])
        tokenizer = main_module.SimpleTokenizer(all_texts, vocab_size=50)

        # Crear mapeo de etiquetas
        unique_acts = sorted(list(set(expanded_train["act"])))
        act_to_idx = {act: idx for idx, act in enumerate(unique_acts)}

        # Convertir etiquetas
        train_labels = [act_to_idx[act] for act in expanded_train["act"]]

        # Crear dataset
        dataset = main_module.DialogDataset(
            expanded_train["dialog"], train_labels, tokenizer.word_to_idx, max_length=20
        )

        # Crear dataloader
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=2)

        # Crear modelo simple
        model = main_module.SimpleRNN(
            vocab_size=tokenizer.vocab_size,
            embedding_dim=32,
            hidden_dim=64,
            output_dim=len(unique_acts),
        )

        # Mini entrenamiento (1 √©poca, pocos batches)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        criterion = torch.nn.CrossEntropyLoss()

        model.train()
        total_loss = 0
        batch_count = 0

        for batch in dataloader:
            if batch_count >= 2:  # Solo 2 batches
                break

            input_ids = batch["input_ids"]
            attention_mask = batch["attention_mask"]
            labels = batch["labels"]

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            batch_count += 1

        avg_loss = total_loss / batch_count
        print(
            f"   ‚úÖ Test de integraci√≥n completado (p√©rdida promedio: {avg_loss:.4f})"
        )
        return True

    except Exception as e:
        print(f"   ‚ùå Error en test de integraci√≥n: {e}")
        traceback.print_exc()
        return False


def generate_execution_plan(time_estimate):
    """Genera un plan de ejecuci√≥n basado en la estimaci√≥n de tiempo"""
    print("üìã PLAN DE EJECUCI√ìN RECOMENDADO")
    print("-" * 50)

    if not time_estimate:
        print("‚ùå No se pudo generar plan sin estimaci√≥n de tiempo")
        return

    total_hours = time_estimate["total_time"] / 3600

    if total_hours < 1:
        print("üöÄ EJECUCI√ìN INMEDIATA")
        print("   ‚úÖ Tiempo estimado: < 1 hora")
        print("   üí° Recomendaci√≥n: Ejecutar ahora")
        print("   üìù Comando: python proyecto_deep_learning_dialogos.py")

    elif total_hours < 3:
        print("‚è∞ EJECUCI√ìN PROGRAMADA")
        print(f"   ‚è±Ô∏è Tiempo estimado: {total_hours:.1f} horas")
        print("   üí° Recomendaci√≥n: Ejecutar durante pausa larga")
        print("   üìù Opciones:")
        print("      ‚Ä¢ Ejecuci√≥n completa: python proyecto_deep_learning_dialogos.py")
        print("      ‚Ä¢ Solo validaci√≥n: python test_proyecto_completo.py")

    else:
        print("üåô EJECUCI√ìN NOCTURNA")
        print(f"   ‚è±Ô∏è Tiempo estimado: {total_hours:.1f} horas")
        print("   üí° Recomendaci√≥n: Ejecutar durante la noche")
        print("   üìù Opciones:")
        print(
            "      ‚Ä¢ Ejecuci√≥n completa: nohup python proyecto_deep_learning_dialogos.py > output.log 2>&1 &"
        )
        print("      ‚Ä¢ Por partes: Ejecutar cada tarea por separado")

    print(f"\n‚ö° ALTERNATIVAS R√ÅPIDAS:")
    print(f"   üß™ Solo validaci√≥n: python test_proyecto_completo.py")
    print(f"   üéØ Demos espec√≠ficos: python ejecutar_partes_especificas.py")
    print(f"   üìä Benchmark: python ejecutar_partes_especificas.py quick")


def main():
    """Funci√≥n principal del validador"""
    print("üß™ VALIDADOR COMPLETO PARA proyecto_deep_learning_dialogos.py")
    print("INCLUYE ESTIMACI√ìN DE TIEMPO DE EJECUCI√ìN")
    print("=" * 70)
    print(f"‚è∞ Iniciado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)

    # 1. Cargar m√≥dulo principal
    print("\nüì¶ Cargando m√≥dulo principal...")
    main_module, module_loaded = load_main_module()

    if not module_loaded:
        print("‚ùå No se pudo cargar el m√≥dulo principal. Abortando tests.")
        return False

    print("‚úÖ M√≥dulo principal cargado exitosamente")

    # 2. Verificar archivos de datos
    data_files_present = test_data_file_requirements()

    # 3. Estimaci√≥n de tiempo de ejecuci√≥n
    print(f"\n{'='*70}")
    time_estimate = estimate_execution_time()
    print(f"{'='*70}")

    # 4. Benchmark de velocidad (opcional)
    if data_files_present:
        print(f"\n{'='*70}")
        benchmark_time = benchmark_training_speed(main_module)
        print(f"{'='*70}")

    # 5. Tests de componentes individuales
    test_results = []

    print(f"\nüîç EJECUTANDO TESTS DE COMPONENTES:")
    print("-" * 50)

    # Test tokenizer
    result = test_tokenizer_class(main_module)
    test_results.append(("SimpleTokenizer", result))

    # Test dataset
    result = test_dataset_class(main_module)
    test_results.append(("DialogDataset", result))

    # Test modelos
    result = test_model_classes(main_module)
    test_results.append(("Clases de Modelos", result))

    # Test funciones utilitarias
    result = test_utility_functions(main_module)

    test_results.append(("Funciones Utilitarias", result))

    # Test expand_dialogues espec√≠fico
    result = test_expand_dialogues_function(main_module)
    test_results.append(("expand_dialogues", result))

    # Test componentes de entrenamiento
    result = test_training_components(main_module)
    test_results.append(("Componentes de Entrenamiento", result))

    # Test componentes de evaluaci√≥n
    result = test_evaluation_components(main_module)
    test_results.append(("Componentes de Evaluaci√≥n", result))

    # 6. Test de integraci√≥n
    print(f"\nüîÑ EJECUTANDO TEST DE INTEGRACI√ìN:")
    print("-" * 50)

    integration_result = run_integration_test(main_module)
    test_results.append(("Test de Integraci√≥n", integration_result))

    # 7. Resumen de resultados
    print(f"\nüìä RESUMEN DE RESULTADOS:")
    print("=" * 70)

    passed_tests = 0
    total_tests = len(test_results)

    for test_name, result in test_results:
        status = "‚úÖ PAS√ì" if result else "‚ùå FALL√ì"
        print(f"{status:10} | {test_name}")
        if result:
            passed_tests += 1

    # 8. Estad√≠sticas finales
    success_rate = (passed_tests / total_tests) * 100

    print(f"\nüìà ESTAD√çSTICAS:")
    print(f"   Tests ejecutados: {total_tests}")
    print(f"   Tests exitosos: {passed_tests}")
    print(f"   Tests fallidos: {total_tests - passed_tests}")
    print(f"   Tasa de √©xito: {success_rate:.1f}%")

    # 9. Verificaciones adicionales
    print(f"\nüîç VERIFICACIONES ADICIONALES:")
    print(
        f"   üìÇ Archivos de datos: {'‚úÖ Presentes' if data_files_present else '‚ùå Faltantes'}"
    )
    print(f"   üéÆ CUDA disponible: {'‚úÖ S√≠' if torch.cuda.is_available() else '‚ùå No'}")
    if torch.cuda.is_available():
        print(f"   üéÆ GPU: {torch.cuda.get_device_name(0)}")
    print(f"   üêç Versi√≥n Python: {sys.version.split()[0]}")
    print(f"   üî• Versi√≥n PyTorch: {torch.__version__}")

    # 10. Plan de ejecuci√≥n
    if time_estimate:
        print(f"\n{'='*70}")
        generate_execution_plan(time_estimate)
        print(f"{'='*70}")

    # 11. Recomendaciones finales
    print(f"\nüí° RECOMENDACIONES FINALES:")

    if success_rate == 100:
        print("   üéâ ¬°Excelente! Todos los tests pasaron")
        print("   ‚úÖ El proyecto est√° listo para ejecutarse")

        if data_files_present and time_estimate:
            total_hours = time_estimate["total_time"] / 3600
            if total_hours < 1:
                print("   üöÄ Tiempo estimado < 1 hora - Ejecutar inmediatamente")
                print("   üìù Comando: python proyecto_deep_learning_dialogos.py")
            elif total_hours < 3:
                print(
                    f"   ‚è∞ Tiempo estimado ~{total_hours:.1f} horas - Planificar ejecuci√≥n"
                )
                print("   üìù Comando: python proyecto_deep_learning_dialogos.py")
            else:
                print(
                    f"   üåô Tiempo estimado ~{total_hours:.1f} horas - Ejecuci√≥n nocturna recomendada"
                )
                print(
                    "   üìù Comando: nohup python proyecto_deep_learning_dialogos.py > output.log 2>&1 &"
                )
        else:
            print(
                "   üì• Descarga los archivos parquet para ejecutar el proyecto completo"
            )

    elif success_rate >= 80:
        print("   üëç La mayor√≠a de tests pasaron")
        print("   üîß Revisa los tests fallidos antes de ejecutar")
        print("   ‚ö†Ô∏è El proyecto podr√≠a funcionar con limitaciones")

    else:
        print("   ‚ö†Ô∏è Varios tests fallaron")
        print("   üîß Revisa y corrige los errores antes de continuar")
        print("   ‚ùå No recomendado ejecutar el proyecto principal")

    # 12. Opciones adicionales
    print(f"\nüéØ OPCIONES DE EJECUCI√ìN:")
    print(f"   üî¨ Validaci√≥n completa: python test_proyecto_completo.py")
    print(f"   üéÆ Demos interactivos: python ejecutar_partes_especificas.py")
    print(f"   ‚ö° Test r√°pido: python ejecutar_partes_especificas.py quick")
    print(f"   üß† Solo modelos: python ejecutar_partes_especificas.py models")
    print(f"   üìä Solo datos: python ejecutar_partes_especificas.py data")

    # 13. Informaci√≥n de archivos de salida
    print(f"\nüìÅ ARCHIVOS QUE GENERAR√Å EL PROYECTO:")
    expected_outputs = [
        "resultados_clasificacion_actos.json",
        "resultados_clasificacion_emociones.json",
        "analisis_rendimiento_actos.csv",
        "analisis_rendimiento_emociones.csv",
        "confusion_matrix_*.png",
        "training_history_*.png",
        "model_comparison_*.png",
        "architecture_*.png",
    ]

    for output in expected_outputs:
        print(f"   üìÑ {output}")

    # 14. Informaci√≥n de tiempo
    print(f"\n‚è±Ô∏è INFORMACI√ìN DE TIEMPO:")
    print(f"   Validaci√≥n completada: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    if time_estimate:
        total_time = time_estimate["total_time"]
        print(f"   Proyecto completo estimado: {timedelta(seconds=total_time)}")

        # Mostrar horario recomendado
        now = datetime.now()
        estimated_finish = now + timedelta(seconds=total_time)
        print(
            f"   Si inicias ahora: Terminar√≠a ~{estimated_finish.strftime('%Y-%m-%d %H:%M:%S')}"
        )

        # Sugerir horario nocturno si es muy largo
        if total_time > 7200:  # M√°s de 2 horas
            tonight = now.replace(hour=22, minute=0, second=0, microsecond=0)
            if tonight < now:
                tonight += timedelta(days=1)
            finish_tonight = tonight + timedelta(seconds=total_time)
            print(
                f"   Ejecuci√≥n nocturna (22:00): Terminar√≠a ~{finish_tonight.strftime('%Y-%m-%d %H:%M:%S')}"
            )

    # 15. Consejos de monitoreo
    if time_estimate and time_estimate["total_time"] > 1800:  # M√°s de 30 minutos
        print(f"\nüëÄ CONSEJOS DE MONITOREO:")
        print(f"   üìä El proyecto mostrar√° progreso en tiempo real")
        print(f"   üíæ Los resultados se guardan autom√°ticamente")
        print(f"   ‚èπÔ∏è Puedes interrumpir con Ctrl+C si es necesario")
        print(
            f"   üìù Para ejecuci√≥n en background: nohup python proyecto_deep_learning_dialogos.py > output.log 2>&1 &"
        )
        print(f"   üëÅÔ∏è Para monitorear: tail -f output.log")

    return success_rate >= 80


if __name__ == "__main__":
    success = main()

    print(f"\n{'='*70}")
    if success:
        print("üéØ VALIDACI√ìN EXITOSA - El proyecto est√° listo")
        print("üöÄ Puedes proceder con la ejecuci√≥n")
        exit(0)
    else:
        print("‚ö†Ô∏è VALIDACI√ìN CON PROBLEMAS - Revisa los errores")
        print("üîß Corrige los problemas antes de ejecutar")
        exit(1)
